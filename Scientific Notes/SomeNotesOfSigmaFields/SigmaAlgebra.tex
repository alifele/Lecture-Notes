\documentclass[11pt,a4paper]{article}

\usepackage{commands}


\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{xcolor}
\usepackage{nameref}

\usepackage{tkz-graph}
\usepackage{dsfont}

\newtheorem*{summary}{Summary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}




\title{Some Notes on $\sigma\text{-algebra}$ and Random Variables}
\author{Ali Fele Paranj}



\begin{document}
	
	\maketitle
	
	
	\section{Introduction}
	The probability theory is so natural that one can start thinking about some basic problems as soon as they start to know how to count! Because finding the answer to most of the problems boils down to counting the number of all possible scenarios, and then reporting the proportion of the outcomes of interest to the total number. The first shock in probability theory is experienced when you sit in a undergraduate level probability course. Then you will learn about the notion of a random variable that is a function! And even more shockingly, there is nothing random about a random variable. Surviving so many abuse of notations (like probability density function, probability function, distribution, law, etc) then you might get a feeling that you understand what probability is saying and you can imagine a peaceful life after your course. However, once one encounters the treatment of probability theory in a graduate level course, at first, you might even think that you are sitting in a wrong class (and continue thinking the same way for 4 more weak if not the whole term!). That is because you will hear about different notions like $\sigma\text{-algebra}$, measurable random variables, a filtration, adaptive random variable, and etc. The situation becomes even more scary when you hear for the first time that conditional expectation is a random variable! In this note, I will provide a more algebraic treatment of the notion of $\sigma\text{-algebra}$ which will help to understand these notions better. Also, by the end of these notes you will have a more tangible sense of the damn sentence ``we can think of $\sigma\text{-algebra}$ as information available to us!''. Seriously.
	
	
	\section{$\sigma\text{-Algebra}$}
	The notion of $\sigma\text{-algebra}$ is easiest to understand when considered in a finite setting. We are not going to review the definition of $\sigma\text{-algebra}$ here. However, we want to highlight that we can look at a $\sigma\text{-algebra}$ of a sample space $ \Omega $ in a way that resembles a vector space. Then we can talk about subspaces of that vector spaces (which will be the notion of sub $ \sigma $-fields). We start with an explicit example.
	
	Let $ \Omega = \set{1,2,3} $ be our sample space. Then for $\sigma\text{-algebra}$ we have many options. One one extreme we can have $ \mathcal{F}_0 = \set{\emptyset, \Omega} $, and on the other extreme we can have 
	\[ \mathcal{F}_3 = \set{\set{1},\set{2},\set{3},\set{1,2},\set{2,3},\set{1,3},\set{1,2,3}, \Omega} \]
	We can consider any of these collections as a collection of functions rather than a collection of sets. For instance, we can define
	\[ \mathcal{F}_3 = \set{f:\set{1,2,3}\to \mathbb{F}_2}, \]
	where $ \mathbb{F}_2 = \set{0,1} $ is a field with characteristic 2. For instance the set $ \set{1} $ will be the same as the function $ f $ where $ f(1) = 1, f(2)=0$, and $f(3) = 0 $. And when the domain of those functions is finite, then the followings are equivalent representations of the same thing.
	\[ f(1)=1,f(2)=0,f(3)=0, \qquad (1,0,0), \qquad \vecttt{1}{0}{0}, \qquad \begin{pmatrix}
		1 & 2 & 3 \\
		1 & 0 & 0 
	\end{pmatrix}, \qquad 100,  \]
	and the list above is not an exhaustive one. Thus we can also represent the elements of $ \mathcal{F}_3 $ as 
	\[ \mathcal{F}_3 = \left\{\vecttt{1}{0}{0}, \vecttt{0}{1}{0}, \vecttt{0}{0}{1},\vecttt{1}{1}{0}, \vecttt{1}{0}{1}, \vecttt{0}{1}{1},\vecttt{1}{1}{1},\vecttt{0}{0}{0} \right\}. \]	
	It is easy to see that $ \mathcal{F}_3 $ forms a vector field on $ \mathbb{F}_2 $. In other way of seeing this is to observe that 
	\[ \mathcal{F}_3 = \bigoplus_{i=1}^3 \mathbb{F}_2, \]
	i.e. $ \mathcal{F}_3 $ is a direct sum (equivalently direct product when we can dealing with finitely many of spaces)  of the underlying field $ \mathbb{F}_2 $.
	But what about the case that we choose
	\[ \mathcal{F}_1 = \set{\set{1},\set{2,3},\set{1,2,3},\emptyset}. \]
	Then how we can show this collection as a vector space in above? It turns out that we can write it as
	\[ \mathcal{F}_1 = \set{f: \set{\set{1},\set{2,3}} \to \mathbb{F}_2}. \]
	Observe that $ \set{\set{1},\set{2,3}} $ is in fact a partition of $ \Omega $. So in general we can summarize as below.
	
	\begin{summary}
		Let $ \Omega $ be a finite sample space, and $ \mathcal{P} = \set{P_1,P_2,\cdots,P_n} $ be any partition. Then the $\sigma\text{-algebra}$ generated by $ \mathcal{P} $, denoted as $ \mathcal{F} = \sigma(\mathcal{P}) $, is given as
		\[ \mathcal{F} = \set{\bigcup_{P\in A} P\ |\ A\subseteq \mathcal{P} }. \]
		Or equivalently
		\[ \mathcal{F} = \set{f: \mathcal{P} \to \mathbb{F}_2} = \bigoplus_{P\in \mathcal{P}} \mathbb{F}_2. \]
		Observe that $ \mathcal{F} $ has a vector space structure. The elements of the partition $ \mathcal{P} $ is called the \textbf{atoms} of $ \mathcal{F} $. Furthermore, the collection $ \mathbb{B} = \set{f_i}_{i\in \mathcal{F}} \subset \mathcal{F} $, where $ f_i(P_j) = \delta_{i,j} $ forms a basis for the $ \mathcal{F} $.
	\end{summary}
	
	We will follow the second interpretation above, as it provides a more clear algebraic structure. To understand the notion of measurable functions (i.e. random variable) we need to introduce the dual basis to $ \mathbb{B} $. Consider the collection of functionals $ \set{\mathds{1}_i}_{i=1}^n $ defined as
	\[ \mathds{1}_{i}: \mathcal{F} \to \mathbb{F}_2, \quad \text{where} \quad \mathds{1}_{i}(f_j) = \delta_{i,j}. \]
	This collection forms a dual basis for $ \mathcal{F} $, i.e. it is a basis for $ \mathcal{F}' $.
	
	\begin{summary}
		In the same setting as the summary above, the collection  $ \set{\mathds{1}_i}_{i=1}^n $ defined as
		\[ \mathds{1}_{i}: \mathcal{F} \to \mathbb{F}_2, \quad \text{where} \quad \mathds{1}_{i}(f_j) = \delta_{i,j} \]
		forms a dual basis for $ \mathcal{F} $. With some abuse of notation, these functions are the similar to the indicator function $ \set{\mathds{1}_{P}}_{P\in \mathcal{P}} $ when we view the $\sigma\text{-algebra}$ as $ \mathcal{F} = \set{\bigcup_{P\in A} P\ |\ A\subseteq \mathcal{P} } $.
	\end{summary}
	
	With this point of view, the notion of measurable functions becomes very easy to grasp. A measurable function is simply an element of $ \mathcal{F}' $. I.e. $ X:\Omega \to \R $ is measurable if and only if we can write it as a linear combination of dual basis. With slight abuse of notation (see the summary below for clarification) we can write:
	\[ X = \sum_{i=1}^{n} \alpha_i \mathds{1}_{i}, \]
	or with the notation of the indicator functions
	\[ X = \sum_{P\in \mathcal{P}} \alpha_i \mathds{1}_{P}. \] 
	
	
	\begin{summary}
		$ X $ is $ \mathcal{F} $-measurable if and only if $ X \in \mathcal{F}' $ in the sense that we can write $ X $ as
		\[ X = \sum_{i=1}^n \alpha_i \mathds{1}_{i}\circ q \]
		where $ q: \Omega\to \mathcal{P} $ is the quotient map.
	\end{summary}
	
	
	
	
	
	
	
	
	\newpage
	\bibliographystyle{dinat}
	\bibliography{references}
	
	
\end{document}