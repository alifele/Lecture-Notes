\chapter{Probability Theory}

%\section[Fundamental Concepts]{\hyperlink{toc}{Fundamental Concepts}}
\section{Foundamentals}

The main concept in the field of statistics and probability is the set theory. Basically all we deal with the sets. The whole theroy of statistics can be built on that. Let's discuss some foundamental concepts in statistics and then build the theory.

\subsection{Random Experiment}
To understand the meaning of random experiment, do not over think! The first thing that comes into our minds when we hear the word "random experiment" is its definition! In a nutshell, random experiment is an experiment that its outcome is unkown to us. Like:

\begin{itemize}
	\item Tossing two coin
	\item Rolling a dice
	\item Measuring the number of possible ReadWrite operations on a piece of EEPROM chip
\end{itemize}

Do not overthink about that. Yes we can go further and discuss stuff like "we can compute the exact movement of dice or coin so it is not random but determenistic" and etc. Here I will not touch the philosophical topics that are very deep and do not necessarily converge the a unified point of view!

The random experiments can be modeled and despite the fact that a random experiment is random, we can deduce many useful information from modeling that. To model a random experiment, we use three important concepts: sample space, events, probability. In the following section, we will discuss each of them in detail.


\subsection{Sample Space}

\begin{definition}[Sample Space]
	
	Sample space $\Omega$ is simply a set that contains \emph{all possible outcomes} of a random experiment/
	
\end{definition}

For each of random experiments described above, we can define a sample space. For example:

\begin{itemize}
	\item $\Omega$ of Tossing Tow Coins: $$\Omega = \{ HH,HT,TH,TT \}$$
	\item $\Omega$ of Rolling a Dice: $$\Omega = \{ 1,2,3,4,5,6 \}$$
	\item $\Omega$ of Rolling Two Dices: $$\Omega = \{ (1,1),(1,2), \ldots, (1,6), \ldots ,(6,6)  \}$$
	\item $\Omega$ of Number of possible ReadWrite operations on a EEPROM chip: $$\Omega = \mathbb{N}$$
\end{itemize}



\subsection{Events}
\begin{definition}[Events]
	Event $E$ is a set of outcomes of a random experiment and is the subset of sample space $\Omega$. 
	$$E \in \Omega$$
\end{definition}


For example for any of the sample spaces specified above, we can define so many possible events. In fact any set that is a subset of the sample space is a valid event of that sample space. For example:

\begin{itemize}
	
	\item Tossing Three Coins
	\begin{itemize}
		\item There are at least on Heads: $$E = \{ HHH,HHT,HTH,THH,HTT,THT,TTH \}$$
		\item There are only two Tails: $$E = \{ TTH,THT,HTT \}$$
	\end{itemize}
	
	\item Rolling Two Dices
	\begin{itemize}
		\item The sum of two dices is 4: $$E = \{ (1,3),(2,2),(3,1) \}$$
		\item there are at least one prime number in the outcome:
		$$E = \{ (1,2),(1,3),(1,5),(2,1),(3,1),(5,1),(2,2),(2,3),(2,5), \ldots ,(5,5)\}$$
	\end{itemize}
	
\end{itemize}

Since we have define everything on the basics of set theory, then now we can correspond the everyday concepts to specific operations in the set theory.

\begin{example}{The Mapping Between Everyday Language and Sets in the Theory of Probability}
	
	\begin{itemize}
		\item At least one of two events $A,B \in \Omega$ happens: $E = A \cup B$.
		\item Tow events $A,B \in \Omega$ occures at the same time: $E = A \cap B$.
		\item Event $A \in \Omega$ does not happen: $E = \overline{A} = \Omega - A$.
		\item The event $A$ happens but $B$ does not happen: $E = A - B$.
		
	\end{itemize}
	
\end{example}



In probability and statistics, we are dealing with three important concepts: sample space $\Omega$, event $E$, and probability $P$.


\begin{definition}[Disjoint events]
	
	If two events has no common elements (i.e. $A \cap B = \varnothing$) then we say that two events are \emph{disjoint}. Basically, if two sets in the venn diagram has nothing is common they are considerent to be disjoint sets.
	
	
	For example for the random experiment of tossing two coins, the events 1) both coins are heads: $A = \{HH\}$ and 2) both coins are tails: $B = \{TT\}$. Two events $A,B$  are two disjoint events. \textbf{Two events being  disjoing is NOT the same as being independent}. We will talk about independet events in future.
	
\end{definition}

Note that since the events are basically sets, we can use theorems of set theory to solve the problems. 

\begin{theorem}[De Morgan's Laws]
	
	If $A,B$ are two sets then:
	
	$$\overline{A \cap B} = \overline{A} \cup \overline{B}$$
	
	$$\overline{A \cup B} = \overline{A} \cap \overline{B}$$
	
\end{theorem}

\begin{proof}
	the proof is left as an excerise!
\end{proof}


\subsection{Probability}

The last foundamental ingeridient in modeling a random experiment, is to define a probability for each event. The probability should intuitively reflect how likely an event is probable to happen. This probability should satisfy some foundamental properties which are explained as follows.

\begin{definition}[Axioms of probability (Kolmogorov axioms)]
	
	Suppose that $A, B \in \Omega$ is an event and $\mathbb{P}$ is a probability function. Then $\prob$ should satisfy the following properites:
	
	\begin{enumerate}
		
		\item $ 0 \leq \prob(A) \leq 1$
		\item $\prob(\Omega) = 1$
		\item For the events $E_1, E_2, ..., E_n \in \Omega$ that are mutually exclusive (i.e. disjoint events): $$\prob(\bigcup_{i} E_i) = \sum_i \prob(E_i)$$
	\end{enumerate}
	
	
\end{definition}



These axioms are called the foundamental axioms of probability and also the Kolmogorov axioms. We are free to define any kind of probabilty function that we want but it is important that 1) It should align with our common sense, 2) It should satisfy the Kolmogorov axioms. 

Using the axioms above, we can observe and prove several interesting properties of the probability function. In the following box we have expressed some of them.

\begin{theorem}[Basic Properties of the Probability Function]
	
	Suppose that $\prob$ is a probability function and $A,B \in \Omega$ are events of the sample space $\Omega$. We can show that the probability function has the following properties:
	
	\begin{enumerate}
		
		\item $\prob(\varnothing) = 0$
		
		\item If $A \subset B$ then $\prob(A) \leq \prob(B)$.
		
		\item $\prob(\overline{A}) = 1 - \prob(A)$.
		
		\item $\prob(A \cup B) = \prob(A) + \prob(B) - \prob(A \cap B)$.
		
	\end{enumerate}
	
\end{theorem}


\begin{proof}
	The properties can be proved using the basic set theory theorems.
	
	\begin{enumerate}
		
		\item Since $\emptyset$ is the complement of $\Omega$, so these two sets are disjoint (i.e. $\emptyset \cap \Omega = \emptyset$). On the other hand from the set theory we know that $\emptyset \cup \Omega = \Omega$. So $\prob(\emptyset \cup \Omega) = \prob(\Omega)$. On the other hand, using the third axiom we can write: $\prob(\emptyset \cup \Omega) = \prob(\emptyset) + \prob(\Omega)$. Comparing the two recent equations we can conclude that $\prob(\emptyset) = 0$.
		
		
		
	\end{enumerate}
	
	
	
	The proofs for 2,3,4 are left as a exercise. However, the solutions can be found in the book "Statistical Modeling and Computation by Kroese" chapter 1. 
	
	
\end{proof}



\begin{example}[Defining a simple probability function]
	
	Let's define a probability function for the rolling n dice experiment that is both aligned with our common sense and also satisfy the Kolmogorov equations. Suppose that the $\Omega$ is the sample space and $E \in \Omega$ is an event. Then let's define:
	
	$$\prob(E) = \frac{\lvert E \rvert}{\lvert \Omega \rvert}$$
	
	in which the $\lvert E \rvert$ means the cardinality (number of elements) of the set $E$.
	
\end{example}

Utilizing the properties of the probability function, we can derive some very important notions, one of which is reflected in the following proposition.

\begin{proposition}[Conditional expansion - Law of total probabilities]
	Let $(\Omega, \mathcal{F}, \prob)$ be a probability space. Let $\mathfrak{F}$ be a finite collection of events $\mathfrak{F} = \set{F_1,F_2,\cdots, F_n}$ that partitions $\Omega$. I.e.
	\begin{enumerate}[(i)]
		\item $F_i \cap F_j = \emptyset \qquad i\neq j$,
		\item $\bigcap_{i} F_i = \Omega$.
	\end{enumerate}
	Let $E \in \mathcal{F}$ be any nonempty event. Then we can write
	\[  \prob(E) = \sum_{i} \prob(E|F_i)\prob(F_i). \]
\end{proposition}
\begin{proof}
	Since $\mathfrak{F}$ partitions $\Omega$ and $E \neq \emptyset$, then $\set{E\cap F_i}_i$ is a partition of $E$. Thus
	\[ \prob(E) = \prob(\bigcup_i (E \cap F_i)) = \sum_i \prob(E \cap F_i) = \sum_i \prob(E|F_i)\prob(F_i). \]
	This completes the proof.
\end{proof}


In dealing with random variables, either continuous or discrete, using the notion of the law of total probabilities helps us to simplify some of the calculations significantly. The following examples are some places that we use this idea to simplify calculations by a lot. 

\begin{example}
	Let $ X_1, X_2, X_3, \cdots $ be i.i.d. real-valued random variable. Let $ T $ be a positive integer valued random variable. Define the the real-valued random variable $ N $ as 
	\[ N = \sum_{i=1}^{T} X_i. \]
	What is the probability generating function for $ N $.
	
	\begin{solution}
		For the generating probability function we have
		\[ G_N(s) = \E{s^N} = \E{s^{X_1+X_2+\cdots+X_T}}. \]
		The problem in evaluating the expression above is that the number of random variables $ X_i $ to be summed up is also a random variable. So the first step is to make this a non-random variable buy conditional expansion.
		\[ G_N(s) = \E{s^N} = \sum_{i\in\N}\E{s^{X_1+\cdots+X_T}\big|T=i}\prob(T=i) = \sum_{i\in\N}\E{s^{X_1+\cdots+X_i}}\prob(T=i) \]
		Since $ X_i $ are all i.i.d., then we can write
		\[ G_N(s) = \sum_{i\in\N}(\E{s^{X_1}})^n\prob(T=i) = G_T(G_{X_1}(s)). \]
	\end{solution}
\end{example}

\begin{example}
	Let $ X,Y $ be two independent random variables. Define $ Z = X+Y $. Find the PDF of $ Z $. 
	\begin{solution}
		First, We need to find $ F_Z(z) = \prob(Z<z) $. For this we can write
		\[ F_Z(z) = \prob(X+Y < z) \]
		Again, we can use conditional expansion to write
		\[ F_Z(z) = \int_{\R} \prob(X+Y<z\big| Y = y)f_Y(y)\ dy = \int_{\R} \prob(X<z-y)f_Y(y)\ dy  = \int_{\R} F_X(z-y)f_Y(y)\ dy. \]
		Then differentiating $ F_z $ with respect to $ z $ we will get the PDF 
		\[ f_Z(z) = \frac{d }{dz}F_Z(z) =  \int_{\R} f_X(z-y)f_Y(y)\ dy = (f_X * f_Y)(z).\]
	\end{solution}
\end{example}

\begin{example}
	Let $ X,Y $ be two real valued random variable, not necessarily independent. Calculate $ \prob(X<Y) $.
	\begin{solution}
		To calculate this we can again use the law of total probabilities. In particular
		\[ \prob(X<Y) = \int_{\R}\prob(X<y)f_Y(y)\ dy = \int_{\R}F_{X|Y}(y)f_Y(y)\ dy. \]
	\end{solution}
\end{example}





\subsection{Isomorphism between random experiments}
Often, there is this intuition that certain random experiments are really the same, although they might look very different from each other. For instance, consider two random experiments. In one, we are playing a dice successively and asking what is the probability that after 5 plays, 1 is not appeared. The second experiment is that we have 6 Urns and we place balls in them successively, i.e. at each step one ball is placed in one of the urns and the chance of a ball to end up in any of the urns in equal. These two experiment, although very different, but looks very similar. There is one way that we can formalize this wage intuition, and that is the notion of isomorphism between sets. We say two sets are isomorphic if there is a bijection between them. And the reason that the previously mentioned experiments feel the same is that the sample space $\Omega$ of these two experiments are in fact isomorphic. 

\section{Random Variables}
Often, we are interested in the some measurements of the outcome of a random experiment rather than knowing the outcome it self. For instance, if the experiment of tossing two dice, we might be interested in asking the question if the sum of two dice is 6, and not concerned over whether the actual outcome was (3,3) or (2,4), etc. These quantities of interest are called random variables. The following definition put this into a more formal definition.

\begin{definition}
	Let $(\Omega, E, \prob)$ be a probability space. Then a random variable $X$ is a function $X: \Omega \to S$, where $S$ called the state space.
\end{definition}

\begin{remark}
	The state space $S$ must have some properties, i.e. being measurable, etc. You can read more about this on the Wikipedia of random variables. Also, the state space $S$ if often $\R$, or in the case of a discrete time Markov chain, $S$ is a finite set (that can be the edge set of a graph).
\end{remark}

Since the value of a random variable is determined by the outcomes of the random experiment, we can assign probabilities to the possible values of the random variable. We use the following notation for this purpose.

\begin{definition}[Notation for probability of random variables]
	Let $X$ be a random variable. Then we define event 
	\[ E = \set{X = a} = \set{\omega \in \Omega: X(\omega) = a}. \]
	Then the following notations are usually used interchangeably:
	\[ \prob(X=a) = \prob(\set{X=a})  \]
	both of which is simply $\prob(E)$.
\end{definition}

\begin{example}
	Let $X$ be a random variable defined to be the sum of two fair dice. Then 
	\begin{align*}
		&\prob(\set{X =2}) = \prob(\set{(1,1)}) = \frac{1}{36},\\
		&\prob(\set{X=3}) = \prob(\set{(1,2),(2,1)}) = \frac{2}{36},\\
		&\prob(\set{X=13}) = \prob(\emptyset) = 0.
	\end{align*}
\end{example}

\begin{example}
	Suppose that we toss a coin having probability $p$ of coming up heads. We continue tossing the coin until we see a heads. Let the random variable $N$ be the number of times we toss the coin. Describe this random variable.
	
	\begin{solution}
		Although, we can always solve this kind of questions in an ad hoc way by just simply following our intuition, but it is always a best practice to try to fine tune our abstract thinking with our intuitive understandings in these kind of example. Then we can use of abstract thinking capability to solve problems that are almost impossible to address by solely depending on the intuition. So, it is a good idea to try to see how does the set $\Omega$ look like. The set $\Omega$ will be the set of all finite string of all $T$ letters terminated with $H$. In other words
		\[ \Omega = \set{H,TH, TTH, TTTH, TTTTH, \cdots}. \]
		Then the random variable $N: \Omega \to \Z$ is basically the length of the string. For instance, if $\omega = TTH \in \Omega$, then $N(\omega) = 3$. Let's calculate
		\[ \prob(N = 3) = \prob(\set{\omega \in \Omega: N(\omega) = 3}). \]
		To solve this, we need to define appropriate events and then condition our probability on those events. Define $F_n$ be the event where the $n$ first outcomes are tails. For instance
		\[ F_1 = \set{TH, TTH, TTTH,\cdots},\ F_2 = \set{TTH, TTTH, TTTTH, \cdots},\ \cdots.\]
		And let $E = \set{N=3} = \set{TTH}$. Then we can condition $\prob(E)$ on $F_2$ 
		\[ \prob(E) = \prob(E|F_2) \prob(F_2) + \prob(E|F_2^c) \prob(F_2^c). \]
		Note that $F_2^c = \set{H, TH}$, this $\prob(E|F_2^c) = \prob(E \cap F_2^c)/\prob(F_2^c) = 0$. Now we need to determine $\prob(F_2)$. Again, we can condition this event on $F_1$. Then we can write
		\[ \prob(F_2) = \prob(F_2|F_1)\prob(F_1) + \prob(F_2|F_1^c) \prob(F_1^c). \]
		with the same argument as above $\prob(F_2|F_1^c) = 0$. Combining these equations we will get
		\[ \prob(E) = \prob(E|F_2) \prob(F_2|F_1) \prob(F_1). \]
		Now these probabilities are easy to calculate which leads to the final answer
		\[ \prob(E) = (1-p)(1-p) p.  \]
		And by induction we can conclude
		\[ \prob(\set{N = n}) = (1-p)^n p.  \]
	\end{solution}
\end{example}


\begin{example}
	Suppose that independent trials, each of which results in $m$ possible outcomes with respective probabilities $p_1, p_2, \hdots,p_m$ such that $\sum_{i=1}^{m}p_i = 1$. Are continually performed. Let $X$ be the number of trials needed until each outcome has occurred at least once. Describe the properties of this random variable.
	\begin{solution}
		It is sometime a good idea to try to imagine what does the sample space look like. Let $\Sigma=\set{s_1, s_2, s_3, \hdots, s_m}$ be a set of $m$ distinct symbols. Then each time we are continually performing the experiment, we are getting each of these symbols with corresponding probability $p_m$. Thus the sample space will be the set of all infinite sequences of these symbols. In other words
		\[ \Omega = \set{\text{all infinite sequence of symbols from $\Sigma$}}. \]
		Then the random number $X(\omega)$ for $\omega \in \Omega$ is basically the length of the prefix string of $\omega$ in which any of the symbols in $\Sigma$ has been occurred at least once. 
	\end{solution}
\end{example}



\subsection{Cumulative Distribution of Random Variable}
The notion of the cumulative distribution of a random variable comes handy in most of the future calculations. Also, this distribution can be used to derive other notions of distributions what are extremely important in applications. 

\begin{definition}[Cumulative distribution]
	Let $X$ be a random variable $X:\Omega \to \R$. Then the cumulative distrubition $F: \R \to \R$ is defined as
	\[ F(x) = \prob(\set{X \leq x}).  \]
\end{definition}

\begin{proposition}
	The cumulative distribution of a random variable has the following properties.
	\begin{enumerate}[(i),itemsep=0pt, topsep=5pt]
		\item $\prob(a<X\leq b) = F(b) - F(a).$
		\item $F(x)$ is a non-decreasing function of $x$.
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}[(i)]
		\item 
		\[ \prob(\set{a<X\leq b}) = \prob(\set{X\leq b} \cap \set{X\leq a}^c) = -\prob(\Omega) + \prob(\set{X\leq b}) + \underbrace{\prob(\set{X\leq a}^c)}_{1-\prob(\set{X\leq a})} = F(b) - F(a).  \]
		\item Let $b_1, b_2 \in \R$ and $b_1 \leq b_2$. Then $\set{X\leq b_1} \subseteq \set{X\leq b_2}$. This implies $$\prob(\set{X\leq b_1}) \leq \prob(\set{X\leq b_2}) \implies F(b_1) \leq F(b_2).$$
		This implies that $F(x)$ is a non-decreasing function. 
	\end{enumerate}
\end{proof}


\section{Probability Generating Function}
In this section we will go through the details of the probability generating function. We start with the following definition.

\begin{definition}[Probability Generating Function]
	Let $ X $ be a random variable with state space $ S = \Z_+ $. Then the probability generating function for this random variable is a function define as
	\[  G_X(s) = \E{s^X} = \sum_{x \in S}s^x \prob(X=x).  \]
\end{definition}

In different areas of mathematics, we often can define something algebraic that is very easy to handle (like differentiation, etc) and carries the important information of the object under study. One of these algebraic symbolic objects is the Tutte polynomial, Chromatic polynomial, matching polynomial, etc. These polynomials are kind of modeling the object under study with tools that are easy to handle. The probability generating function is one of those symbolic objects. Because of the way that is crafted, it carries most of the information about the random variable, while the actual object as a function might have poor properties. This will be more clear in the following proposition. In a nutshell, the probability generating function is more of a symbolic thing rather than actual function with meaning full properties. That is why we generally evaluate this function (and its derivatives) at point 0 or 1. 


\begin{proposition}[Properties of the probability generating function]
	Let $ X $ be a random variable, and $ G_X(s) $ its probability generating function. Then we have
	\begin{enumerate}[(i)]
		\item $ G_X(1) = 1 $.
		\item $ \E{X} = G_X'(1)  $.
		\item $ \var{X} = G_X''(1) - G_X'(1)^2 + G_X'(1) $
		\item Let $X, Y$ be independent random variables. Then we have
		\[  G_{X+Y}(s) = G_X(s) G_Y(s). \]
		\item Let $ X_1, X_2, \cdots $ be iid random variables, and $ N $ be a random variable taking values in $ \Z_+ $. Define $ T = X_1 + X_2 + \cdots + X_N $. Then we have
		\[ G_T(s) = (G_N \circ G_{X_1})(s). \]
	\end{enumerate}
\end{proposition}

\begin{proof}
	The proof for part i,ii, iii, and iv basically follows immediately from the definition. So we will only provide the proof for part iv.\\
	$ T $ is the sum of $ N $ iid random variables where $ N $ is itself a random variable. We can make it a normal variable by using the law of total probabilities.
	\[ G_T(s) = G_{\sum_i^N X_i}(s) = \sum_{n=0}^{\infty}  G_{\sum_i^n X_i}(s) \prob(N = n)  = \sum_{n=0}^{\infty}(G_{X_1})^n\prob(N=n) = G_N(G_{X_1})(s) \]
	and this completes the proof.
\end{proof}

The item (iv) in the proposition above is very important, as it makes the hard calculations easy to do. See the following example for more details.

\begin{example}
	We select a number $ N $ from $ \set{1,2,3,\cdots,100} $ randomly and then generate $ N $ random numbers $ X_1, X_2, \cdots X_N $ from the distribution $ \operatorname{Unif}[0,1]$. Then we compute $ T = X_1 + X_2 + \cdots +X_N $. What is the average of $ T $? 
	
	\begin{solution}
		We know that 
		\[ \E{T} = G'_T(1). \]
		Thus we need to calculate the probability generating function $ G_T(s) $. From part (iv) of the proposition above we know that $ G_T = G_N \circ G_{X_1} $. Thus we will have
		\[ G'_T = G_{X_1}' G_N'\circ G_{X_1}.  \]
		Thus evaluating at $ s=1 $ we will have
		\[ G'_T(1) = G_{X_1}'(1) G_N'(\underbrace{G_{X_1}(1)}_{1}) = \E{X_1} \E{N}. \]
		On the other hand we have $ \E{N} = 50 $ and $ \E{X_1} = 1/2 $. Then 
		\[ \E{T} = 25. \]
		The following figure shows this fact (i.e. convergence of the average value of $ T $ to 25 when we increase the number of experiments.)
		\begin{figure}[h!]
			\centering
			\includegraphics[width=0.5\linewidth]{Images/convergenceOfAverageExp.pdf}
		\end{figure}
		
 	\end{solution}
 	
\end{example}





\section{Solved Problems}
\begin{problem}[From Ross]
	Ben can talk a course in compute science or chemistry. If she takes the computer science course, then she will get A grade with probability $\frac{1}{2}$. If she takes the chemistry course, then she will get A grade with probability $\frac{1}{3}$. She decides to base her decision on the flip of a fair coin. What is the probability that she gets an A in chemistry?
\end{problem}
\begin{solution}
	We define the following events
	\begin{quote}
		$A$: she will get an A grade.\\
		$CO$: she will take the computer science course.\\
		$CH$: she will take the chemistry course.
	\end{quote}
	Then the question is basically asking for $\prob(A \cap CH)$. We can compute it by
	\[ \prob(A \cap CH) = \prob(A|CH)\prob(CH) = \frac{1}{3}\cdot\frac{1}{2} = \frac{1}{6}. \]
\end{solution}

\begin{problem}
	And urn contains seven black balls and five white balls. We draw two times from the urn. Given that the each ball has the same probability to be drawn, what is the probability that both balls drawn are black?
\end{problem}
\begin{solution}
	This question nicely demonstrates the fact that there are many ways to define the event spaces, and not all of them are very useful in computing the desired probability. Define
	\begin{quote}
		$E$: two drawn balls are black.
	\end{quote}
	The question is in fact asking $\prob(E)$. But this even is not very useful in any progress with the solution. Thus we need to define some finer events
	\begin{quote}
		$E_1$: The first drawn ball is black.\\
		$E_2$: The second drawn ball is black.
	\end{quote}
	It is clear that $E = E_1 \cap E_2$. These two finer events allows us to compute the probability of interest given the data we have in our hand.
	\[ \prob(E_1 \cap E_2) = \prob(E_2 | E_1) \prob(E_1) = \frac{6}{11} \cdot \frac{7}{12} \]
\end{solution}

\begin{problem}[From Ross]
	Three men at a party through their hats into the center of the room, and then, after mixing the hats, each pick a hat randomly. What is the probability if non of them get their own hat back.
\end{problem}
\begin{solution}
	There are a million ways to tack a probability problem. We can construct a suitable sample space and then compute the probabilities explicitly, or we can use the properties of the probability function to computer the desired probability without any need to construct the sample space. Here, we will demonstrate two ways.
	
	\textbf{Solving the problem by utilizing the properties of the probability function.} First we need to define some suitable events. There are again many ways to define event sets and each have their own pros and cons. We proceed with the following definition.
	\begin{quote}
		$E_i$: The person $i$ ``selects'' his own hat.  
	\end{quote}
	Also, with this particular construction of the event sets, it is much more easier to compute the complementary probability of the desired probability first and then compute the desired one by simply subtracting it from 1. The complement of the event ``no men gets his own hat back'' is ``at least one man gets his hat back'' which is $\prob(E_1\cup E_2 \cup E_3)$. To compute the terms of this we first need to calculate $\prob(E_i)$, $\prob(E_i \cap E_j)$ where $i\neq j$ and also $\prob(E_1 \cap E_2 \cap E_3)$. We know that $\prob(E_i) = 1/3$ for $i=1,2,3$. That is because it is equally likely he selects any of the hats at the center. For $\prob(E_i\cap E_j)$ we can write
	\[ \prob(E_i\cap E_j) = \prob(E_i|E_j)\prob(E_j) = \frac{1}{2}\cdot \frac{1}{3} =  \frac{1}{6}.   \]
	In which we used the fact that $\prob(E_i|E_j)$ is $\frac{1}{2}$ for distinct $i,j$. That is because given person $j$ selects his hat correctly, then there are two possibilities for $E_i$ to select his hat (he can pick the correct one or the wrong one). Lastly for $\prob(E_1\cap E_2\cap E_3)$ we write
	\[ \prob(E_1\cap E_2\cap E_3) = \prob(E_1|E_2\cap E_3)\prob(E_2\cap E_3) = \prob(E_1|E_2\cap E_3) \prob(E_2 | E_3) \prob(E_3) = 1 \cdot \frac{1}{2} \cdot \frac{1}{3} = \frac{1}{6}.  \]
	Thus 
	\[ \prob(E_1 \cup E_2 \cup E_3) = (1) - (1/2) + (1/6) = \frac{4}{6}. \]
	Then the probability of interest will be
	\[ \prob(E) = 1-\frac{4}{6} = \frac{1}{3}. \]
	
	\textbf{Solving by constructing a sample space.} A suitable sample space for this problem can be the set of all permutations on three letters. This set is
	\[ \Omega = 
	\set{\begin{pmatrix}
			a & b & c \\
			\boxed{a} & \boxed{b} & \boxed{c}
		\end{pmatrix},
		\begin{pmatrix}
			a & b & c \\
			\boxed{a} & c & b
		\end{pmatrix},
		\begin{pmatrix}
			a & b & c \\
			b & a & \boxed{c}
		\end{pmatrix},
		\begin{pmatrix}
			a & b & c \\
			b & c & a
		\end{pmatrix},
		\begin{pmatrix}
			a & b & c \\
			c & a & b
		\end{pmatrix},
		\begin{pmatrix}
			a & b & c \\
			c & \boxed{b} & a
	\end{pmatrix}}.
	\]
	Note that the elements in the box represents the fixed point of the permutation. The probability of interest is basically the number of permutations that has no fixed point. As it is clear from the set $\Omega$, the probability is
	\[ \prob(E) = \frac{2}{6} = \frac{1}{3}. \]
\end{solution}

\begin{problem}[Conditional probability mass function (from Ross)]
	Let $ X,Y $ be two random variables with the joint probability mass function given as 
	\[ P(1,1) = 0.5 \qquad P(1,2)=0.1,\qquad P(2,1)=0.1, \qquad P(2,2)=0.3. \]
	Calculate the conditional probability mass function of $ X $ given that $ Y = 1 $.
\end{problem}
\begin{solution}
	We will use the following identity
	\[ P_{X|Y}(x|y) = \prob(X=x|Y=y) = \frac{\prob(X=x,Y=y)}{\prob(Y=y)}. \]
	Observe that 
	\[ \prob(Y=y) = \sum_x \prob(Y=y,X=x) \]
	thus $ \prob(Y=1) = 0.5 + 0.1 = 0.6. $
	So we will have
	\[ P_{X|Y}(1|1) = \frac{0.5}{0.6} = \frac{5}{6}, \qquad P_{X|Y}(2|1) = \frac{0.1}{0.6} = \frac{1}{6}. \]
\end{solution}

\begin{problem}[Conditional probability mass function for geometric random variables (from Ross)]
	Let $ X_1,X_2 $ be two independent random variables with geometric distributions with parameters $ (n_1,p) $ and $ (n_2,p) $. Calculate the conditional probability mass function of $ X_1 $ given that $ X_1+X_2 = m $.
\end{problem}
\begin{solution}
	First, observe that $ Y = X_1 + X_2 $ is a binomial distribution with parameter $ (n_1+n_2, p) $. Thus we can write
	\[ P_{X_1|Y}(k|m) = \prob(X_1 = k | Y = m) = \frac{\prob(X_1 = k, X_1+X_2 = m)}{\prob(X_1+X_2 = m)} = \frac{\prob(X_1=k, X_2 = m-k)}{\prob(Y = m)} \]
	Since the random variables $ X_1 $ and $ X_2 $ are independent, we can write
	\[ P_{X_1|Y}(k|m) = \frac{\prob(X_1=k)\prob(X_2 = m-k)}{\prob(Y=m)} = \frac{\binom{n_1}{k}\binom{n_2}{m-k}}{\binom{n_1+n_2}{m}}. \]
\end{solution}

\begin{problem}[Conditional probability mass function for Poisson random variables (from Ross)]
	Let $ X,Y $ be two independent Poisson random variables with parameters $ \lambda_1 $ and $ \lambda_2 $ respectively. Calculate the conditional probability mass function for $ X $ given that $ X_1 + X_2 = n $.
\end{problem}
\begin{solution}
	First observe that $ Z = X + Y $ is a Poisson random variable with parameter $ \lambda_1 + \lambda_2 $. Thus we will have
	\[ P_{X|X+Y}(m|n) = \frac{\prob(X=m|X+Y=n)}{\prob(X+Y = n)} = \frac{\prob(X=m,Y=n-m)}{\prob(X+Y = n)} \]
	Given that $ X,Y $ are independent random variables then we can write
	\[ P_{X|X+Y}(m,n) = \frac{\prob(X=m)\prob(Y=n-m)}{\prob(X+Y=n)} = \frac{\lambda_1^n \lambda_2^{n-m}n!}{m!(n-m)!(\lambda_1+\lambda_2)^n} = \binom{n}{m} (\frac{\lambda_1}{\lambda_1+\lambda_2})^m ( \frac{\lambda_2}{\lambda_1+\lambda_2})^{n-m} \]
	Thus the conditional probability mass function of $ X $ given that $ X+Y = n $ will be a binomial random variable with parameter $ (n,\lambda_1/(\lambda_1+\lambda_2)) $. We can now easily compute the conditional expectation value as
	\[ \E{X|X+Y = n} = \frac{n\lambda_1}{\lambda_1 + \lambda_2} \]
\end{solution}

\begin{problem}
	Let $ X,Y $ be two random variables. Prove that 
	\[ \E{\E{X|Y}} = \E{X}. \]
\end{problem}