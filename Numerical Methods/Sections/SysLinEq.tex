



\section{Direct Methods}
\subsection{LU Decomposition}

\subsection{RQ Decomposition}

\subsection{Guassian Elimination}

\subsection{Tridiagonal Matrix}

\subsection{Approximate Method}

We want to solve the following system of equations:

\[ \mat{A} x = b \].

We set the matrix $\mat{A}$ to be: $\mat{A} = \mat{S} - \mat{T}$, in which $\mat{S}$ and $\mat{T}$ are the some matrices which are chosed in a smart way!. Let's plug in the new value of $\mat{A}$ in the system of linear equations:

\begin{align*}
	(\mat{S} - \mat{T}) x &= b \\
	\mat{S} x &= \mat{T} x + b \\
	x &= \invmat{S} (\mat{T} x + b) =  \invmat{S} \mat{T} x + \invmat{S} b \\
\end{align*}
So we will have:

\begin{equation}
	\boxed{x = \invmat{S} \mat{T} x + \invmat{S} b}
	\label{equ:expandedEquation}
\end{equation}

Now let's plug in an initial guess $x_0$ in RHS of the the equation \ref{equ:expandedEquation} and name it $x_1$. Then we can do this repeatedly to get the following equations:



\begin{align*}
	x_1 &= \invmat{S} \mat{T} x_0 + \invmat{S} b \\
	x_2 &= \invmat{S} \mat{T} x_1 + \invmat{S} b \\
	& \vdots \\
	x_n &= \invmat{S} \mat{T} x_{n-1} + \invmat{S} b
\end{align*}


To see if we have get closer to the actual solution of the system of equations, let's asume that the actual solution is $x$. So let's define the following errors:

\begin{align*}
	\epsilon_0 &= x - x_0 \\
	\epsilon_1 &= x - x_1 \\
	\epsilon_2 &= x - x_2 \\
	& \vdots \\
	\epsilon_n &= x - x_n 
\end{align*}

By pluggin in $x_0 = x - \epsilon_0 $ in equation \ref{equ:expandedEquation} we will get:

\begin{align*}
	x_1 &= \invmat{S} \mat{T} (x-\epsilon_0) + \invmat{S}b \\ 
	&= \underbrace{\invmat{S}\mat{T}x + \invmat{S}b}_{x} - \invmat{S}\mat{T}\epsilon_0 \\
	&= x - \invmat{S}\mat{T}\epsilon_0 = x - \epsilon_1 \\
	&\Rightarrow \boxed{ \epsilon_1 = \invmat{S}\mat{T} }
\end{align*}

Using the same logic we will get:

\begin{equation}
	\epsilon_n = (\invmat{S}\mat{T})^n
\end{equation}

So using this iterative method to find the approximate solution of the system of the linear equations, we will converge to the actual solution if the largest eigenvalue of the matrix $\invmat{S}\mat{T}$ is smaller than one. Now the only problem is to find the value of $\mat{S}$ is a clever way such that it meets the convergence criteria and is easy to invert. Note that the time complexity of inverting a matrix is $O(N^3)$. So an inapproporate choice of $\mat{S}$ will be very costly.

\subsection{Jacobi Method}
One idea for $\mat{S}$ is the identity matrix $\mathcal{I}$.

\begin{equation}
	\mat{S} = \mathcal{I} = 
	\begin{pmatrix}
		1 & 0  & \cdots & 0\\
		0 & 1  & \cdots & 0\\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \cdots & 1
	\end{pmatrix}
\end{equation}






\subsection{Guass Seidel Method}

S is the lower trianglar matrix





