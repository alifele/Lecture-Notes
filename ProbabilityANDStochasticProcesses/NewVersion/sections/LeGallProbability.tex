\chapter{Probability by Le Gall}

\section{Integration of Measurable Functions}

The following properties become very useful in the following chapters. 
\begin{proposition}
	Let $ f $ be a \emph{non-negative} measurable function.
	\begin{enumerate}[(i)]
		\item We have
		\[ \int f d\mu = 0 \biImp f = 0,\ \mu\ a.e. \]
	\end{enumerate}
\end{proposition}
\begin{proof}
	For the proof we will have
	\begin{enumerate}[(i)]
		\item Define 
		\[ A_n = \set{f \geq 1/n}. \]
		Then by the Markov inequality
		\[ \mu(A_n) = \mu(\set{f \geq 1/n}) \leq \frac{\int f\d\mu}{1/n} = 0. \]
		Also observe that $ \set{f > 0} = \bigcup_n \set{f \geq 1/n} = \bigcup_n A_n $. Thus
		\[ \mu(\bigcup_n A_n) \leq \sum_n \mu(A_n) = 0. \]
		This implies $ \mu(\set{f>0}) = 0 $, thus $ f = 0 $ almost everywhere.
		
	\end{enumerate}
\end{proof}
\begin{remark}
	For proof for part (i) in the proposition above, I also have the following proof using the conditional expectation. But I don't know how that translates to the integrals. Note that we have $ \E{X} = 0 $ and we want to show $ \prob(X = 0) = 1 $. We can write
	\begin{align*}
		0 = \E{X} &= \E{X\I_{X=0} + X\I_{X\neq 0}} = \E{X\I_{X=0}} + \E{X\I_{X\neq0}} \\
		&= \underbrace{\E{X|X=0}\E{\I_{X=0}}}_{0} + \E{X|X\neq 0} \E{\I_{X\neq 0}} \\
		&= \E{X|X\neq 0} \E{\I_{X\neq 0}}.
	\end{align*}
	Observe that we have $ \E{X|X\neq 0} > 0 $ since $ X $ is a non-negative random variable. Thus we need to have $ \E{I_{X\neq 0}} = \prob(X\neq0) = 1 $.
\end{remark}


\section{Convergence of Random Variables}
In the following we will have a quick review of the different definitions of convergence.

\begin{definition}[Convergence almost surely]
	Let $ X,X_1,X_2,\cdot $ be random variables. We say $ X_n $ converges to $ X $ almost surely if it converges to $ X $ point-wise on a set of measure 1. In other words
	\[ \prob(X_n \to X) = 1, \]
	where $ X_n\to X$ should be interpreted by point wise convergence as $ n\to \infty $.
\end{definition}

\begin{definition}[Convergence in Probability]
	Let $ X,X_1,X_2,\cdots $ be random variables. We say that $ X_n $ converges to $ X $ in probability if for any $ \epsilon>0 $ we have
	\[ \prob(\abs{X_n - X} \geq \epsilon) \to 0 \qquad \text{as $ n\to \infty $}. \]
\end{definition}
\begin{remark}
	Note that if for all $ \epsilon>0 $ the convergence $ \prob(\abs{X_n - X}\geq\epsilon) \to 0 $ is fast enough so that it is summable, i.e. $ \sum\prob(\abs{X_n-X}\geq\epsilon)<\infty $, then by Borel Cantelli we can conclude that $ \prob(\abs{X_n-X}\geq\epsilon\ i.o.) = 0 $, hence $ \prob(\abs{X_n-X}<\epsilon\ a.a.) = 1 $, this $ X_n\to X $ almost surely.
\end{remark}

\begin{definition}[Convergence in $ L^p $]
	Let $ X,X_1,X_2,\cdots $ be random variables with $ \E{\abs{X}^p} < \infty$ as well as $ \E{\abs{X_n}^p} < \infty $ for all $ n \in \N $, then we say that $ X_n $ converges to $ X $ in $ L^p $ if 
	\[ \E{\abs{X_n - X}^p} \to 0\qquad \text{as $ n\to\infty $}. \]
\end{definition}
\begin{remark}
	With the notation $ \norm{X}_p = (\E{\abs{X}^p})^{1/p} $ we can state the theorem above as: if $ \norm{X}_p < \infty $ and $ \norm{X_n}_p <\infty $ for all $ n $, then $ X_n \to X $ in $ L^p $ if 
	\[ \norm{X_n - X}_p \to 0. \]
\end{remark}