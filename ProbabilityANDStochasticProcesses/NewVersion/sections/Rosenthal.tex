\chapter{Probability by Rosenthal}
In this chapter, I will include sporadic notes during my study of probability from the Rosenthal book. Also, I will try to compile a set of solutions for the problems in this book.



\section{Probability Triples}

\begin{definition}[Semialgebra]
	Let $ X $ be a set. A Semialgebra $ \mathcal{I} $ of the subsets of $ X $ is a collection of the subsets of such that 
	\begin{enumerate}[(a)]
		\item $ \emptyset, X \in \mathcal{I} $.
		\item $ \mathcal{I} $ is closed \emph{finite} \textbf{intersection}.
		\item For $ E \in \mathcal{I} $ it complement $ E^c $ can be written as a \emph{finite disjoint} \textbf{union} of sets in $ \mathcal{I} $.
	\end{enumerate}
\end{definition}
\begin{remark}
	One canonical example for a semialgebra is the set of all intervals in $ \R $, where the term interval contains all open, closed and half open intervals, as well as the empty set, singletons, and the whole space. 
\end{remark}

\begin{definition}[Algebra]
	Let $ \mathcal{M} $ be a collection of sets. Then $ \mathcal{M} $ is an algebra if 
	\begin{enumerate}[(a)]
		\item $ \Omega, \emptyset \in \mathcal{M} $
		\item $ \mathcal{M} $ is closed under complements.
		\item $ \mathcal{M} $ is closed under finite intersection.
		\item $ \mathcal{M} $ is closed under finite union.
	\end{enumerate}
\end{definition}

\begin{proposition}
	Let $ \mathcal{I} $ be a semialgebra, and $ \mathcal{F} = \sigma(\mathcal{I}) $. Let $ \mathbb{P},\mathbb{Q} $ be two probability measures defined on $ \mathcal{F} $. Then if $ \mathbb{P} $ agrees with $ \mathbb{Q} $ on $ \mathcal{I} $, then they agree on $ \mathcal{F} $.
\end{proposition}
\begin{remark}
	The condition that $ \mathcal{I} $ is a semialgebra is crucial. See \autoref{prob:BeingSemiAlgebraIsImportant} for an example.
\end{remark}


\newpage

\subsection{Solved Problems}
\begin{problem}
	Let $ \Omega = \set{1,.2,3,4} $. Determine whether or not each of the following is a $\sigma\text{-algebra}$.
	\begin{enumerate}[(a)]
		\item $ \mathcal{F}_1 = \set{\emptyset, \set{1,2},\set{3,4},\set{1,2,3,4}} $.
		\item $ \mathcal{F}_2 = \set{\emptyset,\set{3},\set{4},\set{1,2},\set{3,4},\set{1,2,3},\set{1,2,4},\set{1,2,3,4}} $.
		\item $ \mathcal{F}_3 = \set{\emptyset, \set{1,2},\set{1,3},\set{1,4},\set{2,3},\set{2,4},\set{3,4},\set{1,2,3,4}} $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		$ \, $
		\item $ \mathcal{F}_1  $ is a $\sigma\text{-algebra}$ and the set of its atoms are $ \set{\set{1,2},\set{3,4}} $. 
		\item $ \mathcal{F}_2 $ is a $\sigma\text{-algebra}$ and the set of its atoms are $ \set{\set{3},\set{4},\set{1,2}} $.
		\item $ \mathcal{F}_3 $ is \textbf{not} a $\sigma\text{-algebra}$ because $ \set{1,2},\set{2,3} \in \mathcal{F}_3 $ but $ \set{1,2}\cap\set{2,3} = \set{2} \notin \mathcal{F}_3 $.
 	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ \Omega = \set{1,2,3,4} $, and let $ \mathcal{I} = \set{\set{1},\set{2}} $. Describe explicitly the $\sigma\text{-algebra}$ $ \sigma(\mathcal{I}) $ (i.e. the smallest $\sigma\text{-algebra}$ containing the collection $ \mathcal{I} $).
\end{problem}
\begin{solution}
	The smallest $\sigma\text{-algebra}$ containing the collection $ \mathcal{I} $ is
	\[ \sigma(\mathcal{I}) = \set{\set{1},\set{2},\set{3,4},\set{1,2},\set{2,3,4},\set{1,3,4},\set{1,2,3,4},\emptyset}. \]
	One way to check to see if this is really the smallest $\sigma\text{-algebra}$ is to first observe that the cardinality of $\sigma\text{-algebra}$ of a finite set should always be of the form $ 2^n $ for some $ n \in \N $, where $ n $ is the number of atoms (or the number of the non-empty sets the the $\sigma\text{-algebra}$ does not contain any of its subsets). Observe that $ \set{1} $ and $ \set{2} $ are already the atoms of the $\sigma\text{-algebra}$. Thus the size of $ \sigma(\mathcal{I}) $ must be at least four. However, we know that $ \sigma(\mathcal{I}) $ contains at least $ 5 $ elements (i.e. $ \set{1},\set{2},\set{1,2},\set{1,2,3,4},\emptyset $). This suggests that there should be at least one other atom in the set. Choosing that atom to be $ \set{3,4} $ will yield that $ \sigma(\mathcal{I}) $ that contains $ 8 $ elements. Since this already includes that collection $ \mathcal{I} $, and we can not have any smaller $\sigma\text{-algebra}$ then we are sure that this is the smallest $\sigma\text{-algebra}$.
\end{solution}


\begin{problem}
	Suppose $ \mathcal{F} $ is a collection of subsets of $ \Omega $, such that $ \Omega \in \mathcal{F} $.
	\begin{enumerate}[(a)]
		\item Suppose $ \mathcal{F} $ is an algebra. Prove that $ \mathcal{F} $ is a semialgebra.
		\item Suppose that whenever $ A,B \in \mathcal{F} $, then also $ A\backslash B \equiv A \cap B^c \in \mathcal{F} $. Prove that $ \mathcal{F} $ is an algebra. 
		\item Suppose that $ \mathcal{F} $ is closed under complement, and also closed under finite \emph{disjoint} unions. Give a counter example to show that $ \mathcal{F} $ might not be an algebra. 
	\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}[(a)]
		\item Firstly, Since $ \mathcal{F} $ is an algebra, then it is closed under complement, hence $ \emptyset \in \mathcal{F} $. Secondly, Since it is closed under finite intersection, then it meets the closedness under finite intersection property of a semialgebra. Lastly, let $ E \in \mathcal{F} $. Since $ \mathcal{F} $ is an algebra then $ E^c \in \mathcal{F} $. So we can trivially write $ E^c = E^c $ as a finite disjoint union of sets in $ \mathcal{F} $. Thus $ \mathcal{F} $ is a semialgebra.
		\item Firstly, since $ \Omega \in \mathcal{F} $, then by hypothesis $ \Omega \backslash \Omega = \emptyset \in \mathcal{F} $. Secondly, let $ A \in \mathcal{F} $. Then by hypothesis $ \Omega \ A = A^c \in \mathcal{F} $, thus $ \mathcal{F} $ is closed under complement. Lastly, Let $ A,B \in \mathcal{F} $. By the reasoning above $ B^c \in \mathcal{F} $. And by hypothesis $ A\backslash B^c \in \mathcal{F} $. This implies that $ A \cap B \in \mathcal{F} $
		\item One simple counter example can be constructed when we let $ \Omega = \set{1,2,3,4} $ and then let 
		\[ \mathcal{F} = \set{\Omega,\emptyset, \set{1,2},\set{1,3},\set{1,4},\set{2,3},\set{2,4},\set{3,4}}. \]
		This collection is closed under finite disjoint union as well as complement. But it fails to be an algebra. For instance $ \set{1,2},\set{2,3} \in \mathcal{F} $, but their intersection is not in the collection.
	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ \mathcal{F}_1,\mathcal{F}_2,\cdots $ be a sequence of collections of subsets of $ \Omega $, such that $ \mathcal{F}_n \subseteq \mathcal{F}_{n+1} $ for each $ n $. 
	\begin{enumerate}[(a)]
		\item Suppose that each $ \mathcal{F}_i $ is an algebra. Prove that $ \bigcup_{i=1}^\infty \mathcal{F}_i $ is also an algebra. 
		\item Suppose that each $ \mathcal{F}_i $ is a $\sigma\text{-algebra}$. Show (by counterexample) that $ \bigcup_{i=1}^\infty  \mathcal{F}_i$ need not be a $\sigma\text{-algebra}$.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ \mathcal{G} = \bigcup_{i=1}^\infty \mathcal{F}_i $. First, observe that since $ \Omega, \emptyset \in \mathcal{F}_i $ for all $ i\in \N $ (since all of them are algebra), then it follows that $ \Omega, \emptyset \in \mathcal{G} $. Furthermore, let $ A \in \mathcal{G} $. Then $ A \in \mathcal{F}_i $ for some $ i\in\N $. Since $ \mathcal{F}_i $ is an algebra, then $ A^c \in \mathcal{F}_i $, hence $ A^c \in \mathcal{G} $. Lastly, let $ A,B \in \mathcal{G} $. Then $ A\in\mathcal{F}_i $ and $ B \in \mathcal{F}_j $ for some $ i,j \in \N $. WLOG we can assume $ i \leq j $. Then $ \mathcal{F}_i \subset \mathcal{F}_j $, hence $ A,B \in \mathcal{F}_j $. Since $ \mathcal{F}_j $ is an algebra, then $ A\cap B \in \mathcal{F}_j $. Thus $ A \cap B \in \mathcal{G} $. This proves that $ \mathcal{G} $ is an algebra. 
		\item Let $ \Omega = \N $. Let $ \mathcal{F}_n $ be the smallest $\sigma\text{-algebra}$ that contains the collection $ \set{\set{1},\cdots,\set{n}} $. On other way to think about $ \mathcal{F}_n $ is the $\sigma\text{-algebra}$ that contains the power set of $ \set{1,\cdots,n} $ as well as all of their complements (with respect to $ \Omega $). For instance, we have
		\[ \mathcal{F}_1 = \set{\emptyset,\set{1},\N, \set{1}^c}. \]
		Similarly
		\[ \mathcal{F}_2 = \set{\emptyset,\set{1},\set{2},\set{1,2},\N,\set{1}^c,\set{2}^c,\set{1,2}^c}, \]
		and etc. Let $ A_i = \set{2 i} $. Clearly $ A_i \in \bigcup_i \mathcal{F}_i $. However, $ \bigcup_i A_i \notin \bigcup_i \mathcal{F}_i$ as it does not belong to any $ \mathcal{F}_k $. Thus $ \bigcup_i\mathcal{F}_i $ is not a $\sigma\text{-algebra}$.
	\end{enumerate}
\end{solution}

\begin{problem}
	Suppose that $ \Omega = \N  $ is the set of positive integers, and $ \mathcal{F} $ is the set of all subsets $ A $ such that either $ A $ or $ A^c $ is finite, and $ \prob $ is defined by $ \prob(A) = 0 $ if $ A $ is finite, and $ \prob(A) = 1 $ if $ A^c $ is finite. 
	\begin{enumerate}[(a)]
		\item Is $ \mathcal{F} $ an algebra?
		\item Is $ \mathcal{F} $ a $\sigma\text{-algebra}$?
		\item Is $ \prob $ finitely additive?
		\item Is $ \prob $ countably additive on $ \mathcal{F} $, meaning that if $ A_1,A_2,\cdots \in \mathcal{F} $ are disjoint, and if it happens that $ \bigcup_n A_n \in \mathcal{F} $, then $ \prob(\bigcup_n A_n) = \sum_n \prob(A_n) $?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Yes. First, observe that $ \emptyset, \Omega \in \mathcal{F} $ as $ \emptyset $ is finite, and $ \Omega $ has a finite complement. Further, let $ A \in \mathcal{F} $. Then either it is finite or it has a finite complement, where for both cases we have $ A^c \in \mathcal{F} $. Let $ A_1,\cdots,A_n $ be a finite collection of sets in $ \mathcal{F} $. If all $ A_i $ for $ i=1,\cdots,n $ are finite, then since the finite intersection and complement of any finite collection of finite sets is finite, $ \bigcap_{i=1}^n A_i $ as well as $ \bigcup_{i=1}^n A_i$ are finite as well, thus belongs to $ \mathcal{F} $. If $ A_i $ are all infinite, then since they all belong to $ \mathcal{F} $ then they have finite complement, hence $ \bigcup_{i=1}^n A_i^c $ and $ \bigcap_{i=1}^n A_i^c $ are finite as well, thus belongs to $ \mathcal{F} $. If the collection is not in any of the case above, then there is $ 1\leq j \leq n $ such that $ A_j $ is finite. Thus $ \bigcup_i A_i $ is finite, thus belongs to $ \mathcal{F} $. Being closed under finite intersection and complements implies being closed under finite union.
		
		\item No. Let $ A_n = \set{2n} $. Then $ A_n \in \mathcal{F} $ for all $ n\in\N $. However, $ \bigcup_n A_n \notin \mathcal{F} $ as it is neither finite nor has a finite complement.
		
		\item Yes. First observe that if $ A,B $ are both infinite with $ A^c, B^c $ finite (this $ A,B \in \mathcal{F} $), then $ A\cap B \neq\emptyset $. Otherwise, $ A^c \cup B^c = \N $ which implies that either of them is infinite which is a contradiction. Thus given $ A,B $, if both are finite then $ \prob(A\cup B) = \prob(A) + \prob(B) = 0 $ as $ A\cup B $ is also finite. If both are infinite, then based on our argument above then they are not disjoint, so the argument of additivity does not apply to them. However if WLOG $ A $ is finite and $ B $ is infinite and $ A\cap B = 0 $, then $ A\cup B $ is also infinite thus $ 1 = \prob(A\cup B) = \prob(A) + \prob(B) = 0 + 1 $.
		
		\item No. Let $ A_n = \set{n} $. Then 
		\[ \prob(\bigcup_n A_n) = \prob(\N) = 1 \neq \sum_n \prob(A_n) = 0. \]
	\end{enumerate}
	
\end{solution}

\begin{proposition}
	Let $ \Omega = \N $ and let $ \mathcal{F} $ be the collection of all subsets of $ \Omega $ that is \emph{countable} or has countable \emph{complement}. Then $ \mathcal{F} = \sigma(\mathcal{A}) $ where $ \mathcal{A} = \set{\set{1},\set{2},\cdots} $, i.e. the set of all singletons.
\end{proposition}
\begin{proof}
	Let $ E \in \mathcal{F} $. First, note that the collection $ \mathcal{F} $ is a $\sigma\text{-algebra}$. Then, notice that $ \mathcal{F} $ contain $ \mathcal{A} $ as singletons are finite, hence countable. Since $ \sigma(\mathcal{A}) $ is the smallest $\sigma\text{-algebra}$ that contain $ \mathcal{A} $ then $ \sigma(\mathcal{A})\subset \mathcal{F} $. Let $ E \in \mathcal{F} $. Then $ E $ is either countable or has a countable complement. If $ E $ is countable then it can be written as a countable union of singletons in which each singleton contains one element of $ E $. Thus $ E \in \sigma(\mathcal{A}) $. If $ E^c $ is countable, then $ E^c $ can be written as a countable union of singleton of its element. By applying De Morgan's law $ E $ can be written as a countable union of the complements of singletons (which belong to $ \sigma(\mathcal{A}) $). Thus case also implies $ E \in \sigma(\mathcal{A}) $. Thus $ \mathcal{F} = \sigma(\mathcal{A}) $.
\end{proof}

\begin{problem}
	Suppose that $ \Omega = [0,1] $ is the unit interval, and $ \mathcal{F} $ is the set of all subsets $  A $ such that either $ A $ or $ A^c $ is finite, and $ \prob $ is defined by $ \prob(A) = 0 $ of $ A $ is finite and $ \prob(A) = 1 $ if $ A^c $ is finite. 
	\begin{enumerate}[(a)]
		\item Is $ \mathcal{F} $ an algebra?
		\item Is $ \mathcal{F} $ a $\sigma\text{-algebra}$?
		\item Is $ \prob $ finitely additive?
		\item Is $ \prob $ countably additive on $ \mathcal{F} $?
	\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}[(a)]
		\item Yes. Being closed under complement is immediate from the definition. Thus $ \emptyset, \Omega \in \mathcal{F} $. Let $ A,B \in\mathcal{F} $. Then if $ A,B $ are both finite, then $ A\cap B $ is also finite thus $ A\cap B \in \mathcal{F} $. If $ A,B $ are both infinite, then $ A^c, B^c $ are both finite, so it is $ A^c \cup B^c $. Being closed under complement it implies that $ A\cup B  \in \mathcal{F}$. If one of $ A,B $ is infinite and the other one is finite, then $ A \cap B $ is finite, hence $ A\cap B \in \mathcal{F} $. Thus $ \mathcal{F} $ is a $\sigma\text{-algebra}$.
		\item No. Consider the collection $ \set{A_q}_{q \in \Q} $ where $ q \in A  $. Each of these sets are finite, hence $ A_q \in \mathcal{F} $ for all $ q \in \Q $. However $ \bigcup_q A_q $ is not finite and its complement is also not finite. Thus $ \mathcal{F} $ is not closed under countable union. 
		\item Yes. First observe that if $ A,B \in \mathcal{F} $ both infinite, then their intersection can not be empty, otherwise $ A^c \cup B^c = [0,1] $ which means that at least one of them is infinite which is a contradiction. With this in mind let $ A,B \in \mathcal{F} $. If $  A,B $ both finite with empty intersection then $ 0 = \prob(A\cup B) = \prob(A) + \prob(B) = 0 + 0 $ as $ A\cup B $ is also finite. If WLOG $ A $ is infinite and $ B $ is finite and $ A\cup B = \emptyset$, then $ A\cup B $ is also infinite and we have $ 1 = \prob(A\cup B) = \prob(A) + \prob(B) = 0 + 1 $. Thus $ \prob $ is finitely additive.
		\item Yes. First observe that we can not find any two $ A,B \in \mathcal{F} $ disjoint and infinite with empty intersection since then $ A^c\cup B^c = [0,1] $ that implies at least one of them is infinite. So let $ A_1,A_2,\cdots $ be a sequence of \emph{finite} sets in $ \mathcal{F} $. Then 
		\[ 0 = \prob(\bigcup_i A_i) = \sum_i \prob(A_i) = 0. \]
		In case if just one of $ A_i $ is infinite (no more than two can be infinite at the same time) then 
		$ \bigcup_i A_i  $ is infinite and 
		\[ 1 = \prob(\bigcup_i A_i) = \sum_i \prob(A_i) = 1.  \]
	\end{enumerate}
\end{solution}

\begin{problem}
	\label{prob:countableAdditivityOfProbabilityOnSigmaAlg}
	Suppose that $ \Omega =[0,1] $ is the unit interval, and $ \mathcal{F} $ is the set of all subsets $ A $ such that either $ A $ or $ A^c $ is countable (i.e. finite or countable), and $ \prob $ is defined by $ \prob(A) =0 $ if $ A $ is countable, and $ \prob(A) = 1 $ if $ A^c $ is countable.
	\begin{enumerate}[(a)]
		\item Is $ \mathcal{F} $ an algebra?
		\item Is $ \mathcal{F} $ a $\sigma\text{-algebra}$?
		\item Is $ \prob $ finitely additive?
		\item Is $ \prob $ countably additive?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Yes. Being closed under complement follows immediately from the definition. On the other hand, since $ \emptyset $ is finite, then $ \Omega $ (complement of the empty set) also belongs to $ \Omega $.  Let $ A,B \in \mathcal{F} $. If both are countable then $ A\cap B $ is also countable, thus $ A\cap B \in \mathcal{F} $. If for both their complement is countable, then $ A^c \cup B^c = (A\cap B)^c $ is countable. Thus $ A\cap B \in \mathcal{F} $. If one of them is countable, WLOG $ A $, then $ A\cap B $ is also countable, thus $ A\cap B \in \mathcal{F} $. Thus it is an algebra.  
		\item Yes. Being closed under complement follows from the definition and from this it follows that $ \Omega \in \mathcal{F} $. Let $ E_1,E_2,\cdots $ be a sequence of sets in $ \mathcal{F} $. If at least one of them is countable, then $ \bigcap_i E_i $ is also countable hence belonging to $ \mathcal{F} $. If all is uncountable, then $ \bigcup_i E^c_i $ is countable. We can write $ \bigcup_i E^c_i = (\bigcap_i E_i)^c $ that is finite. Thus $ \bigcap+i E_i \in \mathcal{F} $. This shows that $ \mathcal{F} $ is closed under countable union. Being closed under union follows from being closed under intersection and complement. Thus $ \mathcal{F} $ is a $\sigma\text{-algebra}$.
		\item Yes. We will show additivity for two sets and finite additivity will follow by induction. Let $ A,B \in \mathcal{F} $. If $ A,B $ both are disjoint countable then $ A\cup B $ is also countable. Thus $ 0= \prob(A\cup B) = \prob(A) + \prob(B) = 0+0 = 0 $. If $ A, B $ are both uncountable (i.e. $ A^c, B^c $ are countable) then $ A\cap B $ can not be non-empty, otherwise $ A^c\cup B^c = [0,1] $ which then implies that at least one of $ A^c $ or $ B^c $ be uncountable, which is a contradiction. When one of these sets is countable, WLOG $ A $, then $ A\cup B $ is also uncountable and we have $ 1 = \prob(A\cup B) = \prob(A) + \prob(B) = 1 + 0 $. Thus $ \prob $ is finitely countable.
		\item Yes. Let $ A_1,A_2,\cdots $ be a sequence of disjoint sets in $ \mathcal{F} $. Then at most one set can be uncountable, otherwise they will fail to be disjoint (see the reasoning in part (c)). If non of them are uncountable, then their union is also countable (countable union of countable sets is countable). Thus $ 0 = \prob(\bigcup_i A_i) = \sum_i \prob(A_i) = 0 $. If one of them is uncountable, then the union is also uncountable and we will have $ 1 = \prob(\bigcup_i A_i) = \sum_i \prob(A_i) = 0 +\cdots + 0 + 1 + 0 + \cdots = 1 $. Thus $ \prob $ is additive on $ \mathcal{F} $.
	\end{enumerate}
\end{solution}

\begin{problem}
	For the example of \autoref{prob:countableAdditivityOfProbabilityOnSigmaAlg}, is $ \prob $ uncountably additive?
\end{problem}
\begin{solution}
	No. Otherwise we can write $ \Omega = \bigcup_{x\in\Omega}\set{x} $. But we have
	\[ 1 = \prob(\Omega) = \sum_{x\in\Omega}\set{x} = 0. \]
\end{solution}

\begin{problem}
	Let $ \mathcal{F} $ be a $\sigma\text{-algebra}$, and write $ \abs{\mathcal{F}} $ for the total number of subsets in $ \mathcal{F} $. Prove that if $ \abs{\mathcal{F}}<\infty $, i.e. $ \mathcal{F} $  consists of just a finite number of subsets, then $ \abs{\mathcal{F}}=2^m $ for some $ m \in \N $. (\emph{Hint: Consider those non-empty subsets in $ \mathcal{F} $ which do not contain any other non-empty subset in $ \mathcal{F} $. How can all subsets in $ \mathcal{F} $ be build up from these particular subsets?}).
\end{problem}
\begin{solution}
	Let $ \mathcal{A} $ be the collection of all non-empty sets in $ \mathcal{F} $ whose non of its subsets do not belong to $ \mathcal{F} $. Then for any $ E \in \mathcal{F} $ can be build up from these ``atom'' by union. For each atom there are two possibilities to be present in the union or not. Thus there are in total $ 2^m $ elements in $ \mathcal{F} $.
\end{solution}


\begin{problem}
	\label{prob:BeingSemiAlgebraIsImportant}
	Let $ \Omega = \set{1,2,3,4} $, with $ \mathcal{F} $ the collection of all subsets of $ \Omega $. Let $ \mathbb{P} $ and $\mathbb{Q}$ be two probability measures on $ \mathcal{F} $ such that $ \mathbb{P}\set{1} = \mathbb{P}\set{2} = \mathbb{P}\set{3} = \mathbb{P}\set{4} = 1/4 $, and $ \mathbb{Q}\set{2} = \mathbb{Q}\set{4} = 1/2 $, extended to $ \mathcal{F} $ by linearity. Finally, let $ \mathcal{I}=\set{\emptyset,\Omega,\set{1,2},\set{2,3},\set{3,4},\set{1,4}} $.
	\begin{enumerate}[(a)]
		\item Prove that $ \mathbb{P}(A) = \mathbb{Q}(A) $ for all $ A \in \mathcal{I} $.
		\item Prove that there is $ A \in \sigma(\mathcal{I}) $ with $ \mathbb{P}(A) \neq \mathbb{Q}(A) $.
		\item Why does this not contradict Proposition 2.5.8 (in Rosenthal)?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item By a simple calculation we can show the identity above. For instance
		\[ \mathbb{P}\set{1,2} = \mathbb{P}\set{1} + \mathbb{P}\set{2} = 1/4 + 1/4 = 1/2, \]
		where as
		\[ \mathbb{Q}\set{1,2} = \mathbb{Q}\set{1} + \mathbb{Q}\set{2} = 0 + 1/2 = 1/2. \]
		By a similar computation we can show
		\[ \mathbb{P}\set{2,3}=\mathbb{Q}\set{2,3}=1/2,\quad \mathbb{P}\set{3,4}=\mathbb{Q}\set{3,4}=1/2, \quad \mathbb{P}\set{1,4}=\mathbb{Q}\set{1,4}=1/2, \]
		and so on.
		
		\item First, observe that $ \set{1,2,3} \in \sigma(\mathcal{I}) $. That is because $ \set{3} = \set{2,3}\cap \set{3,4} \in \sigma(\mathcal{I}) $. Thus $ \set{1,2}\cup\set{3} = \set{1,2,3} \in \sigma(\mathcal{I}) $. But
		\[ \mathbb{P}\set{1,2,3} = 3/4, \qquad \mathbb{Q}\set{1,2,3} = 1/2. \]
		Thus if we let $ A = \set{1,2,3} \in \sigma(\mathcal{I}) $ then $ \mathbb{P}(A) \neq \mathbb{Q}(A) $.
		
		\item That is because the proposition 2.5.8 requires the collection $ \mathcal{I} $ be a semialgebra, which is not here. For instance $ \mathcal{I} $ is not closed under finite intersection as $ \set{1,2},\set{2,3} \in\mathcal{I} $ whereas their intersection is not in the collection.
	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ (\Omega, \mathcal{M},\lambda) $ be Lebesgue measure on the interval $ [0,1] $. Let
	\[ \Omega' = \set{(x,y)\in \R^2: 0<x\leq 1, 0<y\leq 1}. \]
	Let $ \mathcal{F} $ be the collection of all subsets of $ \Omega' $ of the form
	\[ \set{(x,y)\in \R^2: x\in A, 0<y\leq 1} \]
	for some $ A \in \mathcal{M} $. Finally, defined a probability $ \mathbb{P} $ on $ \mathcal{F} $ by
	\[ \prob\set{(x,y)\in\R^2: x\in A, 0<y\leq 1} = \lambda(A). \]
	\begin{enumerate}[(a)]
		\item Prove that $ (\lambda', \mathcal{F},\prob) $ is probability space.
		\item Let $ \prob^* $ be the outer measure corresponding to $ \prob $ and $ \mathcal{F} $. Define the subset $ S \subseteq \Omega' $ by
		\[ S = \set{(x,y)\in \R^2: 0<x\leq 1, y = 1/2}. \]
		(Note that $ S \notin \mathcal{F} $.) Prove that $ \prob^*(S)=1 $ and $ \prob^*(S^c)=1 $.
	\end{enumerate}

\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item $ \mathcal{F} $ being a $\sigma\text{-algebra}$ follows immediately from $ \mathcal{M} $ being a $\sigma\text{-algebra}$. To see this, for instance let $ H \in \mathcal{F} $. Then there exists some $ A \in \mathcal{M} $ such that $ H = A \times (0,1] $, where $ \times $ is the Cartesian product of two sets. Then $ H^c = \Omega'\backslash H $ will be given as $ H^c = A^c \times(0,1] $. Since $ A^c \in \mathcal{M} $ then it follows that $ H^c \in \mathcal{F} $. With a similar reasoning we can show that $ \mathcal{F} $ is a $\sigma\text{-algebra}$.
		
		\noindent Furthermore, $ \prob $ being a probability measure follows immediately from the fact that $ \lambda $ is a probability measure. For instance $ \prob(\Omega') = \lambda(\Omega) = 1 $, and $ \prob(B) \geq 0 $ for all $ B \in \mathcal{F} $ since we can write $ B = A \times(0,1] $ where $ A \in \mathcal{M} $ and by definition $ \prob(A) = \lambda(A) > 0 $. Countable additivity also follows from a similar line of reasoning.
		
		\item From the monotonicity of the outer measure and using the fact that $ S \subset \Omega' $ one gets that $ \prob^*(S) \leq \prob^*(\Omega') = \prob(\Omega') = 1 $. Furthermore, observe that we can write $ S = \Omega \times\set{1/2} $. Let $ \set{I_k \in \mathcal{F}} $ be any open cover for $ S $. Thus for each $ I_k $ there exists $ A_k \in\mathcal{M} $ such that $ I_k = A_k \times(0,1] $. Since $ S = \Omega\times\set{1/2} $ the  $ \set{B_k} $ will be an open cover for $ \Omega $. Using the fact that $ \prob(I_k) = \lambda(B_k) $
		\[ 1 = \lambda(\Omega)  \leq \sum_k \lambda(B_k) = \sum_k \prob(I_k)  \]
		Since the inequality above holds for any open cover $ \set{I_k} $ for $ S $ then we can conclude that $ 1 \leq \prob^*(S) $. So far
		\[ \prob^*(S) \geq 1, \qquad \prob^*(S)\leq 1. \]
		Then it follows that 
		\[ \prob^*(S) = 1. \]
	\end{enumerate}
\end{solution}

\begin{problem}
	\label{prob:ModifiedTHeorem}
	\begin{enumerate}[(a)]
		\item Where in the proof of Theorem 2.3.1 was assumption 2.3.3 used?
		\item How would the  of Theorem 2.3.1 be modified?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item It is only used with proving the equality $ \prob(A) = \prob^*(A) $ for $ A \in \mathcal{I} $. I.e. to show that $ \prob^* $ is an extension of $ \prob $ to a larger domain $ \mathcal{M} $ which is a $\sigma\text{-algebra}$ that contain the collection $ \mathcal{I} $.
		\item Then the identity $ \prob^*(A) = \prob(A) $ for $ A \in \mathcal{I} $ should be replaced with $ \prob^*(A) \leq \prob(A) $.
 	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ \Omega=\set{1,2} $, and let $ \mathcal{I} $ be the collection of all subsets of $ \Omega $, with $ \prob(\emptyset)=0, \prob(\Omega) = 1 $, and $ \prob\set{1}=\prob\set{2} = 1/3 $.
	\begin{enumerate}[(a)]
		\item Verify that all assumptions of theorem 2.3.1 other than 2.3.3 are satisfied.
		\item Verify that the assumption 2.3.3 is not satisfies.
		\item Describe precisely the $ \mathcal{M} $ and $ \prob^* $ that would result in this example from the modified version of Theorem 2.3.1 in \autoref{prob:ModifiedTHeorem}.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item First, notice that the power set of $ \Omega $ finite, is always an algebra thus semialgebra. So $ \mathcal{I} $ is a semialgebra. On the other hand by the hypothesis we have $ \prob(\emptyset) = 0 $ and $ \prob(\Omega) = 1 $ which satisfies some of the conditions in Theorem 2.3.1. For the super-additivity, it holds because 
		\[ \prob(\set{1}\cup\set{2}) = 1 > \prob\set{1} + \prob\set{2} = 2/3. \]
		\item It is easy to check as $ \set{1,2},\set{1},\set{2} \in \mathcal{I} $ with $ \set{1,2} \subseteq \set{1}\cup \set{2} $ but
		\[ \prob\set{1,2} \not\leq \prob\set{1}+\prob\set{2}.\]
		\item {\color{red} \noindent TODO: TOBEADDED}
	\end{enumerate}
\end{solution}


\begin{problem}
	Let $ \Omega = [0,1] $. Let $ \mathcal{I}' $ be the set of all half-open intervals of the form $ (a,b], $ for $ 0\leq a < b \leq 1 $, togheter with the sets $ \emptyset, \Omega $, and $ \set{0} $.
	\begin{enumerate}[(a)]
		\item Prove that $ \mathcal{I}' $ is s semialgebra. 
		\item Prove that $ \sigma(\mathcal{I}') = \mathcal{B} $, i.e. that the $\sigma\text{-algebra}$ generated by this $ \mathcal{I}' $ is equal to the $\sigma\text{-algebra}$ generated by the $\sigma\text{-algebra}$ of (2.4.1) in Rosenthal.
		\item Let $ \mathcal{B}'_0 $ be the collection of all finite disjoint unions of elements of $ \mathcal{I}' $. Is $ \mathcal{B}'_0 $ the same as the algebra $ \mathcal{B}_0 $ defined in (2.2.4) Rosenthal?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item By definition $ \mathcal{I}' $ contains $ \emptyset $ and $ \Omega $. To show being closed under intersection let $ A, B \in \mathcal{I}' $. If $ A,B $  are disjoint then $ A\cap B \in \mathcal{I'} $. However if $ A,B $ are not disjoint, then WLOG we can assume that $ A=(a_1,a_2], B = (b_1,b_2] $ where $ a_1 < b_1 < a_2 \leq b_2 $. Thus $ A\cap B = (b_1,a_2] $ which is also at $ \mathcal{I'} $. For the last property of a semialgebra, let $ A \in \mathcal{I'} $. We can assume $ A = (a,b] $ for $ 0\leq a<b \leq 1 $. Then $ A^c = [0,a] \cup (b,1] = \set{0}\cup (0,a] \cup (b,1] $ which is a finite disjoint union of elements of $ \mathcal{I'} $.
		\item By (2.4.1) Rosenthal, $ \mathcal{B} $ is the smallest $\sigma\text{-algebra}$ of all intervals in $ [0,1] $ where the term intervals include all open, closed, half-open, intervals as well as the empty set, singletons, and the whole set $ [0,1] $. First, notice that $ \sigma(\mathcal{I'}) $ contains all of the intervals in $ [0,1] $. That is because by using complements, countable unions, as well as countable intersections one can construct any kind of intervals using the intervals of the type $ (a,b] $. Since $ \mathcal{B} $ is the smallest $\sigma\text{-algebra}$ containing $ \mathcal{I} $ thus $ \sigma(\mathcal{I})\subseteq \sigma(\mathcal{I}') $. To show the equality,  observe that $ \mathcal{I'} \subset \mathcal{I} $ thus $ \sigma(\mathcal{I'}) \subseteq \sigma(\mathcal{I}) $. These two inequalities implies that $ \sigma(\mathcal{I'}) = \sigma(\mathcal{I}) = \mathcal{B}$. 
		\item No it is not. Since $ (1/3,1/2) \in \mathcal{B}_0 $ but not in $ \mathcal{B'}_0 $.
	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ K $ be the Cantor set. Let $ D_n = K \oplus \frac{1}{n} $ be  a shifted Cantor set by $ 1/n $. Let $ B = \bigcup_n D_n $. 
	\begin{enumerate}[(a)]
		\item Draw a rough sketch of $ D_3 $.
		\item What is $ \lambda(D_3) $?
		\item Draw a rough image of $ B $.
		\item What is $ \lambda(B) $?
	\end{enumerate}
\end{problem}
\FloatBarrier
\begin{solution}
	\begin{enumerate}[(a)]
		\item See figure below.
		\item Since $ K \in \mathcal{B} $, then it is shift invariant, thus $ \lambda(K_3) = \lambda(K)  = 0 $.
		\item See the figure below.
		\item From the countable sub-additivity we have
		\[ \lambda(B) = \lambda(\bigcup_n K_n) \leq \sum_n \lambda(K_n) = 0. \]
	\end{enumerate}
	\input{Images/CantorShifted.tex}
	\FloatBarrier
\end{solution}
\begin{remark}
	Note that $ K \oplus K = \bigcup_{x\in K}( K \oplus x) = [0,2] $. I.e. the set of all numbers that can be created by adding two Cantor numbers is all the numbers in $ [0,2] $. Note that the Cantor set has Lebesgue measure zero, however $ [0,2] $ has measure 2. That is because $ \bigcup_{x\in K}) $ is in fact an uncountable union of sets (since a Cantor set is uncountable).
\end{remark}

\begin{problem}
	Let $ \Omega $ be a finite non-empty set, and let $ \mathcal{I} $ consist of all singletons in $ \Omega $, together with $ \emptyset $ and $ \Omega $. Let $ p: \Omega \to [0,1] $ with $ \sum_{\omega \in \Omega}p(\omega) = 1 $, and define $ \prob(\emptyset) = 0,\prob(\Omega) = 1 $, and $ \prob\set{\omega}=\prob(w) $ for all $ \omega \in \Omega $.
	\begin{enumerate}[(a)]
		\item Prove that $ \mathcal{I} $ is a semialgebra.
		\item Prove that (2.3.2) and $ (2.3.3) $ are satisfied. 
		\item Describe precisely the $ \mathcal{M} $ and $ \prob^* $ that result from applying Theorem 2.3.1 in Rosenthal.
		\item Are these $ \mathcal{M} $ and $ \prob^* $ the same as those described in Theorem 2.2.1 in Rosenthal?
 	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item By definition $ \mathcal{I} $ contains $ \emptyset $ and well as $ \Omega $. $ \mathcal{I} $ is also closed under finite intersection as the intersection of two singletons is either a singleton or the empty set, and the intersection of $ \Omega $ with any singleton is a singleton. Furthermore, the intersection of any singleton with empty set is the empty set that is contained in $ \mathcal{I} $. Finally, let $ E \in \mathcal{I} $. If $ E $ is either $ \Omega $ or the empty set, then it complement can trivially be written as the disjoint union of $ \emptyset $ or $ \Omega $ respectively. If $ E $ is a singleton, then $ E^c $ can be written as the disjoint union of the singleton of its elements. Thus $ \mathcal{I} $ is a semialgebra.
		
		\item To check $ (2.3.2) $ let $ A_1,\cdots,A_k \in \mathcal{I} $ disjoint with $ \bigcup_i A_i \in \mathcal{I} $. Then $ \bigcup_i A_i = \Omega $ the collection $ A_i $'s are all of the singletons. Thus 
		\[ 1 = \prob(\bigcup_i A_i) = \prob(\Omega) = \sum_i \prob(A_i) = \sum_{\omega\in\Omega}\prob(\set{\omega}) = \sum_{\omega\in\Omega}p(\omega) = 1. \]
		Thus $ 2.3.2 $ holds with equality
		
		\noindent To verify $ (2.3.3) $ let $ A,A_1,A_2,\cdots,A_k \in \mathcal{I} $ with $ A \subset \bigcup_i A_i $. If $ A $ is empty set, then (2.3.3) holds as $ 0 \leq a $ for all $ a\in [0,1] $. If $ A $ is $ \Omega $, then the the sets $ A_i $ are the sets of all singletons. Thus 2.3.3 holds as $ 1\leq 1 $. Lastly, if $ A $ is a singleton, then at least one of $ A_i $'s should be the same as $ A $. Then $ \prob(A) \leq \prob(A_1) + \cdots + \prob(A_j) +  \cdots + \prob(A_k) $ for some $ 0\leq j \leq n $. Since $ \prob(A_j) = \prob(A) $ then 2.3.3 holds.
		
		\item The collection $ \mathcal{M} $ will be the same as the power set of $ \Omega $. And the probability measure $ \prob^* $ will be give as
		\[ \prob^*(A) = \sum_{\omega\in A} p(\omega). \]
		
		\item Although $ \mathcal{M} $ is the same as in theorem $ 2.3.1 $, but $ \prob^* $ is not the same. The probability measure defined in Theorem 2.3.1 is the uniform probability measure, where here it is not. The probability measure $ \prob^* $ is a more general one and will be the same as probability measure in Theorem 2.3.1 if we choose $ p(\omega) = 1/\abs{\Omega} $.
	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ \prob $ and $ \mathbb{Q} $ be two probability measures defined on the same sample space $ \Omega $ and $\sigma\text{-algebra}$ $ \mathcal{F} $. 
	\begin{enumerate}[(a)]
		\item Suppose that $ \prob(A) = \mathbb{Q}(A) $ for all $ A \in \mathcal{F} $ with $ \prob(A) \leq 1/2 $. Prove that $ \prob = \mathbb{Q} $, i.e. that $ \prob(A) = \mathbb{Q}(A) $ for all $ A \in \mathcal{F} $.
		\item Give an example where $ \prob(A) = \qrob(A) $ for all $ A \in \mathcal{F} $ with $ \prob(A)<1/2 $, such that $ \prob \neq \qrob $, i.e. that $ \prob(A) \neq \qrob(A) $ for some $ A \in \mathcal{F} $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ A \in \mathcal{F} $ with $ \prob(A) > 1/2 $, hence $ \prob(A^c) \leq 1/2 $. Also, let $ E \in\mathcal{F} $ such that $ \prob(E)\leq 1/2 $. By definition (by using 2.3.7 or using the fact that $ \mathcal{F} $ is a $\sigma\text{-algebra}$ thus closed under intersection and complement) we can write
		\[ \prob(A) = \prob(A \cap E) + \prob(A\cap E^c). \]
		Observe that $ A\cap E \subseteq E $ thus by monotonicity $ \prob(A\cap E) \leq \prob(E) \leq 1/2 $. Further more, we can write $ \prob(A\cap E^c) = 1 - \prob(A^c \cup E) $. For the second term in the RHS we have
		\[ \prob(A^c \cup E) = \prob(A^c) + \prob(E) - \prob(A^c\cap E). \]
		Note that $ \prob(A^c)\leq 1/2 $ as well as since $ A^c\cap E \subset A^c $ thus by monotonicity $ \prob(A^c\cap E) \leq \prob(A^c) \leq 1/2 $. Thus
		\[ \prob(A) = \prob(A\cap E) + 1 - \prob(A^c) - \prob(E) + \prob(A^c\cap E). \]
		For all the terms in the RHS, since their measure with respect to $ \prob $ is less than oe equal to $ 1/2 $, thus $ \prob $ and $ \qrob $ agrees on them. Thus 
		\[ \prob(A) = \qrob(A\cap E) + 1 - \qrob(A^c) - \qrob(E) + \qrob(A^c\cap E) = \qrob(A). \]
		This completes the proof.
		
		\noindent \textbf{An easier solution}. Let $ A \in \mathcal{F} $ with $ \prob(A) > 1/2 $. Then $ \prob(A) = 1 - \prob(A^c) = 1 - \qrob(A^c) = \qrob(A) $, where we used the fact that $ \prob(A^c)\leq 1/2 $ thus $ \prob(A^c) = \qrob(A^c) $.
		
		\item Let $ \Omega =  \set{1,2} $ with $ \mathcal{F} $ being the power set of $ \Omega $. Then defined
		\[ \prob(\emptyset) = 0, \quad \prob(\set{1}) = \prob(\set{2}) = 1/2, \quad \prob(\set{1,2}) = 1. \]
		And
		\[ \qrob(\emptyset) = 0, \quad \qrob(\set{1}) = 1/10, \quad \qrob(\set{2})=9/10, \quad \qrob(\set{1,2}) = 1. \]
	\end{enumerate}
\end{solution}
\begin{remark}
	The hypothesis in part (a) in question above means that $ \prob $ and $ \qrob $ on all of the sets that has measure less than or equal to 1/2 w.r.t $ \prob $, must agree on all of the element of $ \mathcal{F} $.
\end{remark}

\begin{problem}
	Let $ (\Omega_1,\mathcal{F}_1,\prob_1) $ be Lebesgue measure on $ [0,1] $. Consider a second probability triple $ (\Omega_2, \mathcal{F}_2, \prob_2) $, defined as follows: $ \Omega_2 = \set{1,2} $, $ \mathcal{F}_2 $ consists of all subsets of $ \Omega_2 $, and $ \prob_2 $ is defined by $ \prob_2\set{1} = 1/3 $ and $ \prob_2\set{2} = 2/3 $, and additivity. Let $ (\Omega,\mathcal{F},\prob) $ be the product measure of $ (\Omega_1, \mathcal{F}_1, \prob_1) $ and $ (\Omega_2,\mathcal{F_2},\prob_2) $.
	\begin{enumerate}[(a)]
		\item Express each of $ \Omega, \mathcal{F} $ and $ \prob $ as explicitly as possible.
		\item Find a set $ A \in \mathcal{F} $ such that $ \prob(A) = 3/4 $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item The set $ \Omega $ is 
		\[ \Omega = \set{1,2}\times [0,1]. \]
		The collection $ \mathcal{F} $ is given by
		\[ \mathcal{F} = \set{\set{1}\times B: B \in\mathcal{B}}\quad \cup\quad \set{\set{2}\times B: B \in \mathcal{B}} \quad\cup\quad \set{\set{1,2}\times B: B \in \mathcal{B}}. \]
		And $ \prob $ is given by
		\[ \prob(\set{1}\times B) = \lambda(B)/3,\quad \prob(\set{2}\times B) = 2\lambda(B)/3,\quad \prob(\set{1,2}\times B) = \lambda(B). \]
		\item One easy choice for such a set would be $ A = \set{1,2}\times(0,3/4) $.
	\end{enumerate}
\end{solution}
\newpage


\section{Further Probabilistic Foundations}
\begin{definition}
	Let $ (\Omega,\mathcal{F},\prob) $ be a probability space and $ E_1,E_2 \in \mathcal{F} $ be two events. Then $ E_1 $ and $ E_2 $ are said to be independent events if and only if we have
	\[ \prob(E_1\cap E_2) = \prob(E_1)\prob(E_2). \]
	Another way to formulate this is to write
	\[ \prob(E_1 | E_2) = \prob(E_1). \]
\end{definition}
\begin{remark}
	The following diagram is very suggestive to make an intuition to see how does two independent events (i.e. sets) look like.
	\input{Images/VanDiagramProb.tex}
	\FloatBarrier
	The intuition is that the proportion of $ A $ in the restricted world $ B $ (i.e. $ \prob(A|B) $) is the same as the proportion of $ A $ in the whole world. I.e.
	\[ \prob(A|B) = \frac{\prob(A\cap B)}{\prob(B)} = \frac{\prob(A)}{\prob(\Omega)} = \frac{\prob(A)}{1} = \prob(A). \]
\end{remark}

\begin{proposition}
	\label{prop:IndependentEvents}
	Let $ A,B $ be two independent events, i.e. $ \prob(A\cap B) = \prob(A) \prob(B) $. Then the pair of events, $ (A^c, B^c) $, $ (A^c,B) $, and $ (A,B^c) $ are also independent.
\end{proposition}
\begin{proof}
	We start by showing that $ A^c, B $ are independent events. Observe that
	\[ \prob(A^c\cap B) = 1 - \prob(A\cup B^c) = 1 - (\prob(B^c)+\prob(A\cap B)) = \prob(B) + \prob(A)\prob(B) = \prob(A^c) \prob(B). \]
	Thus $ A^c $ and $ B $ are independent events. Similarly we can prove that $ A,B^c $ are independent events. To show that the event $ A^c, B^c $ are independent we have
	\begin{align*}
		\prob(A^c\cap B^c) &= 1 - \prob(A\cup B) = 1 - \prob(A) - \prob(B) + \prob(A\cap B) = \prob(A^c) - \prob(B) + \prob(A)\prob(B) \\
		&= \prob(A^c)-\prob(B)(1-\prob(A)) = \prob(A^c)(1-\prob(B)) = \prob(A^c)\prob(B^c). 
	\end{align*}
	Thus $ A^c $ and $ B^c $ are independent.
\end{proof}

\begin{proposition}
	\label{prop:SeqIndependentEvents}
	Let $ A_1,A_2,\cdots $ be a sequence of independent events. Then the sequence of events $ B_1,B_2,\cdots $ are also independent where $ B_i $ is either equal to $ A_i $ or $ A_i^c $. In particular $ A_1^c, A_2^c,\cdots $ is a sequence of independent events.
\end{proposition}
\begin{proof}
 	Use the result of \autoref{prop:IndependentEvents} with Exercise 3.2.2 in Rosenthal.
\end{proof}
 
 
 \begin{observation}
 	Consider the following problem. Let $ A_1,A_2,\cdots, B_1,B_2,\cdots $ be events.
	\begin{enumerate}[(a)]
		\item Prove that 
		\[ (\limsup A_n) \cap (\limsup_n B_n) \supseteq \limsup_n (A_n\cap B_n). \]
		\item Give an example where the above inclusion is strict, and another example where it holds with equality.
	\end{enumerate}
 	Here, I will give a very intuitive explanation of the meaning of $ \limsup_n A_n $ as well as $ \limsup_n B_n $. Consider $ A_1,A_2,\cdots $ as sum lamps in a row that if $ w \in A_i $ then then $ i $th lamp turns on. Thus $ \limsup_n A_n $ are those elements in $ \Omega $ that if we evaluate its presence in the sequence of sets, a pattern will emerge where as you move further in the row of lamps you will still find a lamp that is on. Similarly for $ \limsup_n B_n $. However, the meaning of $ \omega \in (A_n\cap B_n) $ is that there is lamp position $ i $ such that this lamp is on for both $ A_n $ sequence and $ B_n $ sequence. Thus $ \limsup_n (A_n\cap B_n) $ is the set of all elements that if you evaluate its presence in the $ A_n $ sequence and $ B_n $ sequence, no matter how far you move in the sequence, you will still find spots where both lamps for $ A_i $ and $ B_i $ are on.
 \end{observation}
 
 \begin{proposition}[Borel-Cantelli Lemma]
 	Let $ A_1,A_2,\cdots \in \mathcal{F} $.
 	\begin{enumerate}[(a)]
 		\item If $ \sum_n \prob(A_n) < \infty $, then $ \prob(\limsup_n A_n) = 0 $.
 		\item If $ \sum_n \prob(A_n) = \infty $, and the events are \emph{independent}, then $ \prob(\limsup_n A_n) = 1 $.
 	\end{enumerate}
 \end{proposition}
 
\subsection{Solved Problems}
\[  \]
\begin{problem}
	Let $ X $ be a real-valued random variable defined on a probability triple $ (\Omega, \mathcal{F},\prob) $. Fill in the following blanks:
	\begin{enumerate}[(a)]
		\item $ \mathcal{F} $ is a collection of subsets of \blank.
		\item $ \prob(A) $ is a well-defined element of \blank provided that $ A $ is an element of \blank.
		\item $ \set{X\leq 5} $ is shorthand notation for the particular subset of \blank which is defined by \blank.
		\item If $ S $ is a subset of \blank, then $ \set{X\in S} $ is a subset of \blank.
		\item If $ S $ is a \blank subset of \blank, then $ \set{X \in S} $ must be a element of \blank.
	\end{enumerate}
\end{problem}
\begin{solution}
	$ \quad $
	\begin{enumerate}[noitemsep]
		\item $ \Omega $.
		\item $ \R $, $ \mathcal{F} $.
		\item $ \Omega $, $ \set{\omega\in \Omega: X(\omega)\leq 5} $.
		\item $ \mathcal{B} $, $ \mathcal{F} $.
		\item Borel, $ \R $, $\sigma\text{-algebra}$ $ \mathcal{F} $.
	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ (\Omega, \mathcal{F},\prob) $ be Lebesgue measure on $ [0,1] $. Let $ A = (1/2,3/4) $ and $ B = (0,2/3) $. Are $ A $ and $ B $ independent events?
\end{problem}
\begin{solution}
	Yes. It is easy to check that  $ \prob(A \cap B) = \prob(A) \prob(B) $ holds for $ A,B $ as above.
\end{solution}


\begin{problem}
	Give an example of events $ A,B $, and $ C $, each of probability strictly between $ 0 $ and $ 1 $, such that 
	\begin{enumerate}[(a)]
		\item $ \prob(A\cap B) = \prob(A)\prob(B) $, $ \prob(A\cap C) = \prob(A)\prob(C) $, and $ \prob(B\cap C) = \prob(B)\prob(C)$; but it is not the case that $ \prob(A\cap B \cap C) = \prob(A)\prob(B)\prob(C) $.
		\item $ \prob(A\cap B) = \prob(A)\prob(B), \prob(A\cap C) = \prob(A)\prob(C) $, and $ \prob(A\cap B\cap C) = \prob(A)\prob(B)\prob(C) $; but it is not the case that $ \prob(B\cap C) = \prob(B)\prob(C) $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ \Omega = \set{a,b,c,d} $ and $ \prob $ a uniform discrete distribution on $ \Omega $. Let $ A = \set{a,b}, B = \set{a,c}, C = \set{a,d} $. Then we have 
		\[ \prob(A\cap B) = \prob(A\cap C) = \prob(B\cap C) = \prob(\set{a})=\frac{1}{4} = \prob(A)\prob(B) = \prob(A)\prob(C) = \prob(B)\prob(C). \]
		However, 
		\[ \prob(A\cap B\cap C) = \prob(\set{a}) \neq \prob(A)\prob(B)\prob(C) = \frac{1}{8}. \]
		\item Let $ \Omega = \set{1,2,3,4,5,6,7,8} $ and $ \prob $ a uniform discrete distribution on $ \Omega $. Define
		\[ A = \set{1,2,3,4}, \quad B = \set{3,4,5,6}, \quad C = \set{1,3,5,6}. \]
		Then we have $ \prob(A\cap B) = \prob(\set{3,4}) = \prob(A)\prob(B) = \frac{1}{4} $. Also $ \prob(A\cap C) = \prob(\set{1,3})=\prob(A)\prob(C) = \frac{1}{4} $. Furthermore $ \prob(A\cap B \cap C) = \prob(A)\prob(B)\prob(C) = \frac{1}{8} $. However $ \prob(B\cap C) = \prob(\set{3,5,6}) = \frac{3}{8} \neq \prob(B)\prob(C) = \frac{1}{4}$.
	\end{enumerate}
\end{solution}


\begin{problem}
	Suppose $ \set{A_n}\nearrow A $. Let $ f:\Omega\to \R $ be any function. Prove that $ \lim_{n\to\infty}\inf_{\omega\in A_n} f(\omega) = \inf_{\omega\in A}f(\omega) $.
\end{problem}
\begin{solution}
	Since $ \set{A_n}\nearrow A $ then $ A_1\subseteq A_2\subseteq \cdots $ with $ \bigcup_n A_n = A $. Let $ f(A_i) \subset \R $ denote the image of set $ A_i $ under the map $ f $. Then we have $ f(A_1)\subseteq f(A_2) \subseteq \cdots  $. Thus from properties of the infimum we have $ \inf f(A_1) \geq \inf f(A_2) \geq \cdots $. Observe that $ A_i \subset A $ for all $ i\in \N $, thus $ \inf(A_i)\geq \inf(A) $ for all $ i\in \N $. Thus the sequence of real numbers $ \set{\inf f(A_n)} $ is a decreasing sequence bounded from below. By monotone convergence theorem we conclude that $ \lim_{n\to\infty}f(A_n) $ exists. Then the next step is to show that this limit is the same as $ \inf f(A) $. Let $ \epsilon>0 $ given. Then there is $ \alpha\in f(A) $ such that $ \alpha < \inf f(A) + \epsilon $. Because $ f(A) = f(\bigcup_n A_n) = \bigcup_n f(A_n) $, then $ \exists m \in \N $ such that $ \alpha \in f(A_m) $. Then $ \inf f(A_m) \leq \alpha $. Thus $ \inf f(A_m) \leq \inf f(A) + \epsilon  $. Since we can find such $ A_m $ for every $ \epsilon>0 $, and since $ \inf f(A_n) $ is a decreasing sequence bounded below, then by the definition of limit we get
	\[ \lim_{n\to\infty} \inf_{A_n} f(A_n) = \inf_A f(A).  \]
\end{solution}

\begin{problem}
	Let $ (\Omega, \mathcal{F},\prob) $ be a probability triple such that $ \Omega $ is countable, and $ \mathcal{F} = 2^\Omega $. Prove that it is impossible for there to exist a sequence $ A_1,A_2,\cdots \in \mathcal{F} $ which is \emph{independent}, such that $ \prob(A_i)=\frac{1}{2} $ for each $ i $.
\end{problem}
\begin{solution}
	Let $ \omega \in \Omega $. Then define the sequence $ B_n $ as 
	\[ B_n = \begin{cases}
		A_n \qquad \omega \in A_n, \\
		A_n^c \qquad \omega \notin A_n.
	\end{cases} \]
	Then by \autoref{prop:SeqIndependentEvents} the sequence of events $ B_n $ are also independent. and we have $ \prob(B_n)=1/2 $ for all $ n $. By construction we have $ \omega \in \bigcap_n B_n $. Thus 
	\[ \prob(\set{\omega}) \leq \prob(\bigcap_n B_n) = \prod_n \prob(B_n) = \frac{1}{2^n} \]
	Since this is true for all $ n \in \N $ then $ \prob(\set{\omega}) = 0 $ for all $ \omega \in \Omega $. However, since $ \Omega $ is countable we have $ \Omega = \bigcup_{\omega \in \Omega}\set{\omega} $ which is a disjoint countable union. By countable additivity of the probability measure we have
	\[ 1 = \prob(\Omega) = \sum_{\omega\in\Omega}\prob(\omega) = 0,  \]
	which is a contradiction.
\end{solution}
\begin{solution}[A second solution!]
	Since $\mathbb{P}(A_i) = 1/2$ for all $i$, we have $\sum_i \mathbb{P}(A_i) = \infty$. Noting that $A_i$'s are independent then by Borel-Cantelli we have
	
	$$ \mathbb{P}(\{ A_n \ \text{i.o.} \}) = 1.$$
	
	Observe that $ \{ A_n \ \text{i.o.} \} \subseteq \Omega $, thus it is at most countable. So it can be written as a disjoint union of the singletons of its element. Applying the countable additivity of $\mathbb{P}$ will result in 1 = 0 which is a contradiction.
\end{solution}

\begin{problem}
	Let $ (\Omega,\mathcal{F},\prob) $ be the uniform distribution on $ \Omega = \set{1,2,3} $ as in Example 2.2.2. Give an example of a sequence $ A_1,A_2,\cdots\in\mathcal{F} $ such that 
	\[ \prob(\liminf_n A_n) < \liminf_n \prob(A_n) < \limsup_n \prob(A_n) < \prob(\limsup_n A_n). \]
\end{problem}
\begin{solution}
	An easy choice for such a sequence is 
	\[ \set{1},\set{1,2},\set{1,3},\set{2,3},\set{1},\set{1,2},\set{1,3},\set{2,3},\set{1},\cdots. \]
	It is easy to see that $ \liminf_n A_n = \emptyset, \limsup_n A_n = \set{1,2,3}, \liminf_n \prob(A_n) = 1/3$, and $ \limsup_n \prob(A_n) = 2/3 $. Thus we will get
	\[ \prob(\liminf_n A_n) < \liminf_n \prob(A_n) < \limsup_n \prob(A_n) < \prob(\limsup_n A_n). \]
\end{solution}

\begin{problem}
	Let $ \lambda $ be Lebesgue measure on $ [0,1] $, and let $ 0 \leq a \leq b \leq c\leq d \leq 1 $ be arbitrary real numbers. Give an example of a sequence $ A_1,A_2,\cdots $ of subsets of $ [0,1] $, such that $ \lambda(\liminf_f A_n) = a $, $ \liminf_f \lambda(A_n) = b $, $ \limsup_n \lambda(A_n) = c $, and $ \lambda(\limsup_n A_n) = d $. (\emph{Hint: Start with the case $ d = b + c - a $, which is easiest, and then carefully branch out from there.})
\end{problem}
\begin{solution}
	I am not sure how to use the hint, but one of the examples I could construct is considering the events
	\[ A_n = (\frac{1}{4}+\frac{1}{8}\sin(n), \frac{3}{4}-\frac{1}{8}\sin(n)). \]
	It is easy to see that
	\[ \limsup_n A_n = (\frac{1}{8},\frac{7}{8}),\quad  \liminf_n A_n = (\frac{3}{8},\frac{5}{8}). \]
	Thus we have
	\[ \lambda(\limsup_n A_n) = \frac{3}{4},\qquad \lambda(\liminf_n A_n) = \frac{1}{4}. \]
	Furthermore
	\[ \limsup_n\lambda(A_n) = \frac{3}{4},\quad \liminf_n\lambda(A_n) = \frac{1}{4}, \]
	Which satisfies the requirements.
\end{solution}

\begin{problem}
	Let $ A_1,A_2,\cdots, B_1,B_2,\cdots $ be events.
	\begin{enumerate}[(a)]
		\item Prove that 
		\[ (\limsup A_n) \cap (\limsup_n B_n) \supseteq \limsup_n (A_n\cap B_n). \]
		\item Give an example where the above inclusion is strict, and another example where it holds with equality.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ \omega \in \limsup_n (A_n \cap B_n) $. Then $ \forall N > 0 $ there exists $ n > N $ such that $ \omega \in A_n\cap B_n $. By definition this implies that $ \omega \in (\limsup_n A_n \cap \limsup_n B_n) $, and this completes the proof.
		
		\item Fix $ \omega \in \Omega $. Let $ E $ be any set that contains $ \omega $. Let $ A_{2n} = E $ and $ A_{2n+1} = E^c $ for all $ n \in \N $. Furthermore let $ B_{2n} = E^c $ while $ B_{2n+1} = E $. By this construction we have
		\[ \set{\omega} \in \limsup_n A_n, \quad \set{\omega} \in \limsup_n B_n,\quad \text{thus}\quad \set{\omega} \in (\limsup_n A_n) \cap (\limsup_n B_n). \]
		However, since $ A_n \cap B_n = \emptyset $ for all $ n $ we have
		\[ \omega \notin \limsup_n (A_n \cap B_n), \]
		which shows the strict inequality of the inequality we proved in (a).
		
		To show an example which which the equality works, let $ E\subset\Omega $ by any subset. Define $ B_{bn} = A_{an} = E $ where $ (a,b)=1 $ (i.e. are relatively prime), and $ \emptyset $ otherwise. Then any $ \omega \in \limsup_n A_n $ will belong to one $ A_i $ every $ a $ sets, and by design will belong to $ B_i $ every $ b $ sets. Since $ (a,b)=1 $, then these sets will be the same infinitely often, hence $ \omega \in \limsup(A_n\cap B_n) $ as well.
	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ A_1,A_2,\cdots $ be a sequence of events, and let $ N \in \N $. Suppose there are events $ B,C $ such that $ B\subseteq A_n \subseteq C $ for all $ n\geq N $, and such that $ \prob(B) = \prob(C) $. Prove that $ \prob(\liminf_n A_n) = \prob(\limsup_n A_n) = \prob(B) = \prob(C) $.
\end{problem}
\begin{solution}
	We claim 
	\[ B \subseteq \limsup_n A_n \subseteq C, \qquad B \subseteq \liminf_n A_n \subseteq C. \]
	To show the first statement, let $ \omega \in B $. Then since for all $ n $ large enough we have $ B \subseteq A_n \subseteq C $, we have $ B \in A_n $, thus $ B \subseteq \limsup_n A_n $. Furthermore, let $ \omega \in \limsup_n A_n $. Then for all $ N $ we can find $ n>N $ such that $ \omega\in A_n $. By hypothesis $ \omega\in C $. Thus $ \limsup_n A_n \subseteq C $.
	
	\noindent To show the second statement, let $ \omega \in C $. Then since for all $ n $ large enough $ B \subseteq A_n \subseteq C $ we see that $ B\subseteq \liminf_n A_n \subseteq C $. By the monotonicity of the probability we have
	\[ \prob(B) \leq \prob(\limsup_n A_n) \leq \prob(C) ,\qquad \prob(B)\leq \prob(\liminf_n A_n)\leq \prob(C).\]
	Since $ \prob(B) = \prob(C) $ then we conclude that 
	\[ \prob(\limsup_n A_n) = \prob(\liminf_n A_n) = \prob(B) = \prob(C). \]
\end{solution}


\begin{problem}
	Let $ \set{X_n} $ be independent random variables, with $ \prob(X_n = i) = 1/n $ for $ i=1,2,3,\cdots, n $. Compute $ \prob(X_n = 5\ i.o.) $, the probability that an infinite number of the $ X_n $ are equal to $ 5 $.
\end{problem}
\begin{solution}
	Let $ A_n = \set{X_n = 5} $. Then by hypothesis $ \prob(A_n) = 0 $ for $ n < 5 $ and $ \prob(A_n) = 1/n $ for $ n \geq 5 $. Thus $ \sum_n \prob(A_n) = \infty $. As the random variables $ X_n $ are all independent, so is the events $ A_n $. Thus using the Borel-Cantelli Lemma we conclude that $ \prob(A_n\ i.o.) = 1 $.
\end{solution}


\begin{problem}
	Let $ X $ be a random variable with $ \prob(X>0)>0 $. Prove that there exists $ \delta>0 $ such that $ \prob(X \geq \delta) > 0 $. (\emph{Hint: Use the continuity of the probability function})
\end{problem}
\begin{solution}
	Let $ A = \set{X>0} $, and consider the events $ A_n = \set{X > 1/n} $. Observe that $ \set{A_n}\nearrow A $. The sequence of real numbers $ \set{\prob(A_n)} $ is an increasing sequence (by the monotonicity) and is converging to $ \prob(A) $ (by the continuity). Let $ \epsilon = \prob(A)/2 $. Then by the definition of the convergence of real numbers for all $ N>0 $ we have $ \epsilon < \prob(A_n) $ for all $ n>N $. Let $ \delta = 1/N $. This completes the proof.
\end{solution}


\begin{problem}
	Let $ X_1,X_2,\cdots $ be defined jointly on some probability space $ (\Omega,\mathcal{F},\mathcal{P}) $, with $ \sum_{i=1}^{\infty}i^2\prob(i \leq X_n < i+1) \leq C <\infty $ for all $ n $. Prove that $ \prob(X_n \geq n\ i.o.) = 0 $.
\end{problem}
\begin{solution}
	Observe that
	\begin{align*}
		C &\geq \sum_{i=1}^{\infty}i^2\prob(i\leq X_n < i+1) \\
		&\geq \sum_{i=n}^{\infty}i^2\prob(i\leq X_n < i+1) \\
		&\geq n^2 \sum_{i=n}^{\infty}\prob(i\leq X_n<i+1) \\
		&= n^2 \prob(X_n \geq n).
	\end{align*}
	Thus we have $ \prob(X_n\geq n) \leq C/n^2 $. Since $ C/n^2 $ is summable, and the probability function is positive, then $ \prob(X_n \geq n) $ is also summable. I.e.
	\[ \sum_n \prob(X_n \geq n) < \infty. \]
	Using Borel-Cantelli lemma we then have
	\[ \prob(X_n \geq n\ i.o.) = 0. \]
\end{solution}


\begin{problem}
	Let $ \delta,\epsilon>0 $, and let $ X_1,X_2,\cdots $ be a sequence of independent non-negative random variables such that $ \prob(X_i \geq \delta) \geq \epsilon $ for all $ i $. Prove that with probability one, $ \sum_{i=1}^{\infty}X_i = \infty $. I.e. $ \prob(\sum_{i=1}^{\infty}X_i = \infty) = 1 $.
\end{problem}
\begin{solution}
	Let $ A_n = \set{X_n \geq \delta} $. Since $ \prob(A_n)\geq\epsilon $ for all $ n $, then 
	\[ \sum_n \prob(A_n) = \infty. \]
	Since the random variables $ X_1,X_2,\cdots $ are independent, the events $ A_1,A_2,\cdots $ are also independent. Thus by Borel-Cantelli we have
	\[ \prob(A_n\ i.o.) = 1. \]
	We claim
	\[ \limsup_n A_n \subseteq \set{\sum_n X_n = \infty}. \]
	To see this let $ \omega\in \limsup_n A_n $. Then it means that $ \forall m > 0 $ there exists $ n_m > m $ such that $ \omega \in A_{n_m} $, or equivalently $ X_{n_m}(\omega) \geq \epsilon $. Considering the subsequence $ \set{X_{n_m}} $ we see that $ \sum_m X_{n_m}(\omega) = \infty $. Thus the inclusion above holds. By the monotonicity of the probability and using the result above 
	\[ 1 = \prob(\limsup_n A_n) \leq \prob(\set{\sum_n X_n = \infty}). \]
	Thus we conclude that 
	\[ \prob(\set{\sum_n X_n = \infty}) = 1. \]
\end{solution}

\begin{problem}
	Consider infinite, independent, fair coin tossing, and let $ H_n $ be the event that the $ n^\text{th} $ coin is heads. Determine the following probabilities.
	\begin{enumerate}[(a)]
		\item $ \prob(H_{n+1}\cap H_{n+2} \cap \cdots\cap H_{n+9}\ i.o.) $
		\item $ \prob(H_{n+1}\cap H_{n+2}\cap \cdots\cap H_{2n}\ i.o.) $
		\item $ \prob(H_{n+1}\cap H_{n+2}\cap \cdots \cap H_{n+2\log_2 n}\ i.o.) $
		\item Prove that $ \prob(H_{n+1}\cap H_{n+2}\cap\cdots\cap H_{n+\log_2n}\ i.o.) $ must equal either 0 or 1.
		\item Determine $ \prob(H_{n+1}\cap H_{n+2}\cap \cdots \cap H_{n+\log_2n} i.o.) $
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ A_0 = H_1\cap\dots\cap H_{9}, A_1 = H_2\cap\cdots\cap H_{10}, A_2 = H_3\cap\cdots\cap H_{11}, A_3 = H_4\cap\cdots\cap H_{12} $, and so on. Then $ A_1,A_2,\cdots $ are not necessarily independent, but there is a subsequence $ A_0,A_{10},A_{20},\cdots $ that are independent. Observe that $ \prob(A_n) = 1/2^{10} $. Thus 
		\[ \infty = A_0+A_{10}+A_{20}+\cdots \leq \sum_n \prob(A_n)  \]
		 Thus $ \prob(A_{10k}\ i.o.) = 1 $. Since $ \set{A_{10k}\ i.o.} \subseteq \set{A_{k}\ i.o.} $, by monotonicity of probability $ \prob(A_{10k}\ i.o.)\leq\prob(A_k\ i.o.) $. This implies that $ \prob(A_k\ i.o.) = 1 $, i.e. \[ \prob(H_{n+1}\cap\cdots\cap H_{n+9}\ i.o.) = 1. \]
		 \item Observe that $ \prob(H_{n+1}\cap\cdots\cap H_{2n}) = \frac{1}{2^n} $, which is summable.
		 \[ \sum_n \prob(H_{n+1}\cap\cdots\cap H_{2n}) < \infty. \] 
		 This implies that $ \prob(H_{n+1}\cap\cdots\cap H_{2n}) = 0 $.
		 \item The probability $ \prob(H_{n+1}\cap\dots\cap H_{n+2\log_2n}) $ is approximately $ (\frac{1}{2})^{\log_2n^2} = \frac{1}{n^2} $ which is summable. Thus by Borel-Cantelli $ \prob(H_{n+1}\cap\cdots\cap H_{n+2\log_2n}\ i.o.) = 0 $.
		 
		 \item Since $ \set{H_{n+1}\cap\cdots\cap H_{n+\log_2n}} $ is a tail even, then by Kolmogorov zero-one law the probability is either zero or one.
		 
		 \item It is suggestive to write down some of the event explicitly. Let
		 \begin{align*}
		 	&A_2 = H_3,\ A_3=H_4,\\
		 	& A_4=H_5\cap H_6,\ \cdots ,\ A_7 = H_8\cap H_9,\\
		 	& A_8=H_9\cap H_{10}\cap H_{11},\ \cdots,\  A_{15}=H_{16}\cap H_{17}\cap H_{18} \\
		 	& A_{16}=H_{17}\cap H_{18}\cap H_{19}\cap H_{20},\ \cdots,\  A_{31} = H_{32}\cap H_{33}\cap H_{34} \cap H_{35},\\
		 	&A_{32}=H_{33}\cap H_{34}\cap H_{35}\cap H_{36}\cap H_{37},\ \cdots,\ A_{63}=H_{64}\cap H_{65}\cap H_{66}\cap H_{67}\cap H_{67}, \\
		 	&\text{and so on.}
		 \end{align*}
		 Note that each $ A_{2^k} $ up to $ A_{2^{k+1}} $ we have probability $ \frac{1}{2^k} $. However, we can roughly find $ 2^k/k $ independent events among them. 
		 Thus 
		 \[ \infty = \sum_k \frac{2^k/k}{2^k} \leq \sum_n \prob(A_n). \]
		 Thus $ \prob(A_n\ i.o.) = 1 $.
	\end{enumerate}
\end{solution}


\section{Expected Values}
\subsection{Solved Problems}
\begin{problem}
	Let $ X $ be a random variable with finite mean, and let $ a\in \R $ be any real number. Prove that 
	\[ \E{\max(X,a)} \geq \max(\E{X},a). \]
	\emph{Hint: Consider separately the cases $ \E{X}\geq a $ and $ \E{X}<a $.}
\end{problem}
\begin{solution}
	First observe that we can write
	$$ X = X \mathds{1}_{X\geq a} + a \mathds{1}_{X<a}. $$
	So by the linearity of the expectation we can write
	\[\E{\max(X,a)} = \E{X\mathds{1}_{X\geq a}} + \E{a\mathds{1}_{X<a}}. \tag{\halfnote}\]
	For the first term observe that 
	\[ \E{X\mathds{1}_{X\geq a}} \geq \E{a\mathds{1}_{X\geq a}}, \tag{\quarternote} \]
	while for the second term
	\[ \E{a\mathds{1}_{X< a}} \geq \E{X\mathds{1}_{X< a}}. \tag{\eighthnote}\]
	By ($ \quarternote $) and ($ \halfnote $) we get
	\[ \E{\max(X,a)} \geq \E{a\mathds{1}_{X\geq a}} + \E{a\mathds{1}_{X<a}} = \E{a} = a. \]
	And by ($ \eighthnote $) and ($ \halfnote $)
	\[ \E{\max(X,a)} \geq \E{X\mathds{1}_{X< a}} + \E{X\mathds{1}_{X\geq a}} = \E{X}. \]
	These two equations imply that
	\[ \E{\max(X,a)} \geq \max(\E{X},a). \]
\end{solution}




\section{Inequalities and Convergence}
\begin{proposition}[Markov's inequality]
	Let $ X $ be a \emph{non-negative} random variable. Let $ \alpha > 0 $, then
	\[ \prob(X\geq \alpha)\leq \frac{\E{X}}{\alpha} \]
\end{proposition}

\begin{remark}
	Note that the random variable being \emph{non-negative} is the key for the inequality to hold.
\end{remark}

\begin{definition}[Almost Surely Convergence]
	Let $ X_1,X_2,\cdots $ be a sequence of random variables. Then we say $ \set{X_n} $ is converging to the random variable $ X $ \emph{almost surely} if
	\[ \prob(X_n \to X) = 1, \]
	where the arrow notation show a point-wise convergence. 
\end{definition}
\begin{remark}
	The almost sure convergence is when we have a point-wise convergence on a set of measure 1.
\end{remark}

\begin{definition}[Convergence in probability]
	\label{def:convergInProb}
	Let $ X,X_1,X_2,\cdots $ be random variables. We say $ X_n $ converges to $ X $ in \emph{probability} if for all $ \epsilon>0 $ we have
	\[ \prob(\abs{X_n - X} \geq \epsilon) \to 0 \quad \text{as }  n\to\infty . \]
\end{definition}

\begin{observation}
	For a given $ \epsilon $, the sequence of reals formed for each $ n $ by
	\[ \prob(\abs{X_n - X} \geq \epsilon) \]
	is very important. Its convergence (and the rate of convergence up to the summability of the sequence) will draw lines between the almost sure convergence and the convergence in probability. To The following proposition and corollary will make this more clear. 
\end{observation}


The idea of the proof of the following Lemma is very important and will show up again in the future.
\begin{lemma}
	\label{prop:infiniteOftenThenAlmostSurely}
	Let $ \set{X_n} $ be a sequence of random variables and $ X $ a random variable. If $ \forall \epsilon>0 $ we have
	\[ \prob(\abs{X_n-X} \geq \epsilon\ i.o.) = 1, \]
	then 
	\[ \prob(X_n \to X) = 1, \]
	i.e. $ X_n $ converges to $ X $ almost surely.
\end{lemma}
\begin{proof}
	Consider
	\[ \prob(X_n\to X) = \prob(\bigcup_{r=1}^\infty \set{\abs{X_n-X}\leq \epsilon\ a.a.}) = 1 - \prob(\bigcup_{r=1}^\infty \set{\abs{X_n-X}\leq \epsilon\ a.a.} ). \]
	From the countable sub-additivity of the probability measure we know that
	\[ \prob(\bigcup_{r=1}^\infty \set{\abs{X_n-X}\leq \epsilon\ a.a.} ) \leq \sum_{r=1}^\infty\prob(\set{\abs{X_n-X}\leq \epsilon\ a.a.}) = 0. \]
	Thus
	\[ \prob(X_n\to X) = 1 - \prob(\bigcup_{r=1}^\infty \set{\abs{X_n-X}\leq \epsilon\ a.a.} ) = 1. \]
\end{proof}

\begin{remark}[Important!]
	In the proof above we used the following important fact
	\[ \boxed{\set{X_n \to X} = \bigcup_{r=1}^{\infty} \set{\abs{X_n-X}\leq 1/r\ a.a.}} \]
	or equivalently
	\[ \boxed{\set{X_n \to X} = \bigcup_{r\in\Q^+} \set{\abs{X_n-X}\leq q\ a.a.}} \]
	This is literally the definition of the point wise convergence. To see this first remember that
	\[ \set{\abs{X_n-X}\leq \epsilon\ a.a.} = \bigcap_{N=1}^\infty \bigcup_{n=N}^{\infty}\set{\abs{X_n - X} \leq \infty }. \]
	So if we let $ \omega \in \set{X_n \to X} $ then from the definition of the convergence we know that $ \forall N \in \N $ we can find $ n>N $ such that $ \abs{Z_n(\omega) - Z(\omega)} \leq \epsilon $. I.e. by the definition of union and intersection
	\[ \omega \in \bigcap_{N=1}^\infty \bigcup_{n=N}^{\infty}\set{\abs{X_n - X} \leq \infty. } \]
	To show the converse let $ \omega \in \bigcup_{n=N}^{\infty}\set{\abs{X_n - X} \leq \infty. } $. So $ \forall N \in \N $ there exists $ n\leq N $ such that $ \omega \in \set{\abs{X_n - X}\leq \epsilon} $. I.e. $ \abs{X_n(\omega) - X(\omega)} \leq \epsilon  $
	which is precisely the definition of the point wise convergence $ X_n(\omega) \to X(\omega)$.
\end{remark}

\begin{corollary}
	Let $ \set{X_n} $ be a sequence of random variables and $ X $ be a random variable. Then if for all $ \epsilon>0 $
	\[ \sum_n \prob(\abs{X_n-X}\geq \epsilon) < \infty, \]
	then $ X_n $ converges to $ X $ almost surely.
\end{corollary}
\begin{proof}
	Apply Borel-Cantelli along with \autoref{prop:infiniteOftenThenAlmostSurely}.
\end{proof}

\begin{summary}
	\label{sum:ConvergenceInPropVsAlmsotSure}
	Let $ X,X_1,X_2,\cdots $ be random variables. Consider the sequence of reals given by
	\[ a_n =  \prob(\abs{X_n - X}\geq \epsilon). \]
	If for any $ \epsilon>0 $ we have
	\[ a_n \to 0 \]
	as $ n\to\infty $ then by \autoref{def:convergInProb} $ X_n $ converges to $ X $ is probability. However, if the sequence is also summable for any $ \epsilon $, i.e.
	\[ \sum_n a_n < \infty, \]
	then $ X_n $ converges to $ X $ almost surely.
\end{summary}





\subsection{Solved Problems}
\begin{problem}
	Give an example of a random variable $ X $ and $ \alpha > 0 $ such that $ \prob(X\geq\alpha) > \E{X}/\alpha $. (\emph{Hint: Obviously $ X $ can not be non-negative}). Where does the proof of the Markov inequality break down in this case?
\end{problem}
\begin{solution}
	Consider $ (\Omega,\mathcal{F},\prob) $ where $ \Omega = \set{1,2,3,4} $ with uniform probability measure $ \prob $, and $ \mathcal{F} = 2^\Omega $. Define the random variable $ X: \Omega \to \R$ as 
	\[ X(1) = -1,\ X(2)=-2,\ X(3)=3,\ X(4)=4. \]
	It is easy to calculate
	\[ \E{X} = (-1 -2 + 3 + 4) \cdot \frac{1}{4} = 1.  \]
	However
	\[ \prob(X\geq 3) = \prob(X = 3) + \prob(X=4) = \frac{1}{2}. \]
	It is clear that the Markov inequality does not hold.
\end{solution}

\begin{problem}
	Suppose $ X $ is a non-negative random variable with $ \E{X} = \infty $. What does Markov's inequality can say in this case?
\end{problem}
\begin{solution}
	Then the Markov's inequality will be trivially true,
	\[ \prob(X\geq\alpha) \leq \infty. \]
\end{solution}

\begin{problem}
	For general jointly defined random variables $ X $ and $ Y $ prove that $ \abs{\Corr(X,Y)}\leq 1 $. (\emph{Hint: Don't forget the Cauchy-Schwartz inequality}.)
\end{problem}
\begin{solution}
	Recall the formula for $ \Corr $
	\[ \Corr(X,Y) = \frac{\operatorname{Cov}(X,Y)}{\sqrt{\Var(X),\Var(Y)}}. \]
	Note that $ \Var(X) = \operatorname{Cov}(X,X) $, and in general
	\[ \operatorname{Cov}(X,Y)=\E{(X-\mu_X)(Y-\mu_Y)}. \]
	From C-S inequality we have
	\[\abs{ \operatorname{Cov}(X,Y)} = \E{\abs{(X-\mu_X)(Y-\mu_Y)}} \leq \sqrt{\E{(X-\mu_X)^2}\E{(Y-\mu_Y)^2}} = \sqrt{\Var(X) \Var(Y)}. \]
	Thus it immediately follows that 
	\[ \abs{\Corr(X,Y)} \leq 1. \]
\end{solution}
\begin{remark}
	Note the similarities between $ \Corr $ can $ \cos $ as defined for vectors
	\[ \cos(\theta) = \frac{a \cdot b}{\sqrt{(a\cdot a)(b\cdot b)}}. \]
	for which we also have
	\[ \abs{\cos\theta} \leq 1. \]
\end{remark}

\begin{problem}
	Let $ \phi(x) = x^2 $.
	\begin{enumerate}[(a)]
		\item Prove that $ \phi $ is a convex function.
		\item What does Jensen's inequality say for this choice of $ \phi $?
		\item Where in the text have we already see the result of part (b)?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item This follows from 
		\begin{align*}
			\left(\lambda a^2 + (1-\lambda)b^2\right) - \left(\lambda a + (1-\lambda)b\right)^2 &= \lambda a^2 + (1-\lambda)b^2 - \lambda^2a^2 - (1-\lambda)^2b^2 - 2ab\lambda(1-\lambda)\\
			&= \lambda a^2 (1-\lambda) + (1-\lambda)b^2 (1-(1-\lambda)) - 2ab\lambda(1-\lambda) \\
			&= \lambda(1-\lambda)(a^2+b^2 - 2ab) = \lambda(1-\lambda)(b-a)^2 \geq 0.
		\end{align*}
		Thus it follows that
		$$
		\left(\lambda a^2 + (1-\lambda)b^2\right) \geq \left(\lambda a + (1-\lambda)b\right)^2
		$$
		
		\item Jensen's inequality with this choice of $ \phi $ will result in 
		\[ \E{X}^2 \leq \E{X^2}. \]
		
		\item From above and using the definition of variance we have
		\[ \Var(X) = \E{X^2} - \E{X}^2 \geq 0. \]
		Thus this implies that $ \Var(X) $ is always positive.

	\end{enumerate}
\end{solution}

\begin{problem}
	Let $ X_1,X_2,\cdots $ be a sequence of random variables, with $ \E{X_n} = 8 $ and $ \Var(X_n) = 1/\sqrt{n} $
\end{problem}
\begin{solution}
	Using Chebychev's inequality we can write
	\[ a_n = \prob(\abs{X_n - X}\geq \epsilon) \leq \frac{\Var(X_n)}{\epsilon^2} = \frac{1}{\sqrt{n}\epsilon^2}.  \]
	We see that for any choice of $ \epsilon $ the sequence $ a_n \to 0 $ as $ n\to\infty $. Thus by definition the sequence converges to $ 0 $ is probability.
\end{solution}
\begin{remark}
	Observe that the sequence $ a_n $ is not summable for any choice of $ \epsilon>0 $. Thus by \autoref{sum:ConvergenceInPropVsAlmsotSure} we see that $ X_n $ does \emph{not} converge to $ X $ almost surely.
\end{remark}

\begin{problem}
	Give (with proof) an example of two discrete random variables having the same mean and the same variance, but which are not identically distributed.
\end{problem}
\begin{solution}
	We demonstrate this by giving an explicit example. Let $ (\Omega,\mathcal{F},\prob) $ where $ \Omega = \set{1,2,3,4,5,6} $, $ \mathcal{F} = 2^\Omega $, and
	\[ \prob(\set{1})=\prob(\set{2})=\frac{4}{20},\quad \prob(\set{3})=\prob(\set{4})=\prob(\set{5})=\prob(\set{6})=\frac{3}{20}. \]
	Define the random variables $ X,Y $ as
	\[ X(1)=-2,\ X(2)=2,\ X(3)=0,\ X(4)=0,\ X(5)=-1,\ X(6)=1, \]
	and
	\[ Y(1)=-1,\ Y(2)=1,\ Y(3)=-2,\ Y(4)=2,\ Y(5)=-1,\ Y(6)=1. \]
	It is easy to check that
	\[ \E{X} = 0, \qquad \E{Y} = 0. \]
	And
	\[ \Var(X) = \frac{38}{20},\qquad \Var(Y) = \frac{38}{20}. \]
	Consider the following permutation
	\[ 
	\sigma = \begin{pmatrix}
		-2 & -1 & 0 & 1 & 2 \\
		0 & 1 & -2 & 2 & -1
	\end{pmatrix},
	 \]
	 and define the function $ f $ be the extension of $ \sigma $ on $ \R $ where it assume the value 0 for all points in its domain other than $ \set{-2,-1,0,1,2} $. The we will have
	 \[ f(X(1))=0,\ f(X(2))=-1,\ f(X(3))=-2,\ f(X(4))=-2,\ f(X(5))=1,\ f(X(6))=2, \]
	 and
	 \[ f(Y(1))=1,\ f(Y(2))=2,\ f(Y(3))=0,\ f(Y(4))=-1,\ f(Y(5))=1,\ f(Y(6))=2. \]
	 It is easy to calculate
	 \[ \E{f(X)} = \frac{-7}{20},\qquad \E{f(Y)} = \frac{18}{20}. \]
	 The following diagram summarizes the whole idea!
	 \input{Images/masstableDiagram.tex}
	 Note that we can also find other Borel measurable function. For instance you can figure out a continuous function that behaves differently on positive numbers, vs. negative numbers.
\end{solution}

\begin{problem}
	Prove that if $ \set{Z_n} $ converges to $ X $ almost surely, then for each $ \epsilon>0 $ we have $ \prob(\abs{Z_n - Z}\geq \epsilon\ i.o.) = 0 $.
\end{problem}
\begin{solution}
	We start with
	\begin{align*}
		1 = \prob(Z_n\to Z) &= \prob(\bigcap_{q\in\Q^+}\set{\abs{Z_n-Z}<q\ a.a.}) \\
		&= 1 - \prob(\bigcup_{q\in\Q^+}\set{\abs{Z_n-Z}\geqq\ i.o.})
	\end{align*}
	Thus
	\[ \prob(\bigcup_{q\in\Q^+}\set{\abs{Z_n-Z}\geq\ i.o.}) = 0. \]
	Observe that for all $ \epsilon>0 $
	\[ \set{\abs{Z_n-Z}\geq\epsilon\ i.o.} \subseteq \bigcup_{q\in\Q}\set{\abs{Z_n-Z}\geq q\ i.o.}. \]
	Thus by monotonicity of the probability function we will have
	\[ \prob(\set{\abs{Z_n-Z}\geq\epsilon\ i.o.}) \leq \prob(\bigcup_{q\in\Q}\set{\abs{Z_n-Z}\geq q\ i.o.}) = 0. \]
	So
	\[  \prob(\set{\abs{Z_n-Z}\geq\epsilon\ i.o.}) = 0. \]
\end{solution}


\begin{problem}
	Let $ X_1,X_2,\cdots $ be a sequence of random variables that are independent. Then prove that \[ X_n \to X \ a.s. \implies \sum_n \prob(\abs{X_n-X}>\epsilon) < \infty. \]
\end{problem}
\begin{solution}
	We do by the proof by contrapositive. 
	We want to proof 
	$$ \exists \epsilon>0 \quad \sum_{n} \mathbb{P}(|X_n-X|>\epsilon) = \infty \implies P(X_n\to X) = 0 $$
	
	Since $X_n$'s are independent, then by Borel-Cantelli we have
	\[ \mathbb{P}(|X_n-X|>\epsilon\ i.o.) = 1. \tag{1}. \]
	This implies 
	\[ \mathbb{P}(\bigcup_{q\in \mathbb{Q}^+} \{|X_n-X|>q\ i.o.\}) = 1. \tag{2} \]
	(if this is not clear for you see the remark below)
	
	Thus
	$$ \mathbb{P}(\bigcap_{r\in\mathbb{Q}^+}\{|X_n-X|\leq q\ a.a.\}) = 0. $$
	
	which is equivalent to 
	$$ \mathbb{P}(X_n \to X) = 0. \qed $$
\end{solution}
\begin{remark}
	To see why (1) implies (2) see the below:
	$$\{|X_n-X|>\epsilon\ i.o.\} \subseteq \bigcup_{q\in \mathbb{Q}} \{|X_n-X|>q\ i.o.\}.$$
	By monotonicity of the probability measure
	$$ 1 = \mathbb{P}(\{|X_n-X|>\epsilon\ i.o.\}) \leq \mathbb{P}(\bigcup_{q\in \mathbb{Q}^+} \{|X_n-X|>q\ i.o.\}).$$
	Thus 
	$$ \mathbb{P}(\bigcup_{q\in \mathbb{Q}^+} \{|X_n-X|>q\ i.o.\}) = 1. $$
\end{remark}

\begin{problem}
	Let $ X_1,X_2,\cdots $ be a sequence of independent random variable with $ \prob(X_n=3^n) = \prob(X_n = 3^{-n}) = \frac{1}{2} $. Let $ S_n = X_1+\cdots+X_n $.
	\begin{enumerate}[(a)]
		\item Compute $ \E{X_n} $ for each $ n $.
		\item For $ n \in \N $, compute $ R_n $ defined as
		\[ R_n = \sup\set{r\in\R : \prob(\abs{S_n}\geq r) = 1}, \]
		i.e. the largest number such that $ \abs{S_n} $ is always at least $ R_n $.
		\item Compute $ \lim_{n\to\infty}R_n/n $.
		\item For which $ \epsilon>0 $ (if any) is it the case that $ \prob(\frac{1}{n}\abs{S_n}\geq \epsilon)\not\to 0? $
		\item Why does this result not contradict the various laws of large numbers?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Observe that $ X_n $ is a simple random variable, thus
		\[ \E{X_n} = \frac{1}{2}3^n - \frac{1}{2}3^{n} = 0. \]
		
		\item The value of $ R_n $ equal to the case where for $ S_n $ all the random variables $ X_1,X_2,\cdots, X_{n-1} $ assume their lowest value i.e. $ -3,-9,\cdots,-3^{n-1} $ respectively, and the random variable $ X_n $ assumes its largest value, i.e. $ 3^n $.
		So we will have
		\[ S_n = 3^n - (3+9+\cdots+3^{n-1}) = 3^n - \frac{3^n-3}{2} = \frac{3^n+3}{2}. \]
		
		\item From what we had in part (b) we can see that
		\[ \lim_{n\to\infty}\frac{R_n}{n} = \lim_{n\to\infty}\frac{3^n + 3}{2n} = \infty. \]
		
		\item There is no $ \epsilon>0 $ that works. That is because we know that $ \abs{S_n} \geq R_n $ almost everywhere. On the other hand, in part (b) we observed that $ R_n/n \to \infty$ as $ n\to\infty $. So for all $ \epsilon>0 $ the sequence of reals $ \prob(\abs{S_n}/n\geq \epsilon) $ will converge to zero.
		
		\item This does not contradicts any of the law of large number. That is because the variance of $ X_n $ is
		\[ \Var(X_n) = \E{X^2} - \E{X}^2 = 3^n, \]
		which does not remain finite as $ n\to\infty $.
	\end{enumerate}
\end{solution}

\begin{problem}
	Suppose $ \E{2^X}=4 $. Prove that $ \prob(X\geq 3)\leq 1/2 $.
\end{problem}
\begin{solution}[The first method]
	We can start with
	\[ \E{2^X} = \E{2^X\mathds{1}_{X\geq3}} + \E{2^X\mathds{1}_{X<3}} = 4. \]
	For the first and the second terms we have the following bounds
	\[ \E{2^X\mathds{1}_{X\geq3}} \geq 8\prob(X\geq 3), \]
	So 
	\[ 8\prob(X\geq 3) \leq \E{2^X\mathds{1}_{X\geq 3}} = 4 - \E{2^X\mathds{1}_{X<3}}. \]
	This implies that 
	\[ \prob(X\geq 3) \leq \frac{1}{2} - \E{2^X\mathds{1}_{X<3}} \leq \frac{1}{2}, \]
	where we have used the fact that the expectation of a non-negative random variable is positive.
\end{solution}
\begin{remark}[Connection to the conditional expectation]
	If one has already used the conditional expectation before, then they can recover the definition of it using the answer above. 
	\[ \E{X\mathds{1}_B} = \E{X|B} \E{\mathds{1}_B} = \E{X|B}\prob(B).  \]
	In fact the conditional expectation is a way to factor the expectation when the random variable is of the form $ X\mathds{1}_A $.
\end{remark}
\begin{solution}[The second method]
	Define the random variable
	\[ Y = 2^X. \]
	Thus $ X = \log(Y) $, where $ \log $ is considered to be base 2 logarithm function. Thus
	\[ \prob(X\geq 3) = \prob(\log(Y)\geq 3) = \prob(Y\geq 9) \leq \frac{\E{Y}}{9} \frac{4}{9}\leq \frac{4}{8} = \frac{1}{2}, \]
	where we have used the Markov's inequality. Note that the random variable $ Y $ is non-negative and that is the reason we could use the Markov's inequality.
\end{solution}

\begin{problem}
	Give examples of random variables $ Y $ with mean $ 0 $ and variance $ 1 $ such that
	\begin{enumerate}[(a)]
		\item $ \prob(\abs{Y}\geq 2) = 1/4 $.
		\item $ \prob(\abs{Y}\geq 2) < 1/4 $.
	\end{enumerate}
\end{problem}
\begin{solution}
	For all of the examples below let $ ([0,1],\mathbb{B},\lambda) $ be the probability space.
	\begin{enumerate}[(a)]
		\item An easy example is
		\[ X = 2\mathds{1}_{[0,1/8)} -2\mathds{1}_{(1/8,2/8]}. \]
		The graph of this random variable would look like as the following.
		\input{Images/graphOfRandomVariable1.tex}
		\FloatBarrier
		\item The following example is a straightforward one.
		\[ X = 2\mathds{1}_{[0,1/16)} - 2\mathds{1}_{[1/16,2/16)} + \mathds{1}_{(2/16,6/16]} - \mathds{1}_{(6/16,10/16]}. \]
		\input{Images/graphOfRandomVariable2.tex}
	\end{enumerate}
\end{solution}
\begin{remark}
	Note that from the Chebychev inequality we have
	\[ \prob(\abs{Y - \mu_Y}\geq \alpha) \leq \frac{\Var(Y)}{\alpha^2}. \]
	The problem above demonstrates the cases where the equality holds, and the strict inequality holds.
\end{remark}

\begin{problem}
	Suppose $ Y $ is a random variable with finite mean $ \mu_Y $ and $ \Var(Y)=\infty $. What does Chebychev's inequality say in this case?
\end{problem}
\begin{solution}
	The Chebychev's inequality states that for any random variable $ X $ we have
	\[ \prob(\abs{X - \mu_X} \geq \alpha) \leq \frac{\Var(X)}{\alpha^2}. \]
	When $ \Var(X) = \infty $, then the Chebychev's inequality will be the trivial inequality
	\[ \prob(\abs{X - \mu_X}\geq \alpha) \leq \infty. \]
\end{solution}


\begin{problem}
	Give (with proof) an example of a sequence $ \set{Y_n} $ of jointly defined random variables, such that as $ n\to\infty $ we have all of three
	\begin{enumerate}[(i)]
		\item $ Y_n/n $ converges to $ 0 $ in probability.
		\item $ Y_n/n^2 $ converges to $ 0 $ with probability 1.
		\item $ Y_n/n $ does \emph{not} converge to 0 with probability 1.
	\end{enumerate}
\end{problem}
\begin{solution}
	Let $ ([0,1],\mathcal{B},\lambda) $ be the probability space, and define the sequence of random variables $ \set{Y_n} $ given as
	\[ Y_n = n\mathds{1}_{A_n}, \]
	where $ A_n \in \mathcal{B} $ with $ \prob(A_n) = 1/n $. Then we will have
	\[ Y_n/n = \mathds{1}_{A_n},\qquad Y_n/n^2 = \mathds{1}_{A_n}/n. \]
	Then
	\[ \prob(\abs{Y_n/n}\geq \epsilon) = 
	\begin{cases}
		0 \qquad \epsilon > 0\\
		1/n \qquad 0<\epsilon\leq 1
	\end{cases}, \qquad
	\prob(\abs{Y_n/n^2}\geq \epsilon) = 0 \qquad \text{for $ n $ large enough.}
	 \]
	 Thus we can see that $ Y_n/n $ converges to 0 in probability, $ Y_n/n $ does not converge to 0 with probability one (as $ \prob(\abs{Y_n/n}\geq \epsilon) $ is not summable). Lastly, $ Y_n/n^2 $ converges to 0 with probability 1.
\end{solution}


\begin{problem}
	Let $ r\in \N $. Let $ X_1,X_2,\cdots $ be independently distributed random variables having finite mean $ m $, which are $ r\text{-dependent} $, i.e. such that $ X_{k_1},X_{k_2},\dots,X_{k_j} $ are independent whenever $ k_{i+1}>k_i + r $ for each $ i $ (thus independent random variables are 0-dependent). Prove that with probability one $ \frac{1}{n}\sum_{i=1}^{n}X_i \to m $ as $ n\to\infty $. (\emph{Hint: break up the sum into $ r+1 $ different sums.})
\end{problem}
\begin{solution}
	Let $ S_n = \frac{1}{n}\sum_{i=1}^{n}X_i $ represent the partial sums of the infinite sum. Let $ R = r+1 $. We are interested in the subsequence terms of the form $ S_{Rn} $. These terms can be written as
	\begin{align*}
		S_{nR} &= \frac{1}{R}\cdot\frac{1}{n}(X_1+X_{1+R}+X_{1+2R}+\cdots + X_{1+(n-1)R})\\
		&+ \frac{1}{R}\cdot\frac{1}{n}(X_2+X_{2+R}+X_{2+2R}+\cdots + X_{2+(n-1)R}) \\
		&\vdots \\
		&+ \frac{1}{R}\cdot\frac{1}{n}(X_R+X_{R+R}+X_{R+2R}+\cdots + X_{R+(n-1)R}).
	\end{align*}
	Let
	\[ Y_{i,n} = \frac{1}{R}\cdot\frac{1}{n}(X_i+X_{i+R}+X_{i+2R}+\cdots + X_{i+(n-1)R}). \]
	Note that the terms in each $ Y_{i,n} $ are i.i.d. random variables by the hypothesis and all of them have mean $ n $, thus by the SLLN we have
	\[ \prob(Y_{i,n}\to m) = 1 \qquad \forall i\in \N. \]
	On the other hand, the decomposition of the sum above we can write
	\[ S_{nR} = \frac{1}{R}\sum_{i=1}^{R}Y_{{i,n}}. \]
	Each of the terms in the RHS goes to $ m $ with probability one, thus the LHS goes to $ Rm/R = m $ with probability 1.  
\end{solution}
\begin{remark}
	This is a very clear generalization of the notion of the independent random variables. In this general notion the random variables are $ r\text{-dependent} $ if they are independent whenever they are $ r $ terms away from each other. This notion will be very useful in considering the systems who have an exponentially decaying memory through time.
\end{remark}



\section{Distribution of Random Variables}

\begin{proposition}[Cumulative distribution function is right-continuous]
	Let $ X $ be a random variable whose cumulative distribution function is defined as $ F_X(x) = \prob(X \leq x) $. Then $ F_X(x) $ is right-continuous.
\end{proposition}
\begin{proof}
	Fix $ x \in \R $. Let $ \set{x_n} $ be any sequence that $ x_n \to x $ as $ n \to\infty $ and for all $ n $ $ x \leq x_n $. Observe that 
	\[ \bigcap_n (-\infty,x_n] = (-\infty,x], \tag{\eighthnote}\]
	and also 
	\[ (-\infty,x_1] \supseteq (-\infty,x_2] \supseteq (-\infty,x_3] \supseteq \cdots. \tag{\twonotes}\]
	Thus we have $ \set{(-\infty,x_n]}\downarrow (-\infty,x] $ as $ n\to\infty $. Since $ (\eighthnote) $ and $ (\twonotes) $ are both preserved by the pre-image $ \inv{X} $, so we also have
	\[ \set{\inv{X}(-\infty,x_n]}\downarrow \inv{X}((-\infty,x]) \quad \text{as $ n\to\infty $}.\]
	From continuity of probability we have
	\[ \prob(X\leq x_n) \to \prob(X\leq x) \quad \text{as } n \to \infty. \]
	Thus by the definition of cumulative distribution we have
	\[ F_X(x_n) \to F_X(x) \quad \text{as }n\to\infty. \]
\end{proof}
\begin{remark}
	Note that we can not do a similar argument to show that $ F_X(x) $ is left continuous. The obstacle is that if we choose $ \set{x_n} $ such that $ x_n\to x $ and for all $ n $ we have $ x_n \leq x $, then the union
	\[ \bigcup_n(-\infty,x_n] \]
	is not necessarily equal to $ (-\infty,x] $. For instance if we let $ x_n = x - 1/n $ then 
	\[ \bigcup_n(-\infty,x_n] = (-\infty,x). \]
\end{remark}

\begin{proposition}
	Let $ X $ and $ Y $ be two random variables (possibly defined on different probability triples). Then $ \mu_X = \mu_Y $ if and only if $ \E{f(X)} = \E{f(Y)} $ for all Borel-measurable $ f:\R \to \R $ for which either expectation is well-defined.
\end{proposition}
\begin{proof}
	To show the $ \boxed{\Longrightarrow} $, using the change of variable theorem we see that
	\[ \E{f(X)} = \int_\R f d\mu_X = \int_\R f d\mu_Y =  \E{f(Y)}. \]
	To show the $ \boxed{\Longleftarrow} $, let $ B \in \mathcal{B} $, and let $ f = \mathds{1}_B $. Then 
	\[ \E{f(X)} = \E{\mathds{1}_{B}(X)} = \E{\mathds{1}_{X\in B}} = \prob(X\in B) = \mu_X.  \]
	with a similar reasoning we get $ \E{f(Y)} = \mu_Y $. Then since $ \E{f(X)} = \E{f(Y)} $,
	\[ \mu_X = \mu_Y. \]
\end{proof}


\begin{corollary}
	If $ X $ and $ Y $ are random variables with $ \prob(X=Y) = 1 $, then \[ \E{f(X)} = \E{f(Y)} \]
	for all Borel-measurable $ f:\R \to \R $ for which either expectation is well-defined. 
\end{corollary}
\begin{proof}
	We claim that $ \prob(X=Y) = 1 $ implies $ \mu_X = \mu_Y $. To see this let $ B \in \mathcal{B} $. Then
	\begin{align*}
		\mu_X (B) = \prob(\set{X\in B}) &= \prob(\set{X\in B}\cap\set{X=Y} \dot{\cup} \set{X\in B}\cap \set{X\neq Y}) \\
		&=\prob(\set{X\in B}\cap\set{X=Y}) + \prob(\set{X\in B}\cap \set{X\neq Y})
	\end{align*}
	Note that $ \set{X\in B}\cap \set{X \neq Y} \subseteq \set{X \neq Y} $ thus from the monotonicity $ \prob(\set{X\in B}\cap \set{X \neq Y}) \leq \prob(\set{X\neq Y}) = 0 $. Thus
	\[ \mu_X (B) = \prob(\set{X\in B}) = \prob(\set{X\in B}\cap\set{X=Y}) = \prob(\set{Y\in B}\cap\set{X=Y}) = \prob(\set{Y\in B}) = \mu_Y(B). \]
	Now using the proposition above we get that
	\[ \E{f(X)} = \E{f(Y)}. \]
\end{proof}


\subsection{Solved Problems}
\begin{problem}
	Let $ \mu $ have density $ 4x^3\mathds{1}_{0<x<1} $, and let $ \nu $ have density $ x/2\mathds{1}_{0<x<2} $.
	\begin{enumerate}[(a)]
		\item Compute $ \E{X} $ where $ \mathcal{L}(X) = \mu/3 + 2\nu/3 $.
		\item Compute $ \E{Y^2} $ where $ \mathcal{L}(Y) = 1/6\mu + 1/3\delta_2 + 1/2\delta_5 $.
		\item Compute $ \E{Z^3} $ where $ \mathcal{L}(Z) = 1/8\mu+1/8\nu + 1/4\delta_3 + 1/2\delta_4 $.
	\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}[(a)]
		\item Denote $ \mathcal{L}(X) $ with $ \eta $. Then by definition and then using the change of variable principle we have
		\[ \E{X} = \int_\Omega X \prob(d\omega) = \int_\R x \eta(dx) = 1/3\int_\R  x \mu(dx) + 2/3\int_\R x \nu(dx)  \]
		where for the last equality we have used the proposition 6.2.1 Rosenthal. Now using proposition 6.2.3 Rosenthal we can write
		\[ \E{X} = 4/3\int_{0}^{1} x^4 \lambda(dx)  + 1/3\int_{0}^{2} x^2 \lambda(dx)  = 4/15 + 8/9 = 52/45. \]
		where $ \lambda $ is the Lebesgue measure.
		
		\item Similar to the reasoning in part (a), we will get
		\begin{align*}
			\E{X^2} &= 1/6\int_\R x^2 \mu(dx) + 1/3\int_\R x^2 \delta_2(dx) + 1/2\int_R x^2 \delta_5(dx) \\
			&= 2/3 \int_{0}^{1}x^5 \lambda(dx) + 1/3\int_{-\infty}^{+\infty} x^2\delta(x-2)\lambda(dx) + 1/2\int_{-\infty}^{+\infty} x^2\delta(x-5)\lambda(dx) \\
			&= 2/18 + 4/3 + 25/2 = 251/18.
		\end{align*}
		
		\item Similar to the part (a) we will get
		\begin{align*}
			\E{X^3} &= 1/8\int_\R x^3 \mu(dx) + 1/8\int_\R x^3\nu(dx) + 1/4\int_\R x^3 \delta_4(dx) + 1/2\int_\R x^3\delta_4(dx) \\
			&= 1/2\int_{0}^{1}x^6\lambda(dx) + 1/16\int_{0}^{2}x^4\lambda(dx) + 1/4\int_{-\infty}^{+\infty}x^3\delta(x-3)\lambda(dx) + 1/2\int_{-\infty}^{+\infty} x^3 \delta(x-4) \lambda(dx)\\
			&=1/14 + 32/80 + 27/4 + 32.
		\end{align*}
	\end{enumerate}
\end{solution}

\begin{problem}
	Suppose $ \prob(Z=0)=\prob(Z=1)=\frac{1}{2} $, that $ Y\sim \mathcal{N}(0,1),$ and that $ Y $ and $ Z $ are independent. Set $ X=YZ $. What is the law of $ X $?
\end{problem}
\begin{solution}
	Using the conditional probability (see the proposition below), and by letting $ B\in \mathbb{B} $ we can write
	\[ \mu_X(B) = \prob(X\in B) = \prob(YZ\in B) = \prob(0\in B)\prob(Z=0) + \prob(Y\in B)\prob(Z=1) = \frac{\delta_0(B) + \mu_\mathcal{N}(B)}{2}. \]
	Thus the law of $ X $ is given by
	\[ \mu_X = \frac{\delta_0 + \mu_\mathcal{N}}{2}.\]
\end{solution}
\begin{observation}[Conditional probability and conditional expectation]
	The notion of conditional probability and conditional expectation are so natural to work with that one often forgets what is the rigorous basis of these notions. Let $ B,C \in \mathcal{F} $ be two sets that partitions the samples space $ \Omega $. Then for any $ A\in\mathcal{F} $ we can write
	\[ A =( A\cap B) \dot\cup (A \cap C).\]
	Then using the countable additivity of the probability measure we can write
	\[ \prob(A) = \prob(A\cap B) + \prob(A\cap C). \]
	We can then define the notion of the conditional probability that enables us to factor the terms in RHS in some useful sense. Define
	\[ \prob(A|B) = \prob(A\cap B)/\prob(B),\qquad \prob(A|C) = \prob(A\cap C)/\prob(C). \]
	Then we can write
	\[ \boxed{\prob(A) = \prob(A|B)\prob(B) + \prob(A|C)\prob(C).} \]
	We can do the same thing with the notion of the indicator function that is a dual notion for the set theoretic operations we had above. Again, let $ B,C $ defined as above, and let $ X $ be any random variable. Then
	\[ X = X \mathds{1}_B  + X \mathds{1}_C, \]
	where we have used the fact that 
	\[ \mathds{1}_B + \mathds{1}_C = 1 \quad \text{since } \Omega = B \dot\cup C. \]
	Then 
	\[ \E{X} = \E{X\mathds{1}_B} + \E{X\mathds{1}_C}. \]
	Now we define the notion of the conditional expectation that enables us to factor the terms in the RHS. Define
	\begin{align*}
		&\E{X|B} = \E{X\mathds{1}_B}/\E{\mathds{1}_B} = \E{X\mathds{1}_B}/\prob(B),\\ 
		&\E{X|C} = \E{X\mathds{1}_C}/\E{\mathds{1}_C} = \E{X\mathds{1}_C}/\prob(C). 
	\end{align*}
	Then we can write
	\[ \boxed{\E{X} = \E{X|B}\prob(B) + \E{X|C}\prob(C).} \]
\end{observation}

\begin{problem}
	Let $ X\sim \operatorname{Poisson}(5) $.
	\begin{enumerate}[(a)]
		\item Compute $ \E{X} $.
		\item Compute $ \Var(X) $.
		\item Compute $ \E{3^X} $. 
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item To find $ \E{X} $ we can start with
		\begin{align*}
			\E{X} &= \sum_{n=0}^{\infty} n \frac{e^{-\lambda} \lambda^n}{n!} \\
			&=e^{-\lambda} \sum_{n=1}^{\infty} \frac{\lambda^n}{(n-1)!} \\
			&=\lambda e^{-\lambda} \sum_{n=1}^{\infty} \frac{\lambda^{n-1}}{(n-1)!}\\
			&=\lambda e^{-\lambda} \sum_{n=0}^{\infty} \frac{\lambda^{n}}{(n)!}\\
			&=\lambda e^{-\lambda} e^\lambda \\
			&=\lambda,
		\end{align*}
		where we have used the fact that $ e^\lambda = \sum_{n=0}^{\infty}\lambda^n/n!. $
		
		\item To harvest the possible cancellations, we start with
		\begin{align*}
			\E{X(X-1)} &= \sum_{n=1}^{\infty} n(n-1) \frac{e^{-\lambda} \lambda^n}{n!} \\
			&= e^{-\lambda} \sum_{n=2}^{\infty} \frac{\lambda^n}{(n-2)!} \\
			&= \lambda^2 e^{-\lambda} \sum_{n=2}^{\infty} \frac{\lambda^{n-2}}{(n-2)!} \\
			&= \lambda^2 e^{-\lambda} \sum_{n=0}^{\infty} \frac{\lambda^{n}}{n!}\\
			&= \lambda^2 e^{-\lambda} e^\lambda \\
			&= \lambda^2.
		\end{align*}
		Thus $ \E{X(X-1)} = \E{X^2}-\E{X} = \lambda^2 $, which implies
		\[ \E{X^2} = \lambda_2 + \lambda. \]
		Now using the formula for variance we will get
		\[ \Var(X) = \E{X^2} - \E{X}^2 = \lambda^2 + \lambda - \lambda^2 = \lambda. \]
		
		
		\item We can write
		\[ \prob(3^X) = \sum_{n=0}^{\infty} 3^n \prob(X=n) = \sum_{n=0}^{\infty} 3^n \frac{e^{-\lambda}\lambda^n}{n!} = e^{-\lambda} \sum_{n=0}^{\infty} (3\lambda)^n/n! = e^{-\lambda}e^{3\lambda} = e^{2\lambda}. \]
	\end{enumerate}
\end{solution}

\begin{problem}
	Compute $ \E{X},\E{X^2} $ and ere $ \Var(X) $ where the law of $ X $ is given by
	\begin{enumerate}[(a)]
		\item $ \mathcal{L}(X) = \delta_1/2 + \lambda/2, $ where $ \lambda $ is Lebesgue measure on $ [0,1] $.
		\item $ \mathcal{L}(X) = \delta_2/3 + 2\mu_\mathcal{N}/3 $, where $ \mu_\mathcal{N} $ is the standard normal distribution $ \mathcal{N}(0,1) $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item For $ \E{X} $ we can write
		\[ \E{X} = 1/2\int_{[0,1]}x\delta_1(dx) + 1/2\int_{[0,1]}x\lambda(dx) = 1/2\int_{[0,1]} x\delta(x-1)\lambda(dx) + 1/2\int_{[0,1]}x\lambda(dx) = 1/2+1+4 = 3/4.  \]
		Similarly, for $ \E{X^2} $ we can write
		\[ \E{X^2} = 1/2\int_{[0,1]}x^2\delta_1(dx) + 1/2\int_{[0,1]}x^2\lambda(dx) = 1/2\int_{[0,1]} x^2\delta(x-1)\lambda(dx) + 1/2\int_{[0,1]}x^2\lambda(dx) = 1/2 + 1/6 = 2/3. \]
		And using the formula for $ \Var(X) $ we can compute
		\[ \Var(X) = \E{X^2} - \E{X}^2 = 2/3 - 9/16 = 5/48. \]
		\item Similar to part (a), for $ \E{X} $ we have
		\[ \E{X} = 1/3\int_{[0,1]} x\delta_2(dx) + 2/3\int_\R x\mu_\mathcal{N}(dx) = 2/3 + 2/3\underbrace{\int_\R xf(x)\lambda(dx)}_{0} = 2/3, \]
		where $ f(x)=1/2\sqrt{\pi}e^{-x^2/2} $ is the probability density function for the normal distribution. Also note that we have used the fact that mean of the normal distribution is zero. Similarly, for $ \E{X^2} $ we can write
		\[ \E{X^2} = 1/3\int_{[0,1]} x^2\delta_2(dx) + 2/3\int_\R x^2\mu_\mathcal{N}(dx) = 4/3 + 2/3\underbrace{\int_\R x^2f(x) \lambda(dx)}_{1} = 4/3 + 2/3 = 2,  \]
		where we have used the fact that for a random variable $ Z $ with the standard normal distribution $ \Var(Z) = \E{Z^2} = 1 $. And finally
		\[ \Var(X) = \E{X^2} - \E{X}^2 = 2 - 4/9 = 14/9. \] 
	\end{enumerate}

\end{solution}

\begin{problem}
	Let $ X $ and $ X $ be independent, with $ X\sim\mathcal{N}(0,1) $, and with $ \prob(Z=1)=\prob(Z=-1)=1/2 $. Let $ Y = XZ $.
	\begin{enumerate}[(a)]
		\item Prove that $ Y \sim \mathcal{N}(0,1) $.
		\item Prove that $ \prob(\abs{X} = \abs{Y}) = 1 $.
		\item Prove that $ X $ and $ Y $ are not independent.
		\item Prove that $ \Corr(X,Y) = 0 $.
		\item It is sometimes claimed that if $ X $ and $ Y $ are normally distributed random variables with $ \Corr(X,Y)=0 $, then $ X,Y $ are independent. Is this correct?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ B \in \mathcal{B} $. Let $ \mu_Y $ be the distribution of $ Y $. Then
		\[ \mu_Y(B) = \prob(Y \in B) = \prob(X\in B|Z = 1)\prob(Z=1) + \prob(-X\in B| Z=-1)\prob(Z=-1) = \mu_X/2 + \mu_X/2 = \mu_X = \mu_\mathcal{N},  \]
		where we have used the fact that for the random variable $ X \sim \mathcal{N}(0,1)$ we have $ \mu_{X} = \mu_{-X}. $ To see why the random variables $ X $ and $ -X $ have the same law, observe that
		\[ F_{-X}(x) = \prob(-X\leq x) = \prob(X > -x) = 1 - F_X(-x). \]
		On the other hand since
		\[ F_X(t) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{t} e^{-x^2/2} dx \]
		and using the fact
		\[ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{-t} e^{-x^2/2} dx  = \frac{1}{\sqrt{2\pi}}\int_{t}^{\infty} e^{-x^2/2} dx, \]
		we get $ F_{-X}(t) = 1- F_X(t) $, which combining with the expression we found for $ F_{-X}(x) $ above we can see that
		\[ F_{-X}(x) = F_X(x). \]
		By proposition 6.0.2 Rosenthal, we can conclude that $ \mu_X = \mu_{-X} $.
		
		\item[(a')] There is also another easier solution to this. Let $ B\in\mathcal{B} $. Then
		\begin{align*}
			\mu_Y = \prob(Y \in B) &= \prob(XZ \in B) = \frac{1}{2} \prob(XZ \in B | Z = 1) + \frac{1}{2} \prob(XZ \in B | Z = -1) \\
			&= \frac{1}{2} \prob(X \in B) + \frac{1}{2} \prob(X \in \bar{B}),
		\end{align*}
		where $ \bar{B} $ is the conjugate set to $ B $ where $ \bar{B} = \set{-x: x\in B} $. Since $ X $ has standard normal distribution, $ \prob(X\in B) = \prob(X\in\bar{B}) $. This comes from the fact that the density function for $ \mathcal{N} $ is an even function. So
		\[ \prob(Y\in B) = \prob(X\in B). \]
		
		\item We can write
		\begin{align*} 
			\set{\abs{X}=\abs{Y}} = \set{\abs{X} = \abs{Y}}\cap\set{Z=1} \dot\cup \set{\abs{X} = \abs{Y}}\cap\set{Z=-1}
		\end{align*}
		Thus
		\[ \prob(\abs{X}=\abs{Y}) = 1/2\prob(\abs{X}=\abs{X}) + 1/2 \prob(\abs{X} = \abs{-X}) = \prob(\abs{X}=\abs{X}) = 1. \]
		
		\item Let $ B \in \mathcal{B} $. Then since $ \mu_X = \mu_Y $ we have
		\[ \prob(X\in B) = \prob(Y\in B). \]
		On the other hand, 
		\[ \set{X\in B}\cap \set{Y\in B} = \set{X\in B} \cap \set{XZ\in B} = \set{X\in B}\cap \set{Z=1}. \]
		Since $ X,Z $ are independent
		\[ \prob(\set{X\in B}\cap \set{Z=1}) = 1/2\prob(\set{X\in B}). \]
		So we can see that $ \prob(\set{X\in B}\cap\set{Y\in B})$ is not equal to $ \prob(\set{X\in B})\prob(\set{Y\in B}) $ in general, unless $ \prob(B) = 0$. Thus $ X,Y $ are not independent. 
		
		\item By the formula for $ \Corr $ we have
		\begin{align*}
			\Corr(X,Y) &= \E{XY} - \E{X}\E{Y} = \E{X^2Z} - \E{X}\E{XZ} \\
			&= \E{X^2}\E{Z} - \E{X}^2\E{Z} = \E{Z}\Var(X) = 0. 
		\end{align*}
		where we have used the fact that $ X,Z $ are independent, as well as $ X^2, Z $. Thus $ \E{XZ} = \E{X}\E{Z} $, and $  \E{X^2Z}=\E{X^2}\E{Z} $.
		
		\item No it is not. All the calculations above demonstrates a counterexample for this claim.
	\end{enumerate}
\end{solution}

\begin{observation}
	The conclusion in the part (e) of the question above is very important. In this example we can see two random variables $ X,Y $ which both of them has the same distribution, $ \mu_X = \mu_Y $ and their absolute value agrees almost everywhere $ \abs{X} = \abs{Y}\ a.e. $, and their correlation is zero $ \Corr(X,Y) = 0 $. However, we can see that they are \emph{not} independent.
\end{observation}

\begin{problem}
	Let $ X $ be a random variable, and let $ F_X(x) $ be its cumulative distribution function. For fixed $ x\in\R $, we know by right-continuoity that $ \lim_{y\searrow x} F_X(y) = F_X(x) $.
	\begin{enumerate}[(a)]
		\item Give a necessary and sufficient condition that $ \lim_{y\nearrow x}F_X(y) = F_X(x) $.
		\item More generally, give a formula for $ F_X(x) - (\lim_{y\nearrow x}F_X(y)) $, in terms of a simple property of $ X $.
	\end{enumerate}

\end{problem}

\begin{solution}
	\begin{enumerate}[(a)]
		\item A necessary and sufficient condition is $ \mu_X(\set{x}) = 0 $, where $ \mu_X $ is the law, or the distribution of the random variable $ X $.
		\item The value of $ F_X(x) - (\lim_{y\nearrow x}F_X(y)) $ is precisely the value of $ \mu_X $ evaluated on the singleton $ \set{x} $. In other words
		\[ F_X(x) - (\lim_{y\nearrow x}F_X(y)) = \mu_X(\set{x}). \]
	\end{enumerate}
\end{solution}

\section{More Probability Theorems}

\begin{observation}[Almost sure convergence and the convergence of the expected values]
	Let $ X,X_1,X_2,\cdots $ be random variables, and assume that $ X_n\to X $ almost surely. Then the real sequence $ \E{X_n} $ \emph{does not} necessarily converge to $ \E{X} $ as $ n\to \infty $. One example is the following: Let $ X_n = \mathds{1}_{[0,1/n]} $. Then $ X_n\to 0 $ on pointwise on $ (0,1] $. So we can say that $ X_n\to  X $ almost surely on $ [0,1] $. However, a simple calculation reveals that $ \E{X_n} = 1 $ for all $ n $, however $ \E{X} = 0 $. 
\end{observation}



\section{Solved Problems}

\begin{problem}
	For the ``simple counter-example'' with $ \Omega = \N $ and $ \prob(\omega) = 2^{-\omega} $ for $ \omega \in \Omega $ and $ X_n(\omega) = 2^n\delta_{n,\omega} $, verify explicitly that the hypotheses of each of the monotone convergence theorem, the bounded convergence theorem, the dominated convergence theorem, and the uniform integrablity convergence theorem, are all violated.
\end{problem}
\begin{solution}
	\begin{enumerate}[(i)]
		\item The \emph{Monotone Convergence Theorem} can not be used since the sequence $ \set{X_n} $ is not monotone. It is not increasing as
		\[ 0=X_{n+1}(n)\leq X_{n}(n) = 1, \]
		and it is not decreasing as
		\[ 1=X_{n+1}(n+1)\geq X_n(n+1) = 0. \]
		\item The \emph{Bounded Convergence Theorem} can not be used since the sequence is not uniformally bounded. I.e. $ \forall K \in \R $ we can find $ n $ large enough so that $ \abs{X_n} > K $.
		\item The \emph{Dominated Convergence Theorem} can not be used since there does not exists any random variable $ Y $ such that $ \abs{X_n}\leq Y $ with $ \E{Y}<\infty $. That is because to meet the first condition we need to have $ Y(k)\geq 2^k $ for $ k\in\Omega  $, which then will imply
		\[ \E{Y} \geq \sum_{n=1}^\infty 1 = \infty. \]
		\item The \emph{Uniform Integrablity Convergence Theorem} can not be used since collection $ \set{X_n} $ are not uniformally integrable. That is because for all $ \alpha\in \R $, there exists $ N\in \N $ such that $ \forall n>N $ we have $ \E{\abs{X}\mathds{1}_{\abs{X_n}\geq\alpha}} = 1 $.
	\end{enumerate}
\end{solution}


\begin{problem}
	Give an example of a sequence of random variables which is unbounded but still uniformally integrable. For bonus points, make the sequence also be undominated, i.e. violate the hypothesis of the dominated convergence theorem.
\end{problem}
\begin{solution}
	Let $ \Omega = \N $, with $ \prob(\omega) = 2^{-\omega} $. Define $ X_n = \frac{2^n}{n}\delta_{n,\omega} $. To see that the sequence is uniformally integrable first observe that $ \E{\abs{X_n}} = 1/n $, and since $ \abs{X_n}\mathds{1}_{\abs{X_n}\geq \alpha} \leq \abs{X} $ for any $ \alpha\in \R $, we have
	\[ \E{\abs{X_n}\mathds{1}_{\abs{X_n}\geq\alpha}} \leq \E{\abs{X}} = 1/n. \]
	Consider the $ \alpha $ of the form $ \alpha = 2^k/k $ for $ k\in \N $. Then for any fixed $ k\in \N $ we have $ \abs{X_n}\geq \alpha(k) $ for all $ n\geq k $. Thus
	\[ \E{\abs{X_n}\mathds{1}_{\abs{X_n}\geq\alpha(k)}} \leq 1/k, \]
	that goes to zero as $ k\to\infty $ (i.e. $ \alpha\to \infty $). However, the sequence is undominated. That is because the dominating function $ Y $ should assume the value at least $ 2^n/n $ for $ n\in\Omega $ which will lead to
	\[ \E{Y} = \sum_{n=1}^{\infty}\frac{1}{n} = \infty. \]
\end{solution}


\begin{problem}
	Let $ X,X_1,X_2,\cdots $ be non-negative random variables, defined jointly on some probability triple triple $ (\Omega,\mathcal{F},\prob) $, each having finite expected value. Assume that $ \lim_{n\to\infty}X_n(\omega) = X(\omega) $ for all $ \omega\in \Omega $. For $ n,K \in \N $, let $ Y_{n,k}=\min(X_n,K) $. For each of the following statements either prove it must true, or provide a counter example to show it is sometimes false. 
	\begin{enumerate}[(a)]
		\item $ \lim_{K\to\infty}\lim_{n\to\infty} \E{Y_{n,k}} = \E{X} $.
		\item $ \lim_{n\to\infty}\lim_{K\to\infty} \E{Y_{n,k}} = \E{X} $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item This statement is true. To prove this observe that for a fixed $ K $ we have $ \set{Y_{n,k}}\nearrow \min(X,K)$ as $ n\to\infty $. Thus from monotone convergence theorem we have
		\[ \lim_{n\to\infty} \E{Y_{n,k}} = \E{\min(X,K)}. \]
		On the other hand $ \set{\min(X,K)}\nearrow X $ as $ K\to\infty $. So again by monotone convergence theorem we have
		\[ \lim_{K\to\infty}\lim_{n\to\infty} \E{Y_{n,k}} = \lim_{K\to\infty}\E{\min(X,K)} = \E{X}.\]
		
		\item This statement is not true. Let $ \Omega=\N $, $ \prob(\omega) = 2^{-\omega} $ and define $ X_n = 2^{n} $. We know that $ X_n(\omega)\to X(\omega) $ for all $ \omega\in\Omega $ as $ n\to\infty $. Observe that for a fixed $ n $ we have $ \set{Y_{n,k}}\nearrow X_n $ as $ k\to\infty $. So from the monotone convergence theorem 
		\[ \lim_{K\to\infty}\E{\min(X_n,K)} = \E{X_n}. \]
		Thus
		\[ \lim_{n\to\infty }\lim_{K\to\infty}\E{\min(X_n,K)} = \lim_{n\to\infty} \E{X_n} = 1 \neq \E{X} = 0. \]
	\end{enumerate}
\end{solution}
\begin{observation}
	In the question above, we worked with the random variable $ \min(X_n,K) $. This random variable is very interesting. Because if $ X_n\to X $ almost surely, then
	\[ \set{\min(X_n,k)} \nearrow X_n \quad as\ k\to\infty, \]
	and also
	\[ \set{\min(X_n,k)} \nearrow \min(X,k) \quad as\ n\to\infty. \]
	The whole idea of the problem above is precisely around the behaviour of $ \min(X_n,K) $ as shown above.
\end{observation}


\begin{problem}
	Suppose that $ \lim_{n\to\infty} X_n(\omega) = 0 $ for all $ \omega\in \Omega $, but $ \lim_{n\to\infty}\E{X_n}\neq 0 $. Prove that $ \E{\sup_n \abs{X_n}} = \infty $.
\end{problem}
\begin{solution}
	Assume otherwise, i.e. $ \E{\sup_n\abs{X_n}} < \infty $. Let $ Y = \sup_n\abs{X_n} $. The $ Y $ is a possible choice for the dominant r.v. in the theorem 9.1.2. Thus by the dominated convergence theorem we will have
	\[ \lim_{n\to\infty}\E{X_n} = \E{X} = 0. \]
	This contradicts the assumption. So the we should have $ \E{\sup_n\abs{X_n}}=\infty $.
\end{solution}


\begin{problem}
	Prove that Theorem $ 9.1.2 $ implies Theorem 4.2.2, assuming $ \E{\abs{X}}<\infty $. In other words, prove that the Dominated Convergence Theorem, implies the Monotone Convergence Theorem if $ \E{\abs{X}}<\infty $.
\end{problem}
\begin{solution}
	Let $ X,X_1,X_2,\cdots $ be non-negative random variables such that $ \set{X_n}\nearrow X $ as $ n\to\infty $. By hypothesis we have $ \E{\abs{X}} < \infty $. So $ \set{X_n} $ is dominated by $ X $, and using dominated convergence theorem we conclude that $ \E{X_n}\to\E{X} $ as $ n\to\infty $.
\end{solution}

\begin{problem}
	Let $ X_1,X_2,\cdots $ be i.i.d., each with $ \prob(X_i=1)=\prob(X_i=-1)=1/2 $. 
	\begin{enumerate}[(a)]
		\item Compute the moment generating function $ M_{X_i}(s) $.
		\item Use Theorem 9.3.4 to obtains an exponentially-decreasing upper bound on $ \prob(\frac{1}{n}(X_1+\cdots+X_n)\geq 0.1). $ 
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item From the formula for the moment generating function we have
		\[ M_{X_i}(s) = \E{e^{sX_i}} = 1/2e^s + 1/2 e^{-s} = \cosh(s). \]
		
		\item Observe that $ \E{X_i} = M'_{X_i}(0) = 0 $. Let $ \epsilon=0.1 $. Then 
		\[ \rho = \inf_s (e^{-s\epsilon}\cosh(s)) \approx 0.995. \]
	\end{enumerate}
\end{solution}


\begin{problem}
	Let $ X_1,X_2,\cdots $ be i.i.d., each having the standard normal distribution $ \mathcal{N}(0,1) $. Use Theorem 9.3.4 to obtain an exponentially decreasing upper bound on $ \prob(1/n(X_1+\cdots+X_n)\geq 0.1) $. \emph{Hint: You can use the fact that for $ X\sim\mathcal{N}(0,1) $ we have}
	\[ M_X(s) = e^{s^2/2}. \]
\end{problem}
\begin{solution}
	Using the formula for $ \rho $ we can write
	\[ \rho = \inf_{s>0}(e^{-s(m+\epsilon)M_{X_i}(s)}). \]
	Observe that $ m = 0 $, and also $ M_{X_i}(s) = e^{s^2/2} $. Thus
	\[ \rho = \inf_{s>0} (e^{s^2/2-0.1s}). \]
	Doing a numerical computation will reveal that
	\[ \rho \approx 0.995012. \]
\end{solution}


\begin{problem}
	Let $ X\sim \operatorname{Exp}(5) $ with density $ f_X(x) = 5e^{-5x} $ for $ x\geq 0 $ and $ f_X(x) = 0 $ for $ x<0 $.
	\begin{enumerate}[(a)]
		\item Compute the moment generating function $ M_X(s) $.
		\item Use $ M_X(s) $ to compute (with explanation) the expected value $ \E{X} $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Using the formula for the moment generating function we can write
		\begin{align*}
			M_X(s) = \E{E^{sX}} &= \int_{-\infty}^{+\infty} e^sx f_X(x) dx \\
			&= \lambda \int_{0}^{+\infty} e^sx e^{\lambda x} dx \\
			&= \frac{\lambda}{s-\lambda}e^{x(s-\lambda)}\big|_{0}^\infty \\
			&= \frac{\lambda}{\lambda - s},
		\end{align*}
		where we have assumed that $ s-\lambda < 0 $.
		
		\item We know that $ \E{X} = M'_X(0) $. So we first calculate $ M'_X $
		\[ M'_X(s) = \frac{\lambda}{(\lambda-s)^2}. \]
		Then it is straightforward to see that $ \E{X} = 1/\lambda $.
	\end{enumerate}
\end{solution}


\begin{problem}
	Let $ X\sim \text{Poisson}(a) $ and $ Y\sim\text{Poisson}(b) $ be independent. Let $ Z = X+Y $. Use the convolution formula to compute $ \prob(Z=z) $ for all $ z\in\R $, and prove that $ Z\sim \text{Poisson}(a+b) $.
\end{problem}
\begin{solution}
	Recall that for a random variable $ X $ with Poisson distribution $ \operatorname{Poisson(a)} $ we have
	\[ \prob(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}. \]
	On the other hand we know that $ p_Z = p_X \ast p_Y  $. So 
	\begin{align*}
		p_Z(k) &= \sum_{n} p_X(n)p_Y(k-n) = \sum_n \frac{e^{-a}a^n}{n!}\cdot\frac{e^{-b}b^{k-n}}{(k-n)!}\\
		&=\sum_{n=1}^{\infty} e^{-(a+b)}\frac{a^nb^{k-n}}{(k-n)!} \\
		&=  e^{-(a+b)}/k! \frac{k!}{n!(k-n)!}a^n b^{n-k} \\
		&= \frac{e^{-(a+b)}(a+b)^k}{k!}.
	\end{align*}
	Thus $ Z \sim \operatorname{Poisson}(a+b) $.
\end{solution}


\section{Weak Convergence}


\subsection{Solved Problems}

\begin{problem}
	Suppose $ \mathcal{L}(X_n)\Rightarrow \delta_c $ for some $ c\in \R $. Prove that $ \set{X_n} $ convergence to $ c $ in probability.
\end{problem}
\begin{solution}
	Let $ \epsilon>0 $ give. Then
	\begin{align*}
		\prob(\abs{X_n - c} \geq \epsilon) &= 1 - \prob(\abs{X_n - c}<\epsilon) \\
		&= 1-\prob(X_n \in (c-\epsilon,c+\epsilon))\\
		&= 1 - \mu_n ((c-\epsilon,c+\epsilon)) \to 1 - 1 = 0.
	\end{align*}
	Note that we have used the fact that $ \mu_n(A)\to \mu(A) $ for $ A\in \mathcal{B} $ such that $ \mu(\partial A) = 0 $, and the fact that $ \mu((c-\epsilon,c+\epsilon)) = 1 $. So we conclude that $ X_n\to X $ in probability.
\end{solution}

\begin{problem}
	Let $ X,Y_1,Y_2,\cdots $ be independent random variables, with $ \prob(Y_n=1)=1/n $ and $ \prob(Y_n=0)=1-1/n $. Let $ Z_n = X + Y_n $. Prove that $ \mathcal{L}(Z_n)  \Rightarrow \mathcal{L}(X) $, i.e. that the law of $ Z_n $ converges weakly to the law of $ X $.
\end{problem}

\begin{solution}
	Let $ \epsilon>0 $ given. Then 
	\begin{align*}
		\prob(\abs{Z_n-X}\geq \epsilon) = \prob(\abs{Y_n}\geq \epsilon) \to 0 \quad as \quad n\to \infty.
	\end{align*}
	So we observe that $ Z_n $ converges to $ X $ in probability, which implies convergence in distribution.
\end{solution}

\begin{problem}
	Let $ \mu_n = \mathcal{N}(0,1/n) $ be a normal distribution with mean 0 and variance $ 1/n $.  Doe the sequence $ \set{\mu_n} $ converge weakly to some probability measure? If yes, to what measure?
\end{problem}
\begin{solution}
	Yes. $ \mu_n \Rightarrow \delta_0 $ as $ n\to\infty $. To see this let $ f $ denote the density the $ \mathcal{N}(0,1) $ distribution. So 
	\[ f(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}. \]
	Similarly, let $ f_n $ denote the density of the $ \mathcal{N}(0,1/n) $ distribution. So
	\[ f_n(x) = \frac{n}{\sqrt{2\pi}}e^{-x^2n^2/2} = nf(nx). \]
	We want to show that $ F_n $ (cumulative distribution for $ \mathcal{N}(0,1/n) $) converges to $ H $ that is the cumulative distribution of the $ \delta_0 $ distribution on $ \R $ except the origin that is the point of discontinuity of $ H $. Let $ t\neq 0 $. 
	Then 
	\[ F_n(t) = \int_{0}^{t}f_n(t)dt = \int_{0}^{t} nf(nt)dt = \int_{0}^{nt} f(y)dy = F_\mathcal{N}(nt), \]
	where $ F_\mathcal{N} $ is the cumulative distribution for $ \mathcal{N}(0,1) $. Observe that when $ t>0 $ we have $ F_n(t) \to 1 $ as $ n\to \infty $ and when $ t<0 $ we have $ F_n(t) \to 0 $ as $ n\to-\infty $. So $ F_n(t) $ converges to $ H $ when $ t\neq 0 $. So $ \mu_n \Rightarrow \delta_0 $ as $ n\to\infty $.
\end{solution}

\begin{problem}
	Let $ a_1,a_2,\cdots $ be any sequence of non-negative real numbers with $ \sum_i a_i = 1 $. Define the discrete measure $ \mu $ by $ \mu(\cdot) = \sum_{i\in \N}a_i\delta_i(\cdot) $, where $ \delta_i(\cdot) $ is a point-mass at the positive integer $ i $. Construct a sequence $ \set{\mu_n} $ of probability measure, each having a density with respect to Lebesgue measure, such that $ \mu_n \Rightarrow \mu $ as $ n\to\infty $.
\end{problem}
\begin{solution}
	Define $ \mu_n $ as
	\[ \mu_n(A) = \sum_{i=1}^{\infty}na_i \lambda(A\cap(i-1/n,i]) \]
	for any $ A \in \mathcal{B} $. With this definition of $ \mu_n $ we are in fact approximating a discontinuous function ($ F_n $) with a piece-wise linear function as shown below. It is easy to show that $ F_n $ converges point-wise to $ F $ everywhere except at the point of discontinuity.
	\input{Images/discontnousApprox.tex}
	
\end{solution}

\begin{remark}
	In the question above, if there are no limitations that the sequence $ \set{\mu_n} $ should ave a density with respect to Lebesgue measure, then one natural choice is
	\[ \mu_n = \sum_{i}^{n} a_i \delta_i + (\sum_{i=n+1}^{\infty}a_i)\delta_{n+1}. \]
\end{remark}


\begin{problem}
	Let $ \mathcal{L}(Y) = \mu $, where $ \mu $ has continuous density $ f $. For $ n\in \N $, let $ Y_n = \ceil{nY}/n $, and let $ \mu_n = \mathcal{L}(Y_n) $.
	\begin{enumerate}[(a)]
		\item Describe $ \mu_n $ explicitly.
		\item Prove that $ \mu_n \Rightarrow \mu $.
		\item Is $ \mu_n $ discrete, or absolutely continuous, or neither? What about $ \mu $?
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item We start by calculating the cumulative distribution function for $ Y_n $.
		\begin{align*}
			F_n(y) &= \mu_n((-\infty,x]) = \prob(Y_n\in(-\infty,y]) \\
			&= \prob(\frac{\floor{nY}}{n}\in (-\infty,y]) \\
			&= \prob(\floor{nY}\in (-\infty,ny]) \\
			&= \prob(nY \in (-\infty,\floor{ny}+1))\\
			&= \prob(Y \in (-\infty,\frac{\floor{ny}}{n} + 1/n))\\
			&= \prob(Y \in (-\infty,\frac{\floor{nY}}{n}]) + \mu((\frac{\floor{ny}}{n},\frac{\floor{ny}+1}{n})) \\
			&= F(\frac{\floor{ny}}{n}) + \mu((\frac{\floor{ny}}{n},\frac{\floor{ny}+1}{n}))
 		\end{align*}
 		\item First observe that 
 		\[ \frac{\floor{ny}}{n} \to y \quad as \quad n\to\infty. \]
 		On the other hand
 		\[ \mu((\frac{\floor{ny}}{n},\frac{\floor{ny}+1}{n})) \to \mu((y,y)) = \mu(\emptyset) = 0 \quad as \quad n\to\infty. \]
 		So from part (a) we can see that
 		\[ F_n(y) \to F(y) \quad as \quad n\to\infty \]
 		for all $ y\in \R $. Thus $ \mu_n \Rightarrow \mu $ as $ n\to\infty $.
 		\item {\color{red} \noindent TODO: TO BE ADDED.}
	\end{enumerate}
\end{solution}


\begin{problem}
	Let $ f:[0,1]\to (0,\infty) $ be a continuous function such that $ \int_{0}^{1}fd\lambda = 1 $ (where $ \lambda $ is Lebesgue measure on $ [0,1] $). Define probability measure $ \mu $ and $ \set{\mu_n} $ by $ \mu(A) = \int_{0}^{1}f\mathds{1}_A d\lambda $ and $ \mu_n(A) = \sum_{i=1}^{n}f(i/n)\mathds{1}_{A}(i/n)/\sum_{i=1}^{n}f(i/n) $.
	\begin{enumerate}[(a)]
		\item Prove that $ \mu_n \Rightarrow \mu $ as $ n\to\infty $.
		\item Explicitly construct  random variables $ Y $ and $ \set{Y_n} $ so that $ \mathcal{L}(Y)=\mu $, $ \mathcal{L}(Y_n)=\mu_n $, and $ Y_n\to Y $ with probability 1. 
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ A \in \mathcal{B} $ such that $ \mu(\partial A) = 0 $. Then
		\begin{align*}
			\lim_n \mu_n(A) &= \lim_n \frac{\sum_{i=1}^{n}f(i/n)\mathds{1}_{A}(i/n)}{\sum_{i=1}^{n}f(i/n)} \\
			&= \lim_n \frac{\sum_{i=1}^{n}f(i/n)\mathds{1}_{A}(i/n)1/n}{\sum_{i=1}^{n}f(i/n)1/n} \\
			&= \frac{\lim_n \sum_{i=1}^{n}f(i/n)\mathds{1}_{A}(i/n)1/n}{\lim_n \sum_{i=1}^{n}f(i/n)1/n} \\
			&= \frac{\int_{0}^{1} f \mathds{1} d\lambda}{\int_{0}^{1} f d\lambda} \\
			&= \int_{0}^{1}f\mathds{1}_A d\lambda  = \mu(A).
		\end{align*}
		Since $ A $ was arbitrary then $ \mu_n \Rightarrow \mu $ as $ n\to\infty $.
		
		\item From the definition of $ \mu $ and $ \mu_n $ we define $ F_n $ and $ F $ (the cumulative distribution functions) and then define the random variables $ Y, Y_1,Y_2,\cdots $ as
		\[ Y_n(\omega) = \sup_x\set{x: F_n(x)< \omega},\qquad Y(\omega)= \sup_x \set{x: F(x)<\omega}. \]
	\end{enumerate}
\end{solution}
\begin{remark}
	In Rosenthal, Theorem 7.2.1 defines the ``inverse'' of $ F $ as
	\[ Y(\omega) = \inf_x\set{x: F(x)\geq \omega}. \]
	{\color{red} \noindent  It is not clear to me }if these two definitions are the same or not. Because one of them leads to a left continuous function while the other one leads to a right continuous function.
\end{remark}



\section{Interesting Questions From Other Books}

\begin{problem}[From Billingsley]
	\label{prob:DiscreteSpaceIndependentEvents}
	\begin{enumerate}[(a)]
		\item Show that a discrete probability space (see Example 2.8 for the formal definition) can not contain an infinite sequence $ A_1,A_2,\cdots $ of independent events each of probability $ 1/2 $. Note that $ A_n $ ve be identified with heads on the $ n $-the toss of a coin. 
		\item Suppose that $ 0\leq p_n \leq 1 $. and put $ \alpha_n = \min(p_n,1-p_n) $. Show that if $ \sum_n a_n $ diverges, then no discrete probability space can contain independent events $ A_1,A_2,\cdots $ such that $ A_n $ has probability $ p_n $.
	\end{enumerate}
\end{problem}

\begin{solution}
	\begin{enumerate}
		Let $ \set{x} \subset \Omega $. Then $ x $ belongs to only one of the four disjoint sets below
		\[ A_1\cap A_2,\quad A_1^c\cap A_2 \quad A_1\cap A_2^c,\quad A_1^c \cap A_2^c. \]
		Observe that the probability of each of these events are $ 1/4 $ (noting that if $ A,B $ are independent then $ A^c, B^c $ are independent as well). So $ \set{x} $ must have probability $ 1/4 $ at most. Now consider the events $ A_1,A_2,A_3 $. Then $ x $ should belong to only one of the disjoint events below
		\begin{align*}
			A_1\cap A_2\cap A_3, \quad A_1\cap A_2^c\cap A_3, \quad A_1\cap A_2\cap A_3^c, \quad A_1\cap A_2^c\cap A_3^c, \\
			A_1^c\cap A_2\cap A_3, \quad A_1^c\cap A_2^c\cap A_3, \quad A_1^c\cap A_2\cap A_3^c, \quad A_1^c\cap A_2^c\cap A_3^c. 
		\end{align*}
		Since $ A_1,A_2,A_3 $ are independent events the probability of each of the events above are $ 1/8 $. So $ \set{x} $ can have probability $ 1/8 $ at most. Continuing this we will get $ \prob(\set{x}) = 0 $ for all $ x\in \Omega $. On the other hand since $ \Omega $ is countable then 
		\[ 1 = \prob(\Omega ) = \sum_{x\in \Omega} \prob(\set{x}) = 0 \]  
		which is a contradiction. 
	\end{enumerate}
\end{solution}

\begin{remark}
	Note that by discrete probability measure we also require it to be countable.
\end{remark}

\newpage

\section{Canonical Examples}
\begin{example}[Converging in probability but not almost surely]
	Let $ X, X_1,X_2,\cdots $ be random variables with $ X\equiv 0 $, and $ X_n = \mathds{1}_{B_n} $ for some $ B_n \in \mathcal{B} $ such that $ \prob(B)=1/n $. In other words $ \prob(X_n = 1) = 1/n $ and $ \prob(X_n=0) = 1-1/n $. Then 
	\begin{enumerate}[(a)]
		\item $ X_n \to X $ in probability.
		\item $ X_n \not\to X $ almost surly. (Because $ 1/n $ is not summable). 
	\end{enumerate}
\end{example}
\begin{remark}
	\begin{enumerate}[(a)]
		\item Defining $ B_n $ such that $ \prob(B_n) =1/n^2 $ then we will also have the almost sure convergence.
		\item Defining $ X_n = n\mathds{1}_{B_n} $ with $ \prob(B_n)=1/n $, then $ X_n\to X $ in probability, and $ 1 = \E{X_n} \not\to \E{X} = 0 $.
		\item Defining $ X_n = n^2\mathds{1}_{B_n} $ with $ \prob(B_n)=1/n^2 $, then $ X_n\to X $ almost surely, but $ 1 = \E{X_n} \not\to \E{X} = 0. $
	\end{enumerate}
\end{remark}


\begin{example}[Convergence in distribution but not in probability]
	Let $ X,X_1,X_2,\cdots $ be i.i.d. random variables each equal to $ \pm 1 $ with probability $ 1/2 $. Then obviously $ \mu_n \Rightarrow \mu $ however $ X_n $ does not converge to $ X $ in probability. For instance, let $ \epsilon=2 $. Then $ \prob(\abs{X_n-X}\geq 2) = 1/2 \not\to 0 $.
\end{example}


\begin{example}
	We give an example of a sequence of random variables which is unbounded, uniformly integrable, and undominated (in the sense of the hypothesis of dominated convergence theorem). Define
	\[ \Omega = \N,\quad \prob(\Omega)=2^{-\omega},\quad X_n(\omega) = \frac{2^n}{n}\delta_{\omega,n} \]
\end{example}


\begin{observation}
	Let $ x\in \R $. Then $ 1-x < e^{-x} $. This is a very important inequality that can be justified by simply looking at the graph of $ (1-x) $ and $ e^{-x} $. This inequality is used in the proof of the Borel-Cantelli lemma, as well as in Problem \autoref{prob:DiscreteSpaceIndependentEvents}.
\end{observation}