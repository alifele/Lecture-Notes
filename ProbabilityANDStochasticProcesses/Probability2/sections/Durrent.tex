\chapter{Durret Probability}

\section{Conditioning}

Here we will discuss the prove that if $ \E{X} < \infty $ then $ \E{X|\mathcal{B}} < \infty $ as proved in Durret Lemma 4.1.1. This is analogous to the proof (c) page 232 in Le Gall. 
\begin{proposition}
	Assume $ \E{|X|} < \infty $. Then $ Y = \E{X|\mathcal{B}} < \infty $.
\end{proposition}
\begin{proof}
	We can write $ Y $ as
	\[ Y = Y^+ - Y^-. \]
	For $ Y^+ $ we have $ \E{Y^+} < \infty $. Indeed, for $ A = \set{Y > 0} $
	\[ \E{Y^+} = \E{Y\mathds{1}_{A} } = \E{X\mathds{1}_A} \leq \E{\abs{X}\mathds{1}_A} < \infty. \] 
	Similarly for $ Y^- $ we claim $ \E{Y^-} < \infty $. To see this we can write
	\[ \E{Y^-} = \E{\mathds{1}_{A^c} (-Y)} = \E{\mathds{1}_{A^c} (-X)} \leq \E{\mathds{1}_{A^c}\abs{X}} < \infty. \]
	This implies $ \E{Y^+} + \E{Y^-} < \infty $. Using the fact that $ Y = Y^+ + Y^- $ and using the linearity of expectation we can write
	\[ \E{\abs{Y}} < \infty. \]
\end{proof}

\subsection{Solved Problems}
\begin{problem}[Bayes' Formula]
	Let $ G \in \mathcal{B} $ and show that for any $ A \in \mathcal{F} $
	\[ \prob(G|A) = \frac{\int_G\prob(A|\mathcal{B}) d\prob}{\int_\Omega \prob(A|\mathcal{G})d\prob}.  \]
	When $ \mathcal{G} $ is the $\sigma\text{-algebra}$ generated by a partition, this reduces to the usual Bayes' formula
	\[ \prob(G_i|A) = \frac{\prob(A|G_i) \prob(G_i)}{\sum_{j}\prob(A|G_j)\prob(G_j)}. \]
\end{problem}
\begin{solution}
	We start with the definition of $ \prob(G|A) = \prob(G\cap A)/\prob(A) $. For the numerator we can write
	\begin{align*}
		\prob(G\cap A) &= \E{\mathds{1}_G\mathds{1}_A} && \text{(by definition)} \\
		&= \E{\mathds{1}_G \E{\mathds{1}_A|\mathcal{G}}} && \text{(using characterization of cond. expec.)} \\
		&= \E{\mathds{1}_G \prob(A|\mathcal{G})} && \text{(by definition)} \\ 
		&= \int_G \prob(A|\mathcal{G})d\prob.
	\end{align*}
	Similarly, for the denominator we can write
	\begin{align*}
		\prob(A) &= \E{\mathds{1}_A} \\
		&= \E{\E{\mathds{1}_A|\mathcal{G}}}  && \text{(by characterization of cond. expec.)} \\
		&= \E{\prob(A|\mathcal{G})} && \text{(by definition)} \\
		&= \int_\Omega \prob(A|\mathcal{G}) d\prob. 
	\end{align*}
	Thus it implies that 
	\[ \prob(G|A) = \frac{\int_G\prob(A|\mathcal{B}) d\prob}{\int_\Omega \prob(A|\mathcal{G})d\prob}. \]
\end{solution}

\begin{problem}
	Prove \emph{conditional Markov's inequalit}y. If $ Y $ is a non-negative random variable and $ a>0 $, prove that
	\[ \prob(Y\geq a|\mathcal{F}) \leq \frac{\E{Y|\mathcal{F}}}{a}. \]
\end{problem}
\begin{solution}
	Since $ X $ is non-negative and $ a>0 $ then we have
	\[ Y \geq a\mathds{1}_{Y\geq a} \qquad \text{(see Le Gall page 25)}. \]
	Using the monotonicity of conditional expectation (See property (e) page 232 Le Gall), we can write
	\[ \E{Y|\mathcal{F}} \geq a\E{\mathds{1}_{Y\geq a} |\mathcal{F}} = a\prob(Y\geq a|\mathcal{F}). \]
	Then it follows that
	\[ \prob(Y\geq a|\mathcal{F}) \leq \frac{\E{Y|\mathcal{F}}}{a}. \]
\end{solution}

\begin{problem}
	Prove \emph{conditional Chebyshev's inequality}. If $ a>0 $, then
	\[ \prob(\abs{X}\geq a|\mathcal{F}) \leq \frac{\E{X^2|\mathcal{F}}}{a^2}. \]
\end{problem}
\begin{solution}
	This follows immediately from the result of the question above. Indeed
	\[ \prob(\abs{X}>a|\mathcal{F}) = \prob(X^2>a^2|\mathcal{F}) \leq \frac{\E{X^2|\mathcal{F}}}{a^2}, \]
	where for the last inequality we have used the result of the question above (i.e. the conditional Markov inequality).
\end{solution}

\begin{problem}
	Give an example on $ \Omega = \set{a,b,c} $ in which 
	\[ \E{\E{X|\mathcal{F}_1}|\mathcal{F}_2} \neq \E{\E{X|\mathcal{F}_2}|\mathcal{F}_1}. \]
\end{problem}
\begin{solution}
	Let $ \mathcal{F}_1 $ and $ \mathcal{F}_2 $ be given as
	\[ \mathcal{F}_1 = \sigma(\set{a,b},\set{c}), \qquad \mathcal{F}_2 = \set{\set{a},\set{b,c}}. \]
	Let $ X $ be a random variable defined by $ X(a) = 1, X(b) = 3$, and $ X(c) = 5 $. We can denote this mapping with the following convenient notation.
	\[ X = \begin{pmatrix}
		a & b & c \\
		1 & 3 & 5
	\end{pmatrix}. \]
	Then we can calculate
	\[ \E{X|\mathcal{F}_1} = \begin{pmatrix}
		a & b & c \\
		2 & 2&  5
	\end{pmatrix}, \qquad 
	\E{X|\mathcal{F}_2} = \begin{pmatrix}
		a & b & c \\
		1 & 4 & 4 
	\end{pmatrix}, \]
	Furthermore, it is easy to calculate
	\[ \E{\E{X|\mathcal{F}_1}|\mathcal{F}_2} = \begin{pmatrix}
		a & b & c \\
		2 & 3.5 & 3.5
	\end{pmatrix}. \qquad
	\E{\E{X|\mathcal{F}_2}|\mathcal{F}_1} = \begin{pmatrix}
		a & b & c \\
		2.5 & 2.5 & 4
	\end{pmatrix}. \]
\end{solution}

\begin{problem}
	Show that if $ \mathcal{G} \subset \mathcal{F} $ and $ \E{X^2} < \infty $. Then 
	\[ \E{(X-\E{X|\mathcal{F}})^2} + \E{(\E{X|\mathcal{F}} - \E{X|\mathcal{G}})^2} = \E{(X-\E{X|\mathcal{G}})^2}. \]
	Dropping the second term on the left, we get in inequality that says geometrically the larger the subspace the closer the projection is, or statically, more information means a smaller mean square error.
\end{problem}
\begin{remark}
	The equality above is the analogue of the Pythagorean theorem as depicted below.
	\input{sections/images/orthogonalProjections.tex}
\end{remark}
