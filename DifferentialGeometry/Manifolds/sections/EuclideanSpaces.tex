\chapter{Euclidean Spaces}

\section{Basic Notions and Definitions}

\subsection{A Review on the algebraic structures}

Here in this chapter I will be covering the details of some notions that was challenging for me do digest in the first read.


\begin{definition}[Axioms of Group]
	Group is a set $ A $ along with a binary operation $ *: A\times A \to A $ that satisfies the following properties. Let $ a,b,c \in A $, then
	\begin{itemize}
		\item \textbf{Associativity}: $ a*(b*c) = (a*b)*c $.
		\item \textbf{Identity element}: $ \exists 1 \in A $ such that 
		\[ 1*a = a*1 = a. \]
		\item \textbf{Inverse element}: $ \forall a \in A\ \exists\hat{a}\in A $ such that 
		\[ a*\hat{a} = \hat{a}*a = 1. \]
	\end{itemize}
\end{definition}
\begin{remark}
	A set along with a binary operation that does not satisfy any properties is called a \textbf{magma}. If the binary operation is only associative, then we are dealing with \textbf{semi-group}. If the binary operation has an identity element as well, then we call this algebraic structure as \textbf{monoid}.
\end{remark}

\begin{definition}[Axioms of Ring]
	A ring is a set $ R $ along with two operations $ +: R\times R \to R $ and $ *: R\times R \to R $, where
	\begin{itemize}
		\item $ (R,+) $ is an Abelian group.
		\item $ (R,*) $ is a monoid.
		\item The operator $ (*) $ has distributive (left and right) law over $ (+) $ i.e.
  			\[a*(b+c) = (a*b)+(a*c), \qquad (b+c)*a = (b*a) + (c*a).\].
	\end{itemize}
\end{definition}

\begin{remark}
	\textbf{Field} is a ring where every non-zero element (i.e. inverse element in the $ (R,+) $ group in the ring) has a multiplicative inverse.
\end{remark}

\begin{definition}[Axioms of Module]
	A \textbf{module} is a group $ M $ along with a ring $ R $ where the monoid of the ring acts on $ M $ (through scalar multiplication) (i.e. it satisfies the idenity and compatibility properties) and satisfies the distributive property. I.e.
	\begin{itemize}
		\item \textbf{Compatibility of the monoid action}: $ a,b \in R,\ u \in M $ then 
		\[ a(bu) = (ab)u. \]
		\item \textbf{Identity of the monoid action}: Let $ 1 $ be the identity element of the ring $ R $. Then $ \forall u \in M $
		\[1u = u1 = u. \]
		\item \textbf{Distribution law}: $ a,b \in R $ and $ u,v \in M $ then
		\begin{itemize}
			\item $ (a+b)u = au + bu $.
			\item $ a(u+v) = au + av $.
		\end{itemize}
	\end{itemize}
\end{definition}
\begin{remark}
	A module $ (M,R) $ is called a \textbf{vector space}, if the \textbf{ring} $ R $ is a \textbf{field}.
\end{remark}

\begin{definition}[Axioms of Algebra]
	\label{def:algebra}
	An Algebra over field $ F $ is a ring $ A $ that $ F $ acts on it (thus $ A $ has vector space structure as well), where the monoid operation of $ F $ (i.e. multiplication) satisfies the homogeneity property. I.e. for $ r \in F $ and $ u,v \in A $ we have
	\[ r(uv) = (ru)v = u (rv). \]
\end{definition}

There are some important observations when combining different algebraic structures with each other to get a new one. The first is that when we combine two structures with different operators, then the operators need to satisfy the distributive laws. Also, note that when an algebraic structure (like group or monoid) acts on another algebraic structure, we need to have the identity and and compatibility conditions satisfied.

The following diagram shows how different algebraic structures are combined with each other to produce another structure.


\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{Images/algebraicStructures.pdf}
\end{figure}
\FloatBarrier
Note that in the figure above, I have used some non-standard notations to make the figure concise. For instance, the expression ``\textbf{$ R_{\text{mon}} @ M \ \text{with $\cdot$}$}'' means that the monoid structure in the field $ R $ acts on the group $ M $ with the ($ \cdot $) symbol. Or the expression ``\textbf{$ \times $ in $ M_{\text{mon}} $ satisfies homogen cond.}'' means the multiplication operation of the monoid structure inside the ring $ M $ satisfies the homogeneity condition (see the definition of the algebra in \autoref{def:algebra} ). Finally, $ M_{g} $ means the group structure inside the ring $ M $.


\subsection{Directional Derivative}
The notion of directional derivative is very central in generalization of the multi-variable calculus manifolds. 

\begin{definition}[Directional derivative]
	\label{def:directionalDerivative}
	Let $ f:U \to \R $ where $ U \subset \R^n $ and $ f \in C^\infty(U) $. Then we define a directional derivative at $ p\in U $ and in the direction $ v \in T_p(\R^n) $ as 
	\[ D_v f \big|_p = \lim_{t\to 0 } \frac{f(p + tv) - f(p)}{t} = \frac{d}{dt}\big|_{t=0} f(p+vt). \]
	We denote the set of all directional derivatives at $ p $ by $ \mathcal{D}(C_p^\infty(U)) $.
\end{definition}
\begin{remark}
	By the chain rule we have
	\[ D_v f \big|_p = \sum_{i=1}^{n} v^i \frac{\partial f}{\partial x^i}(p). \]
\end{remark}


\begin{proposition}[Directional derivative is $ R-$linear map satisfying the Leibniz rule]
	A directional derivative $ D_v $ at point $ p $ is a $ \R-$linear operator that maps  \[f\in C_p^\infty \mapsto D_vf \in \R\] such that satisfies the Leibniz rule,
	\[ D_v(fg) = D_v(f)g + fD_v(g). \]
\end{proposition}
\begin{proof}
	Let $ D_v \in \mathcal{D}(C_p^\infty(U)) $. Then by the remark above we can write it as  
	\[ D_v f \big|_p = \sum_{i=1}^{n} v^i \frac{\partial f}{\partial x^i}(p). \]
	Since each of the partial derivatives above are $ \R-$linear and satisfy the Leibniz rule, then $ D_v $ inherits those properties as well.
\end{proof}

\section{Tangent Spaces and Derivations}

\begin{definition}[Informal definition of tangent space]
	Let $ p \in \R^n $. A tangent space at $ p $ denoted by $ T_p(\R^n) $ is a linear space containing all of vectors emerging from $ p $.
\end{definition}
\begin{remark}
	Note that the definition above is an informal definition of the tangent space. For a more formal and technical definition, we can use the notion of curves, or the notion of manifolds. We wont' touch this level of technicality in the early chapters.
\end{remark}

To distinguish between points in $ \R^n $ and vectors in $ T_p(\R^n) $ we denote a point $ p\in\R^n $ by 
\[ p = (p^1, p^2, \cdots, p^n), \]
while for a vector $ v \in T_p(\R^n) $ we write
\[ v = \<  v_1,v_2,\cdots,v_n \>. \]

As we observed in \autoref{def:directionalDerivative}, we have a natural one-to-one correspondence between each $  v\in T_p(\R^n) $ and a $ D_v \in \mathcal{D}(C_p^\infty(U)) $, i.e. these linear spaces are isomorphic.

\begin{proposition}
	Let $ p \in\R^n $. The set of all directional derivatives at $ p $, i.e. $ \mathcal{D}(C_p^\infty(U)) $ is isomorphic to the tangent space at $ p $ i.e. $ T_p(\R^n) $.
\end{proposition}
\begin{proof}
	Proof follows immediately from the following natural association in the definition of the directional derivative.
	
	\[ D_v f \big|_p \longleftrightarrow \sum_{i=1}^{n} v^i \frac{\partial f}{\partial x^i}(p) \]
\end{proof}



\begin{definition}[Point derivations at a point]
	\label{def:pointDerivation}
	Let $ p \in \R^n $, and $ U $ an open set containing $ p $. A \textbf{derivation at $ p $} or a \textbf{point-derivation of $ C_p^\infty(U) $} is a \emph{linear} operator
	\[ D: C_p^\infty(U) \to \R \]
	such that satisfies the Leibniz property, i.e. for $ f,g \in C_p^\infty(U) $ we have
	\[ D(fg) = D(f)g - fD(g). \]
	We denote the set of all such maps as $ \mathbb{D}(C_p^\infty(U)) $.
\end{definition}

\begin{remark}
	We know that a directional derivative at $ p $ satisfies the Leibniz rule. Thus $ \mathcal{D}(C_p^\infty(U)) \subset \mathbb{D}(C_p^\infty(U))$. On the other hand, we know that both $ \mathcal{D} $ and $ \mathbb{D} $ are linear spaces. So $ \mathcal{D}(C_p^\infty(U)) $ is in fact a \emph{linear subspace} of $\mathbb{D}(C_p^\infty(U))$. Thus the zero of these two linear spaces match.
\end{remark}


The following Lemma follows form the algebraic property of the point derivation.

\begin{lemma}
	\label{lem:PointDerivationOfConstantFunction}
	Let $ D $ be a point derivation of $ C_p^\infty(U)$. Then $ D(c) = 0 $ for any constant function  $ c $. 
\end{lemma}
\begin{proof}
	Let $ c $ be a constant function, i.e. a real number. Since $ D $ is $ R- $linear, then we can write $ D(c) = c D(1). $ On the other hand, from the Leibniz property we can write
	\[ D(1) = D(1 \cdot 1) = D(1) + D(1) = 2D(1) \]
	Thus $ D(1) = 0 $, which implies $ D(c) = 0 $.
\end{proof}

The following theorem is very important as it states that all of the point derivations are in fact the directional derivatives and vise-versa. This is a very interesting result, since we are in fact stating that an operator is a point derivative if and only if it satisfies an algebraic property. This means that we can abstract away all of the detailed limit processes in the definition of derivative, and replace that with an axiomatic requirement which is a purely algebraic property. We can see these kind of ideas, i.e. axiomatic generalization all over the mathematics. For instance, in the PDE theory, at some point we need to relax the definition of derivative and talk about the weak derivatives. To do so, we get an identity that derivatives satisfy and carefully use that identity to define the notion of weak derivatives. 

\begin{theorem}[The set of all point-derivations is isomorphic to the set of all directional derivatives]
	Let $ p \in U \subset \R^n $. Then $ \mathbb{D}(C_p^\infty(U)) $ is isomorphic to $ T_p(\R^n) $.
\end{theorem}

\begin{proof}
	Let $ \phi : T_p(\R^n) \to \mathbb{D}(C_p^\infty(U))  $ be a linear isomorphism between the linear spaces. We need to show that this map is surjective and bijective. To show the bijectivity, we use the fact that a linear map is bijective if and only if its kernel is a singleton. To find the kernel of the map, let need to find all points in $ T_p(\R^n) $ that maps to the zero of $ \mathbb{D} $. As we discussed in the remark of \autoref{def:pointDerivation}, the zero $ \mathbb{D} $ is the same as the zero directional derivative, i.e. $ D_v = 0 $. We need to prove that $ v $ is the zero vector, i.e. the zero of $ T_p(\R^n) $. To do this, we apply the $ D_v $ to the coordinate functions
	\[ 0 = D_v(x^i) = \sum_{j=1}^n v^j \frac{\partial x^i}{\partial x^j} = v^j\]
	Thus $ v = 0 \in T_p(\R^n) $, thus $ \phi $ is injective. In other words, the injectivity follows immediately from $ T_p(\R^n) $ being isomorphic to $  \mathcal{D} $, and $  \mathcal{D} $ being a linear subspace of $ \mathbb{D} $.
	
	To prove the surjectivity, let $ D $ be a point derivation at $ p $, and let $ (f,V) $ be a representative of a germ in $ C_p^\infty $. Marking $ V $ smaller if necessary, we may assume that $ V $ is an open ball, hence star shaped. By Taylor's approximation theorem we know there exists $ C^\infty $ functions $ g_i(x) $ such that
	\[ f(x) = f(p) + \sum_{i=1}^{n} (x^i-p^i)g_i(x),\qquad g_i(p) = \frac{\partial f}{\partial x^i}(p). \]
	From Lemma \autoref{lem:PointDerivationOfConstantFunction}, we know that $ D(f(p)) = 0$ as well as $ D(p^i) = 0 $. Thus we can write
	\[ D(f(x)) = \sum_{i=1}^{n}(D(x^i)g_i(x) + (p^i - p^i)D(g_i(x))) =  \sum_{i=1}^{n}D(x^i)g_i(x) = \sum_{i=1}^{n}D(x^i)\frac{\partial f}{\partial x^i}(p). \]
	This is in fact a directional derivative in the direction $ v = \< D(x^1), D(x^2), \cdots, D(x^n) \> $. So for every $ D $ in $ \mathbb{D} $ we can find a vector in $ T_p(\R^n) $. Thus $ \phi $ is surjective.
\end{proof}

\begin{observation}
	Let $ D $ be a point derivation of $ C_p^\infty (U) $. This corresponds to the directional derivative at $ p $ corresponding to the vector 
	\[ v = \< D(x^1), D(x^2), \cdots, D(x^n) \>. \]
\end{observation}

\begin{observation}
	Let $ p \in U \subset \R^n $. Then 
	\[ T_p(\R^n) \equiv \mathcal{D}(C_p^\infty(U)) \equiv \mathbb{D}(C_p^\infty(U)), \]
	i.e. they are all isomorphic linear spaces.
\end{observation}

Because of the observation above, we identify the standard basis $ \set{e^1, e^2, \cdots, e^n} $ with the partial derivatives $ \set{\frac{\partial}{\partial x^1}, \frac{\partial}{\partial x^2}, \cdots,\frac{\partial}{\partial x^n}}. $ Thus we can write a vector $ v \in T_p(\R^n) $ as 
\[ v = \sum_{i=1}^{n} v^i \frac{\partial}{\partial x^i}. \]

\section{Solved Problems}
\begin{problem}[Algebra structure on $ C_p^{\infty} $]
	Define carefully addition, multiplication, and scalar multiplication in $ C_p^\infty $. Prove that addition in $ C_p^\infty $ is commutative.
\end{problem}
\begin{solution}
	First, note that the elements of $ C_p^\infty $ are actually the equivalence classes, where two functions are equivalent if they both define the same germ.
	\begin{itemize}
		\item For the definition of the addition, we can use the point-wise addition of the functions. However, we need to check to see if this definition is well-defined (i.e. the result of the addition of two functions does not depend on the choice of representative of the equivalence class). Let $ f_1, f_2, g_1, g_2 \in C_p^\infty$ where $ f_1 $ and $ f_2 $ define the same germ, and similarly for $ g_1 $ and $ g_2 $. Then, we claim that $ f_1+g_1 $ define the same germ as $ f_2 + g_2 $. That is because for $ f_1, f_2 $ there is an open set $ U_1 $ containing $ p $ where $ f_1(x) = f_2(x) \ \forall x \in U $. Similarly, there is an open set $ U_2 $ that contains $ p $ and for all $ x \in U_2 $ we have $ g_1(x) = g_2(x) $. Let $ W = U_1 \cap U_2 $. Then on for all $ x \in W $ we have $ f_1(x) + f_2(x) = g_1(x) + g_2(x) $. Hence $ f_1+g_1 $ defines the same germ as $ f_2 + g_2 $.
		\item For the scalar multiplication, we can use the notion of scalar multiplication in functions, and following an idea similar to the reasoning above, we can show that this definition is well-defined.
		\item For the multiplication on $ C_p^\infty $ we can use of the point-wise multiplication of functions as the definition, and with a similar reasoning to the one in item 1, we can show that this definition is well-defined.
	\end{itemize}
	For the commutativity of the addition on $ C_p^\infty $, we need to emphasis that it follows immediately from the commutativity of the point-wise addition of functions.
	
\end{solution}


\begin{problem}[Vector space structure on derivations at a point]
	Prove that the set of all point derivatives is closed under addition and scalar multiplication.
\end{problem}
\begin{solution}
	Let $ D $ and $ D' $ be derivations at $ p\in\R^n $, and define $ \hat{D} = D + D' $. Let $ f,g \in C_p^\infty $. Then we can write
	\begin{align*}
		\hat{D} (fg) = (D + D')(fg)
	\end{align*}
	On the other hand we have
	\[ D(fg) = D(f)g + fD(g), \qquad D'(fg) = D'(f)g + fD'(g). \]
	Adding two equations we will get
	\[ D(fg) + D'(fg) = (D(f) + D'(f))g + f(D(g)+D'(g)) \]
	Defining $ \hat{D} = (D+D')(f) = D(f)+D'(f) $ we will get
	\[ \hat{D}(fg) = \hat{D}(f) g + f \hat{D}(g).  \]
	which shows that $ \hat{D} $ is also a point derivation at $ p $.
	For the scalar multiplication, let $ r \in \R $ and define $ \tilde{D} = rD $. By defining $ (rD)(f) = rD(f) $, we can write
	\[ (rD)(fg) = rD(fg) = rD(f)g + rfD(g) = rD(f)g + frD(g),  \]
	which shows that $ rD $ also satisfies the Leibniz property, this it is a point derivation.
\end{solution}

\begin{problem}
	Let $ A $ be an algebra over a field $ K $. If $ D_1 $ and $ D_2 $ are derivations of $ A $, show that $ D_1\circ D_2 $ is not necessarily a derivation (it is if $ D_1 $ or $ D_2 = 0 $), but $ D_1\circ D_2 - D_2 \circ D_1 $ is always a derivation of $ A $.	
\end{problem}
\begin{solution}
	{\color{orange} TO BE ADDED. }
\end{solution}



\begin{problem}
	Find the inversions in the permutation $ \tau = (1\ 2\ 3\ 4\ 5) $.
\end{problem}
\begin{solution}
	This permutation can be written as
	\[ \begin{pmatrix}
		1 & 2 & 3 & 4 & 5 \\
		2 & 3 & 4 & 5 & 6
	\end{pmatrix}. \]
	Thus the number of inversions can be written as $ (2,1),(3,1),(4,1)$, and $ (5,1) $.
\end{solution}

\begin{problem}
	Let $ f:V^k \to \R $ be a k-linear function defined on vector space $ V $. Show that the following functions is alternating.
	\[ Af = \sum_{\sigma \in S_k} \sign(\sigma)\ \sigma f. \]
\end{problem}
\begin{solution}
	Let $ \tau \in S_k $. Then
	\[ \tau (Af)  = \sum_{\sigma \in S_k} \sign(\sigma)\ (\tau \sigma)f = \sum_{\sigma \in S_k} \sign(\sigma)\sign(\tau)\sign(\tau)\ (\tau \sigma)f = \sign(\tau) \sum_{\sigma \in S_k} \sign(\tau\sigma)\ (\tau \sigma)f  \]
	Note that since in the sum above $ \sigma $ runs through all permutations $ S_k $, so does $ \tau \sigma $. Thus we can write
	\[ \tau (A f) = \sign(\tau) (Af). \]
	This shows that $ Af $ is alternating.
\end{solution}


\begin{problem}
	Let $ f: V^k \to \R $ be a k-linear function. Show that $ Sf $ given below is symmetric.
	\[ Sf = \sum_{\sigma \in S_k} \sigma f. \]
\end{problem}
\begin{solution}
	Let $ \tau \in S_k $. Then we
	\[ \tau (Sf) = \sum_{\sigma \in S_k} \tau \sigma f = Sf. \]
	Note that the last equality above holds, since $ \sigma $ rums through all permutations $ S_k $ and so does $ \tau \sigma. $
\end{solution}