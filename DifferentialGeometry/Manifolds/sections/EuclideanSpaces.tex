\chapter{Euclidean Spaces}

\section{Basic Notions and Definitions}

\subsection{A Review on the algebraic structures}

Here in this chapter I will be covering the details of some notions that was challenging for me do digest in the first read.


\begin{definition}[Axioms of Group]
	Group is a set $ A $ along with a binary operation $ *: A\times A \to A $ that satisfies the following properties. Let $ a,b,c \in A $, then
	\begin{itemize}
		\item \textbf{Associativity}: $ a*(b*c) = (a*b)*c $.
		\item \textbf{Identity element}: $ \exists 1 \in A $ such that 
		\[ 1*a = a*1 = a. \]
		\item \textbf{Inverse element}: $ \forall a \in A\ \exists\hat{a}\in A $ such that 
		\[ a*\hat{a} = \hat{a}*a = 1. \]
	\end{itemize}
\end{definition}
\begin{remark}
	A set along with a binary operation that does not satisfy any properties is called a \textbf{magma}. If the binary operation is only associative, then we are dealing with \textbf{semi-group}. If the binary operation has an identity element as well, then we call this algebraic structure as \textbf{monoid}.
\end{remark}

\begin{definition}[Axioms of Ring]
	A ring is a set $ R $ along with two operations $ +: R\times R \to R $ and $ *: R\times R \to R $, where
	\begin{itemize}
		\item $ (R,+) $ is an Abelian group.
		\item $ (R,*) $ is a monoid.
		\item The operator $ (*) $ has distributive (left and right) law over $ (+) $ i.e.
  			\[a*(b+c) = (a*b)+(a*c), \qquad (b+c)*a = (b*a) + (c*a).\].
	\end{itemize}
\end{definition}

\begin{remark}
	\textbf{Field} is a ring where every non-zero element (i.e. inverse element in the $ (R,+) $ group in the ring) has a multiplicative inverse.
\end{remark}

\begin{definition}[Axioms of Module]
	A \textbf{module} is a group $ M $ along with a ring $ R $ where the monoid of the ring acts on $ M $ (through scalar multiplication) (i.e. it satisfies the idenity and compatibility properties) and satisfies the distributive property. I.e.
	\begin{itemize}
		\item \textbf{Compatibility of the monoid action}: $ a,b \in R,\ u \in M $ then 
		\[ a(bu) = (ab)u. \]
		\item \textbf{Identity of the monoid action}: Let $ 1 $ be the identity element of the ring $ R $. Then $ \forall u \in M $
		\[1u = u1 = u. \]
		\item \textbf{Distribution law}: $ a,b \in R $ and $ u,v \in M $ then
		\begin{itemize}
			\item $ (a+b)u = au + bu $.
			\item $ a(u+v) = au + av $.
		\end{itemize}
	\end{itemize}
\end{definition}
\begin{remark}
	A module $ (M,R) $ is called a \textbf{vector space}, if the \textbf{ring} $ R $ is a \textbf{field}.
\end{remark}

\begin{definition}[Axioms of Algebra]
	\label{def:algebra}
	An Algebra over field $ F $ is a ring $ A $ that $ F $ acts on it (thus $ A $ has vector space structure as well), where the monoid operation of $ F $ (i.e. multiplication) satisfies the homogeneity property. I.e. for $ r \in F $ and $ u,v \in A $ we have
	\[ r(uv) = (ru)v = u (rv). \]
\end{definition}

There are some important observations when combining different algebraic structures with each other to get a new one. The first is that when we combine two structures with different operators, then the operators need to satisfy the distributive laws. Also, note that when an algebraic structure (like group or monoid) acts on another algebraic structure, we need to have the identity and and compatibility conditions satisfied.

The following diagram shows how different algebraic structures are combined with each other to produce another structure.


\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{Images/algebraicStructures.pdf}
\end{figure}
\FloatBarrier
Note that in the figure above, I have used some non-standard notations to make the figure concise. For instance, the expression ``\textbf{$ R_{\text{mon}} @ M \ \text{with $\cdot$}$}'' means that the monoid structure in the field $ R $ acts on the group $ M $ with the ($ \cdot $) symbol. Or the expression ``\textbf{$ \times $ in $ M_{\text{mon}} $ satisfies homogen cond.}'' means the multiplication operation of the monoid structure inside the ring $ M $ satisfies the homogeneity condition (see the definition of the algebra in \autoref{def:algebra} ). Finally, $ M_{g} $ means the group structure inside the ring $ M $.


\subsection{Directional Derivative}
The notion of directional derivative is very central in generalization of the multi-variable calculus manifolds. 

\begin{definition}[Directional derivative]
	\label{def:directionalDerivative}
	Let $ f:U \to \R $ where $ U \subset \R^n $ and $ f \in C^\infty(U) $. Then we define a directional derivative at $ p\in U $ and in the direction $ v \in T_p(\R^n) $ as 
	\[ D_v f \big|_p = \lim_{t\to 0 } \frac{f(p + tv) - f(p)}{t} = \frac{d}{dt}\big|_{t=0} f(p+vt). \]
	We denote the set of all directional derivatives at $ p $ by $ \mathcal{D}(C_p^\infty(U)) $.
\end{definition}
\begin{remark}
	By the chain rule we have
	\[ D_v f \big|_p = \sum_{i=1}^{n} v^i \frac{\partial f}{\partial x^i}(p). \]
\end{remark}


\begin{proposition}[Directional derivative is $ R-$linear map satisfying the Leibniz rule]
	A directional derivative $ D_v $ at point $ p $ is a $ \R-$linear operator that maps  \[f\in C_p^\infty \mapsto D_vf \in \R\] such that satisfies the Leibniz rule,
	\[ D_v(fg) = D_v(f)g + fD_v(g). \]
\end{proposition}
\begin{proof}
	Let $ D_v \in \mathcal{D}(C_p^\infty(U)) $. Then by the remark above we can write it as  
	\[ D_v f \big|_p = \sum_{i=1}^{n} v^i \frac{\partial f}{\partial x^i}(p). \]
	Since each of the partial derivatives above are $ \R-$linear and satisfy the Leibniz rule, then $ D_v $ inherits those properties as well.
\end{proof}

\section{Tangent Spaces and Derivations}

\begin{definition}[Informal definition of tangent space]
	Let $ p \in \R^n $. A tangent space at $ p $ denoted by $ T_p(\R^n) $ is a linear space containing all of vectors emerging from $ p $.
\end{definition}
\begin{remark}
	Note that the definition above is an informal definition of the tangent space. For a more formal and technical definition, we can use the notion of curves, or the notion of manifolds. We wont' touch this level of technicality in the early chapters.
\end{remark}

To distinguish between points in $ \R^n $ and vectors in $ T_p(\R^n) $ we denote a point $ p\in\R^n $ by 
\[ p = (p^1, p^2, \cdots, p^n), \]
while for a vector $ v \in T_p(\R^n) $ we write
\[ v = \<  v_1,v_2,\cdots,v_n \>. \]

As we observed in \autoref{def:directionalDerivative}, we have a natural one-to-one correspondence between each $  v\in T_p(\R^n) $ and a $ D_v \in \mathcal{D}(C_p^\infty(U)) $, i.e. these linear spaces are isomorphic.

\begin{proposition}
	Let $ p \in\R^n $. The set of all directional derivatives at $ p $, i.e. $ \mathcal{D}(C_p^\infty(U)) $ is isomorphic to the tangent space at $ p $ i.e. $ T_p(\R^n) $.
\end{proposition}
\begin{proof}
	Proof follows immediately from the following natural association in the definition of the directional derivative.
	
	\[ D_v f \big|_p \longleftrightarrow \sum_{i=1}^{n} v^i \frac{\partial f}{\partial x^i}(p) \]
\end{proof}



\begin{definition}[Point derivations at a point]
	\label{def:pointDerivation}
	Let $ p \in \R^n $, and $ U $ an open set containing $ p $. A \textbf{derivation at $ p $} or a \textbf{point-derivation of $ C_p^\infty(U) $} is a \emph{linear} operator
	\[ D: C_p^\infty(U) \to \R \]
	such that satisfies the Leibniz property, i.e. for $ f,g \in C_p^\infty(U) $ we have
	\[ D(fg) = D(f)g - fD(g). \]
	We denote the set of all such maps as $ \mathbb{D}(C_p^\infty(U)) $.
\end{definition}

\begin{remark}
	We know that a directional derivative at $ p $ satisfies the Leibniz rule. Thus $ \mathcal{D}(C_p^\infty(U)) \subset \mathbb{D}(C_p^\infty(U))$. On the other hand, we know that both $ \mathcal{D} $ and $ \mathbb{D} $ are linear spaces. So $ \mathcal{D}(C_p^\infty(U)) $ is in fact a \emph{linear subspace} of $\mathbb{D}(C_p^\infty(U))$. Thus the zero of these two linear spaces match.
\end{remark}


The following Lemma follows form the algebraic property of the point derivation.

\begin{lemma}
	\label{lem:PointDerivationOfConstantFunction}
	Let $ D $ be a point derivation of $ C_p^\infty(U)$. Then $ D(c) = 0 $ for any constant function  $ c $. 
\end{lemma}
\begin{proof}
	Let $ c $ be a constant function, i.e. a real number. Since $ D $ is $ R- $linear, then we can write $ D(c) = c D(1). $ On the other hand, from the Leibniz property we can write
	\[ D(1) = D(1 \cdot 1) = D(1) + D(1) = 2D(1) \]
	Thus $ D(1) = 0 $, which implies $ D(c) = 0 $.
\end{proof}

The following theorem is very important as it states that all of the point derivations are in fact the directional derivatives and vise-versa. This is a very interesting result, since we are in fact stating that an operator is a point derivative if and only if it satisfies an algebraic property. This means that we can abstract away all of the detailed limit processes in the definition of derivative, and replace that with an axiomatic requirement which is a purely algebraic property. We can see these kind of ideas, i.e. axiomatic generalization all over the mathematics. For instance, in the PDE theory, at some point we need to relax the definition of derivative and talk about the weak derivatives. To do so, we get an identity that derivatives satisfy and carefully use that identity to define the notion of weak derivatives. 

\begin{theorem}[The set of all point-derivations is isomorphic to the set of all directional derivatives]
	Let $ p \in U \subset \R^n $. Then $ \mathbb{D}(C_p^\infty(U)) $ is isomorphic to $ T_p(\R^n) $.
\end{theorem}

\begin{proof}
	Let $ \phi : T_p(\R^n) \to \mathbb{D}(C_p^\infty(U))  $ be a linear isomorphism between the linear spaces. We need to show that this map is surjective and bijective. To show the bijectivity, we use the fact that a linear map is bijective if and only if its kernel is a singleton. To find the kernel of the map, let need to find all points in $ T_p(\R^n) $ that maps to the zero of $ \mathbb{D} $. As we discussed in the remark of \autoref{def:pointDerivation}, the zero $ \mathbb{D} $ is the same as the zero directional derivative, i.e. $ D_v = 0 $. We need to prove that $ v $ is the zero vector, i.e. the zero of $ T_p(\R^n) $. To do this, we apply the $ D_v $ to the coordinate functions
	\[ 0 = D_v(x^i) = \sum_{j=1}^n v^j \frac{\partial x^i}{\partial x^j} = v^j\]
	Thus $ v = 0 \in T_p(\R^n) $, thus $ \phi $ is injective. In other words, the injectivity follows immediately from $ T_p(\R^n) $ being isomorphic to $  \mathcal{D} $, and $  \mathcal{D} $ being a linear subspace of $ \mathbb{D} $.
	
	To prove the surjectivity, let $ D $ be a point derivation at $ p $, and let $ (f,V) $ be a representative of a germ in $ C_p^\infty $. Marking $ V $ smaller if necessary, we may assume that $ V $ is an open ball, hence star shaped. By Taylor's approximation theorem we know there exists $ C^\infty $ functions $ g_i(x) $ such that
	\[ f(x) = f(p) + \sum_{i=1}^{n} (x^i-p^i)g_i(x),\qquad g_i(p) = \frac{\partial f}{\partial x^i}(p). \]
	From Lemma \autoref{lem:PointDerivationOfConstantFunction}, we know that $ D(f(p)) = 0$ as well as $ D(p^i) = 0 $. Thus we can write
	\[ D(f(x)) = \sum_{i=1}^{n}(D(x^i)g_i(x) + (p^i - p^i)D(g_i(x))) =  \sum_{i=1}^{n}D(x^i)g_i(x) = \sum_{i=1}^{n}D(x^i)\frac{\partial f}{\partial x^i}(p). \]
	This is in fact a directional derivative in the direction $ v = \< D(x^1), D(x^2), \cdots, D(x^n) \> $. So for every $ D $ in $ \mathbb{D} $ we can find a vector in $ T_p(\R^n) $. Thus $ \phi $ is surjective.
\end{proof}

\begin{observation}
	Let $ D $ be a point derivation of $ C_p^\infty (U) $. This corresponds to the directional derivative at $ p $ corresponding to the vector 
	\[ v = \< D(x^1), D(x^2), \cdots, D(x^n) \>. \]
\end{observation}

\begin{observation}
	Let $ p \in U \subset \R^n $. Then 
	\[ T_p(\R^n) \equiv \mathcal{D}(C_p^\infty(U)) \equiv \mathbb{D}(C_p^\infty(U)), \]
	i.e. they are all isomorphic linear spaces.
\end{observation}

Because of the observation above, we identify the standard basis $ \set{e^1, e^2, \cdots, e^n} $ with the partial derivatives $ \set{\frac{\partial}{\partial x^1}, \frac{\partial}{\partial x^2}, \cdots,\frac{\partial}{\partial x^n}}. $ Thus we can write a vector $ v \in T_p(\R^n) $ as 
\[ v = \sum_{i=1}^{n} v^i \frac{\partial}{\partial x^i}. \]


\section{Summary}

\begin{summary}
	Let $ f \in A_k(V) $ and $ g \in A_l(V) $. Then
	\[ f\wedge g = (-1)^{kl}(g\wedge f). \]
	In particular, if $ f $ is odd-linear function, then 
	\[ f \wedge f = 0. \]
\end{summary}

\begin{summary}
	Consider the following $ n\times n $ matrix
	\[ 
	M = \begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\
		a_{21} & a_{22} & \cdots & a_{2n} \\
		\vdots & \vdots & \ddots & \vdots \\
		a_{n1} & a_{n2} & \cdots & a_{nn}
	\end{pmatrix}
	 \]
	The, determinant is defined to be
	\[ \det(A) = \sum_{\sigma \in S_n} \sign(\sigma)\ a_{1\sigma(1)}a_{2\sigma(2)}\dots a_{n\sigma(n)} \]
\end{summary}

\begin{summary}
	Let $ V $ be a vector space, and $ \alpha^1,\cdots, \alpha^k $ be 1-linear functions (i.e. 1-covectors) defined on $ V $. Then we have
	\[ (\alpha^1  \wedge \cdots \wedge \alpha^k)(v_1,\cdots,v_k) = \det\left[ \alpha^i(v_j) \right]. \] 
	In particular, For two 1-covectors we have
	\[ (\alpha^1 \wedge \alpha^2)(v_1,v_2) = 
	\det \matt{\alpha^1(v_1)}{\alpha^1(v_2)}{\alpha^2(v_1)}{\alpha^2(v_2)}
	 \]
\end{summary}

\begin{summary}
	Let $ V $ be a vector space with dimension $ n $, with $ \set{e_1,\cdots,e_n} $ as its basis and $ \set{\alpha^1, \alpha^2, \cdots, \alpha^n} $ as its dual basis. Let e$ A_k(V) $ be the set of all \textit{alternating} k-linear functions defined on $ V $. 
	Then we have
	\[ \dim A_k = \binom{n}{k}. \]
	In particular, let $ V = \R^n $ with standard basis $ \set{e_1,e_2,e_3} $, and the corresponding dual basis $ \set{\alpha^1,\alpha^2,\alpha^3} $. Then we have
	\begin{align*}
		\mathbb{B}(A_0) &= \set{1 \in \R}\\
		\mathbb{B}(A_1) &= \set{\alpha^1,\alpha^2,\alpha^3},\\
		\mathbb{B}(A_2) &= \set{\alpha^1 \wedge \alpha^2, \alpha^1 \wedge \alpha^3, \alpha^2 \wedge \alpha^3},\\
		\mathbb{B}(A_3) &= \set{\alpha^1 \wedge \alpha^2 \wedge \alpha^3}.
	\end{align*}
	It is clear from the basis sets above that the dimensions of $ A_0,A_1,A_2 $ and $ A_3 $ are $ 1,3,3,1 $ respectively.
	
	Also, observe that $ A_k = 0 $ for all $ k > n $. This follows from the fact that $ f\wedge f = 0 $ for $ f $ a 1-covector.
\end{summary}

\begin{summary}
	Let $ V $ be $ n $ dimensional vector space with a basis $ \set{e_1,\cdots,e_n} $, and $ \set{\alpha^1,\cdots,\alpha^n} $ as its dual basis. Let $ L_k(V) $ be the set of all $ k $-linear functions defined on $ V $. Then a basis for $ L_k(V) $ is
	\[ \mathcal{B} = \set{\alpha^{i_1} \otimes \cdots \otimes \alpha^{i_k}} \]
	for all multi-indices $ (i_1,\cdots,i_k) $. This shows that 
	\[ \dim L_k(V) = n^k. \]
	For instance, Let $ V = \R^2 $. Then for $ L_1(V) $ we have
	\[ \mathcal{B}(L_1(V)) = \set{\alpha^1,\alpha^2,\alpha^3}. \]
	Similarly, for $ L_2(V) $ we have
	\[ \mathcal{B}(L_2(V)) = \set{\alpha^1\otimes \alpha^2,\alpha^1\otimes \alpha^3,\alpha^2\otimes \alpha^3,\alpha^2\otimes \alpha^1,\alpha^3\otimes \alpha^1,\alpha^3\otimes \alpha^2,\alpha^1\otimes\alpha^1,\alpha^2\otimes\alpha^2,\alpha^3\otimes\alpha^3} \]
	For a side by side comparison, compare it with the basis for $ A_2(V) $ that is 
	\[ \mathcal{B}(A_2(V)) = \set{\alpha^1 \wedge \alpha^2, \alpha^2 \wedge \alpha^3, \alpha^1 \wedge \alpha^3}. \]

	
\end{summary}

\begin{summary}
		Looking at the dimension $ L_1(V), L_2(V), \cdots $, it reveals the fact that why most of the non-technical people are interested in interpreting tensors as multi-dimensional matrices! For instance, when $ V = \R^2 $ and $ K=3 $, then $ L_3(V) $ has dimension $ 2^3 = 8 $ with the basis
	\[
	\mathcal{B} (\mathbb{R}^2) = 
	\begin{aligned}
		\{&\alpha^1 \otimes \alpha^1 \otimes \alpha^1, \alpha^1 \otimes \alpha^1 \otimes \alpha^2, \alpha^1 \otimes \alpha^2 \otimes \alpha^1, \alpha^1 \otimes \alpha^2 \otimes \alpha^2, \\
		&\alpha^2 \otimes \alpha^1 \otimes \alpha^1, \alpha^2 \otimes \alpha^1 \otimes \alpha^2, \alpha^2 \otimes \alpha^2 \otimes \alpha^1, \alpha^2 \otimes \alpha^2 \otimes \alpha^2 \}
	\end{aligned}
	\]
	It is very tempting to think of elements of $ L_3(V) $ as $ 2\times 2\times 2 $ matrix like structures. In the case of $ L_2(V) $ we can identify $ L_2(V) $ with the set of all $ 2\times 2 $ matrices.
\end{summary}

\section{Solved Problems}
\begin{problem}[Algebra structure on $ C_p^{\infty} $]
	Define carefully addition, multiplication, and scalar multiplication in $ C_p^\infty $. Prove that addition in $ C_p^\infty $ is commutative.
\end{problem}
\begin{solution}
	First, note that the elements of $ C_p^\infty $ are actually the equivalence classes, where two functions are equivalent if they both define the same germ.
	\begin{itemize}
		\item For the definition of the addition, we can use the point-wise addition of the functions. However, we need to check to see if this definition is well-defined (i.e. the result of the addition of two functions does not depend on the choice of representative of the equivalence class). Let $ f_1, f_2, g_1, g_2 \in C_p^\infty$ where $ f_1 $ and $ f_2 $ define the same germ, and similarly for $ g_1 $ and $ g_2 $. Then, we claim that $ f_1+g_1 $ define the same germ as $ f_2 + g_2 $. That is because for $ f_1, f_2 $ there is an open set $ U_1 $ containing $ p $ where $ f_1(x) = f_2(x) \ \forall x \in U $. Similarly, there is an open set $ U_2 $ that contains $ p $ and for all $ x \in U_2 $ we have $ g_1(x) = g_2(x) $. Let $ W = U_1 \cap U_2 $. Then on for all $ x \in W $ we have $ f_1(x) + f_2(x) = g_1(x) + g_2(x) $. Hence $ f_1+g_1 $ defines the same germ as $ f_2 + g_2 $.
		\item For the scalar multiplication, we can use the notion of scalar multiplication in functions, and following an idea similar to the reasoning above, we can show that this definition is well-defined.
		\item For the multiplication on $ C_p^\infty $ we can use of the point-wise multiplication of functions as the definition, and with a similar reasoning to the one in item 1, we can show that this definition is well-defined.
	\end{itemize}
	For the commutativity of the addition on $ C_p^\infty $, we need to emphasis that it follows immediately from the commutativity of the point-wise addition of functions.
	
\end{solution}


\begin{problem}[Vector space structure on derivations at a point]
	Prove that the set of all point derivatives is closed under addition and scalar multiplication.
\end{problem}
\begin{solution}
	Let $ D $ and $ D' $ be derivations at $ p\in\R^n $, and define $ \hat{D} = D + D' $. Let $ f,g \in C_p^\infty $. Then we can write
	\begin{align*}
		\hat{D} (fg) = (D + D')(fg)
	\end{align*}
	On the other hand we have
	\[ D(fg) = D(f)g + fD(g), \qquad D'(fg) = D'(f)g + fD'(g). \]
	Adding two equations we will get
	\[ D(fg) + D'(fg) = (D(f) + D'(f))g + f(D(g)+D'(g)) \]
	Defining $ \hat{D} = (D+D')(f) = D(f)+D'(f) $ we will get
	\[ \hat{D}(fg) = \hat{D}(f) g + f \hat{D}(g).  \]
	which shows that $ \hat{D} $ is also a point derivation at $ p $.
	For the scalar multiplication, let $ r \in \R $ and define $ \tilde{D} = rD $. By defining $ (rD)(f) = rD(f) $, we can write
	\[ (rD)(fg) = rD(fg) = rD(f)g + rfD(g) = rD(f)g + frD(g),  \]
	which shows that $ rD $ also satisfies the Leibniz property, this it is a point derivation.
\end{solution}

\begin{problem}
	Let $ A $ be an algebra over a field $ K $. If $ D_1 $ and $ D_2 $ are derivations of $ A $, show that $ D_1\circ D_2 $ is not necessarily a derivation (it is if $ D_1 $ or $ D_2 = 0 $), but $ D_1\circ D_2 - D_2 \circ D_1 $ is always a derivation of $ A $.	
\end{problem}
\begin{solution}
	{\color{orange} TO BE ADDED. }
\end{solution}



\begin{problem}
	Find the inversions in the permutation $ \tau = (1\ 2\ 3\ 4\ 5) $.
\end{problem}
\begin{solution}
	This permutation can be written as
	\[ \begin{pmatrix}
		1 & 2 & 3 & 4 & 5 \\
		2 & 3 & 4 & 5 & 6
	\end{pmatrix}. \]
	Thus the number of inversions can be written as $ (2,1),(3,1),(4,1)$, and $ (5,1) $.
\end{solution}

\begin{problem}
	Let $ f:V^k \to \R $ be a k-linear function defined on vector space $ V $. Show that the following functions is alternating.
	\[ Af = \sum_{\sigma \in S_k} \sign(\sigma)\ \sigma f. \]
\end{problem}
\begin{solution}
	Let $ \tau \in S_k $. Then
	\[ \tau (Af)  = \sum_{\sigma \in S_k} \sign(\sigma)\ (\tau \sigma)f = \sum_{\sigma \in S_k} \sign(\sigma)\sign(\tau)\sign(\tau)\ (\tau \sigma)f = \sign(\tau) \sum_{\sigma \in S_k} \sign(\tau\sigma)\ (\tau \sigma)f  \]
	Note that since in the sum above $ \sigma $ runs through all permutations $ S_k $, so does $ \tau \sigma $. Thus we can write
	\[ \tau (A f) = \sign(\tau) (Af). \]
	This shows that $ Af $ is alternating.
\end{solution}


\begin{problem}
	Let $ f: V^k \to \R $ be a k-linear function. Show that $ Sf $ given below is symmetric.
	\[ Sf = \sum_{\sigma \in S_k} \sigma f. \]
\end{problem}
\begin{solution}
	Let $ \tau \in S_k $. Then we
	\[ \tau (Sf) = \sum_{\sigma \in S_k} \tau \sigma f = Sf. \]
	Note that the last equality above holds, since $ \sigma $ rums through all permutations $ S_k $ and so does $ \tau \sigma. $
\end{solution}


\begin{problem}
	If $ f $ is a 3-linear function on a vector space $ V $ and $ v_1, v_2, v_3 \in V $, what is $ (Af)(v_1, v_2, v_3) $?
\end{problem}
\begin{solution}
	\[ (Af)(v_1, v_2, v_3) = f(v_1,v_2,v_3)-f(v_2,v_1,v_3)+f(v_3,v_1,v_2)-f(v_1,v_3,v_2)+f(v_2,v_3,v_1)-f(v_3,v_2,v_1) \]
\end{solution}

\begin{problem}
	Show that the tensor product of multi-linear functions is associative: If $ f,g $, and $ h $ are multi-linear functions on $ V $, then
	\[ (f\otimes g)\otimes h = f \otimes (g\otimes h). \]
\end{problem}
\begin{solution}
	We start with the left hand side. I.e. 

	\begin{align*}
		((f \otimes g) \otimes h)(v_1 \cdots, v_{k+l+m}) 
		&= (f \otimes g)(v_1,\cdots,v_{k+l})h(k_{k+l+1},\cdots,k_{k+l+m}) \\ 
		&= \big(f(v_1,\cdots,v_k)g(v_{k+1},\cdots,v_{k+l})\big)h(v_{k+l+1},\cdots,v_{k+l+m})\\
		&= f(v_1,\cdots,v_k) \big(g(v_{k+1},\cdots,v_{k+l})h(v_{k+l+1},\cdots,v_{k+l+m})\big) \\
		&= (f \otimes (g \otimes h))(v_{1},\cdots,v_{k+l+m}).
	\end{align*}
\end{solution}

\begin{problem}
	Consider following two ways that we can express the sum in the wedge product formula. Let $ f \in A_k(V) $ and $ g \in A_l(V) $. Then the wedge product $ f \wedge g $ can be written as
	\[ (f \wedge g)(v_1,\cdots,v_{k+l}) = \frac{1}{k!l!}\sum_{\sigma \in S_k}\sign(\sigma)\ \sigma f(v_{\sigma(1)},\cdots,v_{\sigma(k)})g(v_{\sigma(k+1)},\cdots,v_{\sigma(k+l)}). \]
	Or, alternatively, we can write it as $ (k,l)-$shuffle
	\[ (f \wedge g)(v_1,\cdots,v_{k+l}) = \sum_{(k,l)- \text{shuffles} \ \sigma}\sign(\sigma)\ \sigma f(v_{\sigma(1)},\cdots,v_{\sigma(k)})g(v_{\sigma(k+1)},\cdots,v_{\sigma(k+l)}). \]
	Now, as a concrete example, let $ f,g \in A_2(V) $. Write $ f\wedge g $ in these two forms.
\end{problem}

\begin{solution}
	First, we want to use the sum on the all permutations. 
	\begin{align*}
		 4(f \wedge g)(v_1, v_2, v_3, v_4) = f(v_1,v_2)g(v_3,v_4) - f(v_2,v_1)g(v_3,v_4) + f(v_2,v_1)g(v_4,v_3) - f(v_1,v_2)g(v_4,v_3)  \\
		 - f(v_1,v_3)g(v_2,v_4) + f(v_3,v_1)g(v_2,v_4) - f(v_3,v_1)g(v_4,v_2) + f(v_1,v_3)g(v_4,v_2)   \\
		 + f(v_1,v_4)g(v_2,v_3) - f(v_4,v_1)g(v_2,v_3) + f(v_4,v_1)g(v_3,v_2) - f(v_1,v_4)g(v_3,v_2)  \\
		 +f(v_2,v_3)g(v_1,v_4) - f(v_3,v_2)g(v_1,v_4) + f(v_3,v_2)g(v_4,v_1) - f(v_2,v_3)g(v_4,v_1)   \\
		 - f(v_2,v_4)g(v_1,v_3) + f(v_4,v_2)g(v_1,v_3) - f(v_4,v_2)g(v_3,v_1) + f(v_2,v_4)g(v_3,v_1)  \\
		 +f(v_3,v_4)g(v_1,v_2) - f(v_4,v_3)g(v_1,v_2) + f(v_4,v_3)g(v_2,v_1) - f(v_3,v_4)g(v_2,v_1).
	\end{align*}
	Note that in every row, the functions are equal to each at (because $ f,g $ are alternating). Thus we have the factor of 4 not to count the redundant terms. However, we can do this sum using the (2,2)-shuffles. This means that we only keep the very first column, as their argument permutation is the same as all (2,2)-shuffles on 4 symbols. Thus we can write
	\begin{align*}
		(f\wedge g)(v_1,v_2,v_3,v_4) = &f(v_1,v_2)g(v_3,v_4) - f(v_1,v_3)g(v_2,v_4) + f(v_1,v_4)g(v_2,v_3)\\
		 + &f(v_2,v_3)g(v_1,v_4) - f(v_2,v_4)g(v_1,v_3)\\
		 + &f(v_3,v_4)g(v_1,v_2).
	\end{align*}
\end{solution}
\begin{observation}
	How do we count the (2,2)-shuffles of $ \set{v_1,v_2,v_3,v_4} $? We start by a vertical line where in its left side we put $ v_1,v_2 $ and in its right side we write $ v_3,v_4 $. Like the following table. This is already a shuffle (identity). To write the next shuffle, we keep $ v_1 $ in the first position, and write the next symbol whose its subscript is larger than 2 (i.e. $ v_3 $), and write the remaining symbols in the right hand side of the vertical line in increasing order. Then we continue this process, until there are no possible shuffles for its first element be $ v_1 $. Then we put $ v_2 $ in the first place and right next to it we write the next symbol that its subscript is larger than 2 (i.e. $ v_3 $), and we continue.

	\begin{center}
		\begin{tabular}{c|c}
			$ v_1,v_2 $ & $ v_3,v_4 $  \\
			$ v_1,v_3 $ & $ v_2,v_4 $ \\
			$ v_1,v_4 $ & $ v_2,v_3 $ \\
			& \\
			$ v_2,v_3 $ & $ v_1,v_4 $ \\
			$ v_2,v_4 $ & $ v_1,v_3 $ \\
			& \\
			$ v_3,v_4 $ & $ v_1,v_2 $ \\
		\end{tabular}
	\end{center}
	
	In order to find the sign of the each of these permutations, it is enough to extract the independent cycles. For instance, consider the following shuffle, corresponding to last row above.
	\[ \sigma = \begin{pmatrix}
		1 & 2 & 3 & 4 \\
		3 & 4 & 1 & 2
	\end{pmatrix}  \]
	This permutation is the same as the permutation given by the cycle $ \tau = (1\ 3)(2\ 4) $. To extract this we start with $ 1 $ and track where it goes, and we do this tracking until we get back to 1. Then we start this process with the remaining element until we get all of the cycles. This cycle decomposition of $ \tau $ clearly shows that $  \tau $ is an even permutation (can be written as 2 cycles). However, in the case of $ v_2,v_4,v_1,v_3 $, the corresponding permutation is 
	\[ \tau = 
	\begin{pmatrix}
		1 & 2 & 3 & 4 \\
		2 & 4 & 1 & 3
	\end{pmatrix}.
	 \]
	 We can write this as $ \tau = (1\ 2\ 4\ 3) $. As with every cycle, we can decompose this into smaller cycles (not necessarily independent) as $ \tau = (1\ 3)(1\ 4)(1\ 2) $. This shows that this particular permutation is odd.
	
\end{observation}

\begin{problem}
	Verify 
	\[ f \wedge g = (-1)^{k+l}(g\wedge f) \]
	for $ f $ and $ g $ being $ k $ and $ l $ linear maps correspondingly. Do this verification by considering $ f\in A_2(V) $ and $ g \in A_1(V) $.
\end{problem}

\begin{solution}
	For $ f\wedge g $ we can write (using $ (2,1) $-shuffles)
	\[(f\wedge g)(v_1,v_2,v_3) = f(v_1,v_2)g(v_3) - f(v_1,v_3)g(v_2) + f(v_2,v_3)g(v_1).\]
	However, for $ g\wedge f $, we can use $ (1,2) $-shuffles to show
	\[ (g\wedge f)(v_1,v_2,v3) = g(v_1)f(v_2,v_3) - g(v_2)f(v_1,v_3) + g(v_3)f(v_1,v_2).  \]
	Evaluating two expressions above reveals that $ f\wedge g = g \wedge f. $
\end{solution}

\begin{problem}[Tensor product of covectors (from W. Tu)]
	Let $ e_1,e_2,\cdots,e_n $ be a basis for vector space $ V $ and let $ \alpha^1,\cdots,\alpha^n $ be its dual basis in $ V^\vee $. Suppose $ \left[g_{i,j}\right] \in \R^{n\times n} $ is an $ n\times n $ matrix. Define a bilinear function $ f: V\times V \to \R $ by
	\[ f(v,w) = \sum_{1 \leq i,j \leq n} g_{i,j}v^i w^j \]
	for $ v = \sum v^i e_i $ and $ w = \sum w^j e_j $ in $ V $. Describe $ f $ in terms of the tensor products of $ \alpha^i $ and $ \alpha^j $, $ 1 \leq i, j \leq n $.
\end{problem}
\begin{solution}
	We know that $ v^i = \alpha^i (v) $, and similarly $ w^j = \alpha^j (w) $. So we can write
	\[ v^i w^j = \alpha^i(v) \alpha^j (w) = (\alpha^i \otimes \alpha^j)(v,w). \]
	Thus the bilinear function can be written as
	\[ f = \sum_{1\leq i,j \leq n} g_{i,j} \alpha^i \otimes \alpha^j. \]
	This bilinear function is very similar to the notion of weighted inner product.
\end{solution}

\begin{problem}[Hyperplanes(from W. Tu)]
	\begin{enumerate}[(a)]
		\item Let $ V $ be a vector space of dimension $ n $, and $ f:V \to \R$ a nonzero linear functional. Show that $ \dim \ker f  = n-1 $. A linear subspace of $ V $ of dimension $ n-1 $ is called a hyperplane in $ V $.
		\item Show that a nonzero linear functional on a vector space $ V $ is determined up to multiplicative constant by its kernel, a hyperplane in $ V $. In other words, if $ f $ and $ g : V \to \R $ are nonzero linear functionals and $ \ker f = \ker g $, then $ g = cf $ for some constant $ c \in \R $.
	\end{enumerate}
	
\end{problem}

\begin{solution}
	\begin{enumerate}[(a)]
		\item 	We can use the Rank-Nullity theorem for the linear function $ f $. This theorem states that 
		\[ \dim \rank f + \dim \ker f = \dim V. \]
		We know that $ \dim\rank f = 1 $, and $ \dim V = n $. This implies $ \dim\ker f = n-1 $.
		
		\item Let $ f,g $ be two linear functionals on $ V $ that has the same kernel, which we call it the set $ K $. Since $ K $ is $ n-1 $ dimensional linear subspace of $ K $, then we can find a basis for it, call it $ \mathcal{B}_1 = \set{\hat{e}_1,\cdots , \hat{e}_{n-1}} $. Let $ \hat{e}_n $ be a vector normal to $ \mathcal{B}_1 $. We can do all of these since $ V $ is $ n $ dimensional and we have just modified the basis vectors as described above. Let $ z\in V $ where $ z = \sum_{i=1}^{n} z_i \hat{e}_i $. Thus we have
		\[ f(z) = z_n f(\hat{e}_n), \qquad g(z) = z_n g(\hat{e}_n). \]
		This is true because $ f(\hat{e}_i) = g(\hat{e}_i) = 0 $ for all $ \hat{e}_i \in \mathcal{B} $. Since $ f,g $ are non-zero functional, then we can write
		\[ \frac{f(z)}{g(z)} = \frac{f(\hat{e}_n)}{g(\hat{e}_n)} = c \qquad \text{for some $ c\in \R $}\]
		Thus we can write
		\[ f = cg. \]
		Note that $ c $ is determined by $ \hat{e}_n $. $ \hat{e}_n $ itself is determined by the kernel of $ f,g $, i.e. the hyperplane mentioned above.
	\end{enumerate}
\end{solution}

\begin{observation}
	I have this weird observation that I think it must be wrong, but I don't know how! Consider the set of all linear functionals defined on $ \R^2 $. From duality, we know that this set (denoted by $ L_1(\R^2) $) is isomorphic to $ \R^2 $. But here comes the strange argument that leads to the non-real observation. As we saw in the question above, every hyperplane (in this case every line that passes through the origin) determines a linear functional up to a constant multiplication. On the other hand, we know that the set of all liens passing through the origin is the real projective plane (which is in fact $ L_1(\R^2)/\sim $ where $ \sim $ identifies linear functionals that are constant multiple of each other), which definitely is not isomorphic to $ \R^2 $.
\end{observation}

\begin{problem}[A basis for $ k $-tensors (from W. Tu)]
	Let $ V $ be a vector space of dimension $ n $ with basis $ e_1,\cdots,e_n $. Let $ \alpha^1,\cdots,\alpha^n $ be the dual basis for $ V^\vee $. Show that a basis for the space $ L_k(V) $ of k-linear functions on $ V $ is $ \set{\alpha^{i_1}\otimes \cdots \otimes \alpha^{i_k}} $ for all multi-indices $ (i_1,i_2,\cdots,i_k) $. In particular, this show that $\dim L_k(V) = n^k $.
\end{problem}

\begin{solution}
	To show this, we need to show that the basis vectors are linearly independent, and spans all $ L_k(v) $. To show the linear independence, consider the following linear combination of all basis vectors, where the total sum is set to zero.
	\[ \sum C_{i_1,\cdots,i_k} \alpha^{i_1}\otimes \cdots \otimes \alpha^{i_k}  = 0, \]
	where the sum is done on all of the multi-indices $ (i_1,\cdots,i_k) $. This sum implies that all of the coefficients are zero. We can see this by applying $ (e_{j_1},\cdots,e_{j_k}) $ to both sides. We have
	\[ (\alpha^{i_1}\otimes \cdots \otimes \alpha^{i_k})(e_{j_1},\cdots,e_{j_k}) = 0 \]
	if there is any mismatch between the multi-indices $ I, J$. This implies $ C_{j_1,\cdots,j_k} = 0$ for all multi-indices. Thus we can conclude that the basis vectors are linearly independent.
	
	To show that the basis vectors span all the space $ L_k(V) $, let $ f \in L_k(V) $ be any k-tensor. Define
	\[ g = \sum f(e_{i_1},\cdots,e_{i_k})\alpha^{i_1}\otimes \cdots \otimes \alpha^{i_k}. \]
	We can see that 
	\[ g(e_{j_1},\dots,e_{j_k}) = f( e_{j_1},\dots,e_{j_k} ) \qquad \text{for all multi-indices} \]
	From linearity we can deduce that $ f=g $. Thus the basis vectors span the whole space.
\end{solution}