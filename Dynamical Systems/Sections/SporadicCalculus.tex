\section{Random Tips and Tricks}

In this section I will cover the basics of calculus which I have used throughout the text. 

\subsection{Level Curves}
Here, I will focus my arguments to 2D scalar function and vector fields, as it is easier to imagine and also plot. However, it can easily been generalized to higher dimensions. 

Let $F: \R^2 \to \R$ be a function. This function is called a scalar function, as it assigns an scalar value to each point in the $\R^2$ plane. This function can be visualized using it graph which is 
\[ \text{Graph}(F) = \{ (x,y,z) \in \R^3: z = F(x,y) \}, \]
which is basically a surface in 3D. Consider the following plots which represent the graph of different scalar function.

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{Images/ScalarFunctions}
\end{figure}

As we can see in the figure above, graph of a scalar function is not always very informative, as certain portions of the function might not be visible due to the projection of the 3d plot. Another idea is to use level curves of the function to represent it. Level curves of a scalar function is defined as
\[ LC_c(F) = \{ (x,y)\in\R^2: F(x,y)=c,\ c\in \R \}. \]
Or as an alternative definition, a level curve of $F$ is a path $\gamma:\R\to\R^2$ that satisfies
\[ F(\gamma(t)) = c. \]
The following figure represents the level curves of the functions represented in the figure above.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{Images/ScalarFunctionsLevelCurves.pdf}
\end{figure}

\newpage
The time derivative of the function $F\circ\gamma$ is zero. The time derivative of $F\circ\gamma$ is the directional derivative of $F$ in the direction of $\gamma'(t)$ evaluated at $\gamma(t)$. Because
\[ \frac{d}{dt} F(\gamma(t)) =  \frac{\partial F}{\partial x}\big|_{\gamma(t)} \gamma'_1(t) + \frac{\partial F}{\partial y}\big|_{\gamma(t)} \gamma'_2(t) = \nabla F \big|_{\gamma(t)} \cdot \gamma'(t) = D_{\gamma'(t)} F \big|_{\gamma(t)}.\]

Thus a level curve of $F(x,y)$ passing through $p = (p_1, p_2)\in\R^2$, is a curve $\gamma(t) = (\gamma_1(t),\gamma_2(t))$ that is the solution of the following initial value problem
\[ F_x (\gamma(t)) \gamma'_1(t) + F_y(\gamma(t)) \gamma'_2(t) =0, \qquad \gamma(0) = p.\]
Let $x = x(t) = \gamma_1(t)$ and $y = y(t) = \gamma_2(t)$. Then the equation above can be written as
\[ F_x(x,y) x' + F_y(x,y) y' = 0, \qquad x(0)=p_1,\ y(0)=p_2.\]
This differential equation determines the level curve corresponding to $F(x,y)=F(p_1,p_2)$. We can write $y$ in terms of $x$ as 
\[ y = f(x) = (\gamma_2 \circ \gamma_1^{-1}) (x) \]
Thus the time derivative of $y$ can be written as $y' = \frac{d y}{dx} x'$. Assuming $x' \neq 0$ we can simplify the differential equation above as
\[ \frac{df}{dx} = \frac{- F_x(x,y)}{F_y(x,y)},\qquad f(p_0) = p_1. \]
Solving this initial value problem will determine the desired level curve. 

\begin{example}
	We want to find the level curve of $F(x,y) = x^2 + y^2$ which passes through $(1,1)\in\R^2$. To do this, we need to solve the following initial value problem
	\[ \frac{dy}{dx} = -\frac{2x}{2y}, \qquad y(1) = 1. \]
	By the method of separation of variables we will get
	\[ y^2 + x^2 = 1, \]
	which is a circle passing through the origin and radius 1. 
\end{example}

Note that we could do the whole business with following the concept of implicit differentiation (see a Calculus text book like Stewart).

\begin{example}
	In this example we are calculating the level-curves of the Lotka-Voltera model given as (nondimensionalized)
	\[ \dot{x} = x(1-y), \qquad \dot{y} = \delta y (x-1). \]
	To find the level curves, assume there exists a function $H:\R^2 \to \R$ such that 
	\[ \frac{d}{dt} H(X(t))=0, \]
	where $X(t) = (x(t),y(t))^T \in \R^2$ such that $\dot{X} = F(X)$, where $F:\R^2 \to \R^2$ is the RHS of the Lotka-Voltera system. In other words, $X(t)$ is a solution of the model that satisfies the differential equation. By chain rule we can write
	\[ H_x(x(t), y(t)) \dot{x}(t) + H_y(x(t),y(t)) \dot{y}(t) = 0. \tag{\smiley}  \]
	The equation $(\smiley)$ is the central equation and is very important. First, assume that $H_y(x(t),y(t)) \neq 0$ for all $t\in \R$. Then we can use the implicit function theorem and write $y = y(x)$. Thus $\dot{y}(t) = y' \dot{x}(t)$. And assuming $\dot{x}(t) \neq 0$ for all $t\in \R$ we can write $(\smiley)$ as 
	\[ y' = -\frac{ H_x(x(t), y(t))}{ H_y(x(t), y(t))}. \]
	On the other hand, we know that $\dot{x}(t) = f_1(x,y)$ and $\dot{y}(t) = f_2(x,y)$, where $f_1$ and $f_2$ are the component functions of the vector field $F$. Thus $(\smiley)$ we lead to 
	\[ -\frac{ H_x(x(t), y(t))}{ H_y(x(t), y(t))} = \frac{f_2(x,y)}{f_1(x,y)}.  \]
	Thus we can write
	\[ \frac{dy}{dx} = \frac{f_2(x,y)}{f_1(x,y)}. \]
	Using the specific vector field for the Lotka-Voltera model we can write
	\[ \frac{dy}{dx} = \frac{\delta y(x-1)}{x(1-y)}. \]
	Now we can solve this ODE by the method of the separation of variables.
	\[ \frac{1-y}{y} dy = \delta \frac{x-1}{x} dx. \]
	By integrating the two sides of the equation we will have
	\[ \ln(y) - y = \delta (x - \ln(x)) + c. \]
	Thus the level curves of the function 
	\[ H(x,y) = \ln(y) - y - \delta (x - \ln(x))  \]
	will yield the trajectories of the Lotka-voltera system in the $(x,y)$ plane. The following figure represents these curves for $\delta = 1.9$.
	\begin{center}
		\includegraphics[width=0.4\textwidth]{Images/LotkaVolterraLevelCurves.png}
	\end{center}

\end{example}


\subsection{Complexifying and Realifying A Vector Field}
Sometimes, it is very beneficial to express a particular vector field in complex variables, as it can possibly simplify the expression a lot. For instance, consider the following vector field.
\begin{align*}
	\dot{x}_1 = \mu x_1 - \omega x_2 + b(x_1^3 + x_1x_2^2),\\
	\dot{x}_2 = \omega x_1 + \mu x_2 + b(x_2^3 + x_1^2x_2).
\end{align*}
This vector field looks very messy in the rectangular coordinates. However, if we assume $x_1$ and $x_2$ are the real and imaginary parts of a complex variable $z$, i.e. $z = x_1 + ix_2$, then we can simplify the vector field a lot. After doing the transformation, we will have only one expression for the ODE system, as a complex variable is naturally 2D. To do the change of variable, we have several options. The very first elementary option is to follow our nose, and use the linear algebra constructs. Note that since $x_1$ is the real part of $z$ and $x_2$ is the imaginary part, then the following identities hold
\[ x_1 = \frac{z +\bar{z}}{2}, \quad x_2 = \frac{z - \bar{z} }{2i}. \]
Thus in vector notation we have
\[ \vectt{x_1}{x_2} = \frac{1}{2}\matt{1}{1}{-i}{i} \vectt{z}{\bar{z}}. \]
Now we can substitute the equations in the expression for the ODE system to find the expression in terms of the new variable $z$. The second option, is to be a little bit careful and try to compute $\dot{z} = \dot{x}_1 + i \dot{x}_2$, and try to group the terms in a neat way to produce $z = x_1 + i x_2$ terms to be substituted with $z$, while trying to make terms like $\mu + i\omega$ to replace it with $\lambda$. 

After following either of ways, we will have
\[ \dot{z} = \lambda z + b z \abs{z}^2, \]
where $\lambda = \mu + i\omega$. This is a huge step forward. The expression in terms of $z$ is much more neat and we can easily see what is going on with the dynamics. It is super easy to determine the contribution of the higher order terms in the dynamics, which was quite hard to accomplish when we were working with the rectangular coordinates.

Now, in order to get the expression back in the rectangular coordinates, we can substitute $z = x + iy$ in the expression for $\dot{z}$ and try to group the terms and separate the real and imaginary parts to determine the expressions for $x_1$ as well as for $x_2$. Equivalently, we can use the following coordinate change
\[  \vectt{z}{\bar{z}} = \matt{1}{i}{1}{-i} \vectt{x_1}{x_2}.  \]

\subsection{What Happens When Turning a Matrix to a Jordan Normal Form}
Linear algebra is amazing as it handles many messy processes and computations in a very neat and clean way. However, it is always a great practice to ask this question that ``What I would have done if I didn't know linear algebra?''. Here we will explain the answer to this question in turning a matrix to Jordan normal form. 

For any real matrix, the complex eigenvalues always comes in pairs. I.e. if a real matrix happen to have a complex eigenvalue $\lambda_1 = \mu+i\omega$, then there is certainly another eigenvalue $\lambda_2 = \mu-i\omega$. Also, the eigenvectors associated with any pair of complex conjugate eigenvalues are always complex conjugate themselves. I.e. assume that $q$ is the eigenvector associated with the eigenvalue $\lambda_1$, then $\bar{q}$ is the eigenvector associated with the eigenvector $\lambda_2$. The following shows the process of turning a $2\times2$ matrix into its Jordan normal form to observe that what is really happening under the hood.

Consider the following system of ODE
\[ \dot{u} = Au, \qquad A = \matt{a_{11}}{a_{12}}{a_{21}}{a_{22}}. \]
where $A$ has two complex eigenvalues $\lambda_1 = \mu + i\omega$ and $\lambda_2 = \mu - i\omega$, and associated with them two complex conjugate eigenvectors $q = \vectt{q_1}{q_2} = \vectt{x_1}{x_2} + i\vectt{y_1}{y_2}$ and $\bar{q} = \vectt{\bar{q_1}}{\bar{q_2}}$, where $q_1, q_2 \in \C$. Now there are two distinct paths that can take us to the Jordan normal form of the matrix. One is to turn the matrix into it Real normal form first and then turn it into the Jordan normal form, and then other one is to directly transform the matrix to its Jordan normal form. For the first path, in order to transfer the matrix $A$ to its real normal form, we do the following coordinate change
\[  u = \matt{\Re{q_1}}{-\Im{q_1}}{\Re{q_2}}{-\Im{q_2}} v = \matt{x_1}{-y_1}{x_2}{-y_2} v = P_1 v.  \]
where we have assumed that $\lambda_1$ is the eigenvalue with positive imaginary part. Then the ODE system will be
\[ \dot{v} = \inv{P_1}AP_1 v = R v. \]
After this transformation, the matrix $R$ will be like below
\[ R = \matt{\mu}{-\omega}{\omega}{\mu}. \]
Now turning this matrix to the Jordan normal form (a diagonal matrix with the complex eigenvalues sitting in the main diagonal) has a clear explanation that we described in the previous section as Complexifying a vector field. By evaluating the matrix $R$ it turns out that it has two eigenvectors as $r_1 = \frac{1}{2}\vectt{1}{-i}$ and $r_2 = \frac{1}{2}\vectt{1}{i}$. Thus we need to do the following change of variable to turn the matrix into its normal form
\[ v = \frac{1}{2}\matt{1}{1}{-i}{i} \xi = P_2 \xi. \]
In the components we will have
\[ 
v_1 = \frac{1}{2}(\xi_1 + \xi_2),\qquad v_2 = \frac{1}{2i}(\xi_1 - \xi_2).
\]
And then the ODE system will be
\[ \dot{\xi} = \inv{P_2}RP_2\ \xi = \inv{P_2}\inv{P_1}A P_1 P_2\ \xi =  J \xi,   \]
where the matrix $J$ will be
\[ J = \matt{\mu+i\omega}{0}{0}{\mu-i\omega}. \]
Thus it turns out that in fact $\xi_1$ is the complex conjugate of $\xi_2$, and this step in changing the real normal form to Jordan normal form is the same as simply letting
\[ \xi = v_1 + i v_2. \]
I.e. letting the components of the vector $v$ be the real and imaginary part of the complex variable $\xi$.

However, it is also possible to get here directly from $A$, by simply doing the following coordinate change
\[ u = P\hat{z} = \matt{q_1}{\bar{q_1}}{q_2}{\bar{q_2}} \vectt{z_1}{z_2}, \]
and we can predict that $z_2$ is in fact the complex conjugate of $z_1$. By letting $z_1 = \bar{z_1}$ we can write
\[ u = \matt{q_1}{\bar{q_1}}{q_2}{\bar{q_2}} \vectt{z_1}{\bar{z_1}}  \]
which is simply listing the eigenvectors in the column of a vector when we do the general diagonalization. In this direct transformation we in fact set
\[ u_1 = q_1z_1 + \conj{q_1 z_1},\qquad u_2 = q_2z_1 + \conj{q_2z_1}, \]
or in a vector notation (let $z = z_1 = z_2$) we will have
\[ u = qz + \conj{qz}. \]
Now, the only question that remains is the is there a way to express $z_1$ neatly in terms of $u_1$ and $u_2$, as we did in transforming the real normal form to the Jordan normal form? The answer is yes, but it required a little bit more careful treatment. We know that $q$ is the eigenvector of the matrix $A$ with eigenvalue $\lambda$. We define the eigenvector of the adjoint matrix $A^*$ with eigenvalue $\conj{\lambda}$ to be $p$. I.e. 
\[ A^* p = \conj{\lambda} p. \]
Then $\dotprod{p}{\bar{q}} = 0$. That is because 
\[ \lambda \dotprod{p}{\bar{q}} = \dotprod{\conj{\lambda}p}{\bar{q}} = \dotprod{A^*p}{\bar{q}} = \dotprod{p}{A\bar{q}} = \dotprod{p}{\conj{\lambda q}} = \conj{\lambda}\dotprod{p}{\bar{q}},  \]
and since $\lambda - \conj{\lambda} = 2i\omega \neq 0$, then $\dotprod{p}{\bar{q}} = 0$. Also we can show (I don't know how yet) that
\[ \dotprod{p}{q} \neq 0,\]
and we can normalize $p$ in a way that $\dotprod{p}{q} = 1$. With this setting, and observing that $u = zq + \conj{zq}$ it is very easy to observe that
\[ z = \dotprod{p}{u}. \]
It is now clear that in the special case where we were transforming the real normal form to Jordan normal form the vector $p$ was actually
\[ p = \vectt{1}{-i}. \]





