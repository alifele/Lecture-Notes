\chapter{Random Notes}

\section{Interesting Observations from Roman}

\begin{observation}[Geometric Interpretation of Dual Vectors]
	The notion of the dual space of a vector space is somewhat abstract and one usually struggles to have a geometric realization of the functionals and dual spaces. Here, I provide a very interesting point of view. Let $ V $  be a finite dimensional vectors space. Then every $ f \in V^* $ is characterized by a hyperplane $ H $ such that $ H = \operatorname{ker} f $. 
	
	\noindent With this point of view, $ f(x) = 0 $ corresponds to the fact that $ x \in  H $. Also, it is very straight forward to see the following properties of functionals with this geometric point of view.
	\begin{proposition}
		\begin{enumerate}[(a)]
			\item If $ f(x) \neq 0 $ then 
			\[ V = \langle x \rangle \oplus \ker f. \]
			\item For every $ x\in V $ there exists $ f \in V^* $ such that $ f(x) \neq 0 $.
			\item For $ x\in V $, $ f(x) = 0 $ for all $ f\in V^* $ implies $ x = 0 $.
			\item 
		\end{enumerate}
	\end{proposition}
	\begin{proof}(Geometric interpretation)
		\begin{enumerate}[(a)]
			\item If $ x\notin H $ for some hyperplane $ H $, then 
			\[ V = \langle x \rangle \oplus H. \]
			\item Given any point of the space, there is some hyperplane that misses that particular point.s
			\item The only point that belongs to all hyperplanes is the origin.
		\end{enumerate}
	\end{proof}
\end{observation}



\begin{observation}[More Geometric Interpretation of Dual Vectors]
	The characterization above, i.e. identifying the linear functionals with their kernel, i.e. hyperplanes, work surprisingly well in characterizing very interesting facts. For instance, we can have the following definition of the annihilators of a set.
	\begin{definition}[Annihilators]
		Let $ M \subset V $ (no necessarily a linear subspace). Then the annihilators of $ M $, denoted by $ M^0 $ is the set of all linear functionals that kills $ M $. I.e.
		\[ M^0 = \set{f \in V^*| f(M) = 0}. \]
		With the geometric point of view above, the annihilators of $ M $ is the set of all hyperplanes that contain $ M $.
	\end{definition}
	For instance, let $ L $ be a one dimensional linear subspace of $ \R^3 $. Then $ L^0 $ will be the set of all hyperplanes containing $ L $. Each such hyperplane can be represented by a normal vectors. So the set of all hyperplanes containing $ L $ is isomorphic to a plane perpendicular to $ L $ and going through the origin (more generally, any 2-dimensional linear subspace of $ \R^3 $ that does not contain $ L $). It is now very straightforward to see the result of Theorem 3.14 part (2). The set $ M^{00} $ is the set of all hyperplanes containing $ M^0 $. There is just one such hyperplane, and since it can be parameterized using one normal vector (along $ L $), we have 
	\[ M^{00} \simeq \operatorname{span}L. \]
\end{observation}


\begin{observation}[Double Dual Map]
	We start with the following definition.
	\begin{definition}
		Let $ \tau \in \mathcal{L}(U,V) $. The dual map $ \tau^\times \in \mathcal{L}(V^*,U^*) $ and, the double dual map $ \tau^{\times\times} = \mathcal{L}(U^{**},V^{**}) $ is defined as
		\[  (\tau^\times f)(u) = f(\tau u), \qquad \text{for $ u\in U,\ f\in V^* $}, \]
		and 
		\[  (\tau^{\times\times} E)(f) = E (\tau^\times f), \qquad \text{for $ E\in V^{**},\ f\in W^* $}.  \]
		In finite dimension, the following is a very useful characterization of $ \tau^{\times\times} $. Let $ u\in U $ and using the canonical map $ u \mapsto E_u \in V^{**} $, where $ E_u $ is the evaluation map at $ u $.Also let $ f\in V^* $. Then we can write
		\begin{align*}
			(\tau^{\times\times} E_u)(f) &= E_u(\tau^\times f) \\
			&= (\tau^\times f)(u) \\
			&=f(\tau u) \\
			&=E_{\tau u} (f).
		\end{align*} 
		Thus we have
		\[ \tau^{\times\times}E_u = E_{\tau u}. \]
	\end{definition}
\end{observation}

\begin{observation}[Geometric Interpretation of Dual Map]
	For $ \tau \in \mathcal{L}(V,W) $, the dual map $ \tau^\times \in \mathcal{L}(W^*,V^*) $ is given by
	\[ (\tau^\times f)(v) = f(\tau v),  \]
	where $ f\in W^* $ and $ v \in V $. Using out geometric point of view of the functionals (as hyperplanes) we can have a geometric interpretation of what is the dual of a map. The following is a high level summary:
	\begin{quote}
		Let $ f\in W^* $ be a functional, i.e. a hyperplane. Then $ \tau^\times $ returns a hyperplane in $ V $ that is the pre-image of restriction of $ f $ to $ \im(\tau) $.
	\end{quote}
	For instance, if $ \tau : \R^2 \to \R^3 $ the inclusion map that sends $ \R^2 $ to the $ xy $ plane in $ \R^3 $, the $ \tau^\times $ map maps the following red hyperplane (as a functional in $ \R^3 $) to the green hyperplane (as a functional in $ \R^2 $).
	\begin{center}
			\includegraphics[width=0.4\linewidth]{Images/DualMap}
	\end{center}
	
	Using the interpretation above we can have the following ``geometric'' proof of the following facts in Roman (presented in Theorem 3.19).
	\begin{proposition}
		Let $ \tau \in \mathcal{L}(V,W) $. Then
		\begin{enumerate}[(a)]
			\item $ \ker(\tau^\times) = \im(\tau)^0 $.
			\item $ \im(\tau^\times) = \ker(\tau)^0 $.
		\end{enumerate}
		\begin{proof}[Geometric proof]
			\begin{enumerate}[(a)]
				\item We want to show $ \ker(\tau^\times ) \subset \im(\tau)^0 $. Let $ f\in \ker(\tau^\times) $ be a hyperplane (i.e. functional). This means that if we restrict $ f $ to $ \im(\tau) $ and then consider its pre-image, it should be the whole space (i.e. the zero functional). Thus $ f $ should contain $ \im(\tau) $. So $ f\in \im(\tau)^0 $ (remember that $ \im(\tau)^0 $) is the set of all hyperplanes containing $ \im(\tau) $. For the converse, we want to show $ \im(\tau)^0 \subset \ker(\tau^\times) $. Let $ f \in \im(\tau)^0 $. I.e. $ f $ is a hyperplane that contains $ \im(\tau) $. So restricting $ f $ to $ \im(\tau) $ will be whole $ \im(\tau) $. So the pre-image of the restriction of $ f $ to $ \im(\tau) $ will be the whole space $ V $ (thus the zero functional). So $ f\in \ker(\tau^\times) $. \emph{Note: We have used the fact that for any linear map $ \tau $ we have $ \im(\tau) \simeq \dom(\tau) $}.
				
				\item 
				
			\end{enumerate}
		\end{proof}
	\end{proposition}
\end{observation}



\begin{observation}[Coordinate maps]
	Let $ (V,F) $ be a vector space (defined on the field $ F $) with finite dimension $ n $. Once we choose an ordered basis for $ V $, like $ \mathcal{B} = (v_1,\cdots,v_n) $, we can define the coordinate map
	\[ \phi_\mathcal{B}: V \to F^n, \]
	that
	\[ v = \sum_i \alpha_i v_n \mapsto \begin{bmatrix}
		\alpha_1 \\
		\vdots \\
		\alpha_n
	\end{bmatrix}. \]
	In particular, for the basis vectors we have $ \phi(v_i) = e_i $, where $ e_i $ is a column vector whose entries are all zero, but the $ i^\text{th} $ row. This coordinate map $ \phi $ justifies the name ``vector space'' for this algebraic structure. The elements of any finite dimensional vector space defined on $ F $ can be ``coordinated'' by the elements of $ F^n $.
\end{observation}

\begin{observation}
	As a continuiation of the note above, lets now focus on the linear maps $ \mathcal{L}(F^n,F^m) $. We know that every matrix in $ A \in \mathcal{M}_{n,m} $ induces a linear map $ \tau_A \in \mathcal{L}(F^n,F^m) $, given by
	\[ \tau_A(v) = A v. \]
	The converse is also true. Every linear map $ \tau \in \mathcal{L}(F^n,F^m) $ has a matrix representation $ A \in \mathcal{M}_{n,m} $ given by
	\[ A = (\tau e_1 | \cdots | \tau e_n), \]
	i.e. apply $ \tau $ on the basis vectors, write the coordinates of the resulting vector in the columns of a matrix to get the matrix representation of the linear transformation.
\end{observation}


\begin{observation}
	I have started to notice a very interesting interaction between the following objects, and each pair of these notions induces a similar feeling. I have not yet been able to quantify this feeling. But I am sure there is some connection there. 
	\begin{center}
		\begin{tabular}{|c|c|c|c|}

			surjective & ker & spanning & exists \\
			\hline
			injective & img & linearly independent & for all \\

		\end{tabular}
	\end{center}
\end{observation}



\begin{observation}[Change of basis]
	Consider the following diagram.
	% https://q.uiver.app/#q=WzAsNixbMSwxLCJWIl0sWzEsMywiVyJdLFsyLDAsIkZebiJdLFswLDAsIkZebiJdLFsyLDQsIkZebSJdLFswLDQsIkZebSJdLFswLDIsIlxccGhpX1xcbWF0aGNhbHtCfSIsMl0sWzAsMywiXFxwaGlfXFxtYXRoY2Fse1xcdGlsZGUgQn0iXSxbMSw0LCJcXHBoaV97XFxtYXRoY2Fse0N9fSJdLFsxLDUsIlxccGhpX1xcbWF0aGNhbHtcXHRpbGRlIEN9IiwyXSxbMCwxLCJcXHRhdSIsMV0sWzIsNCwiW1xcdGF1XV97XFxtYXRoY2Fse0J9XFxtYXRoY2Fse0N9fSIsMCx7ImN1cnZlIjotMn1dLFszLDUsIltcXHRhdV1fe1xcbWF0aGNhbHtcXHRpbGRlIEJ9XFxtYXRoY2Fse1xcdGlsZGUgQ319IiwyLHsiY3VydmUiOjJ9XSxbMiwzLCJcXHBoaV97XFxtYXRoY2Fse0J9XFxtYXRoY2Fse1xcdGlsZGUgQn19IiwyXSxbNCw1LCJcXHBoaV97XFxtYXRoY2Fse0N9XFxtYXRoY2Fse1xcdGlsZGUgQ319Il1d
	\[\begin{tikzcd}
		{F^n} && {F^n} \\
		& V \\
		\\
		& W \\
		{F^m} && {F^m}
		\arrow["{[\tau]_{\mathcal{\tilde B}\mathcal{\tilde C}}}"', curve={height=12pt}, from=1-1, to=5-1]
		\arrow["{M_{\mathcal{B}\mathcal{\tilde B}} = \phi_{\mathcal{B}\mathcal{\tilde B}}}"', from=1-3, to=1-1]
		\arrow["{[\tau]_{\mathcal{B}\mathcal{C}}}", curve={height=-12pt}, from=1-3, to=5-3]
		\arrow["{\phi_\mathcal{\tilde B}}", from=2-2, to=1-1]
		\arrow["{\phi_\mathcal{B}}"', from=2-2, to=1-3]
		\arrow["\tau"{description}, from=2-2, to=4-2]
		\arrow["{\phi_\mathcal{\tilde C}}"', from=4-2, to=5-1]
		\arrow["{\phi_{\mathcal{C}}}", from=4-2, to=5-3]
		\arrow["{M_{\mathcal{C}\mathcal{\tilde C}} = \phi_{\mathcal{C}\mathcal{\tilde C}}}", from=5-3, to=5-1]
	\end{tikzcd}\]
	
	where $ V,W $ are vector spaces with dimension $ n $ and $ m $ respectively. Furthermore, $ \mathcal{B} $ and $ \mathcal{\tilde B} $ are two ordered basis for $ V $ with the coordinate maps $ \phi_\mathcal{B} $ and $ \phi_\mathcal{\tilde B} $ respectively. Similarly $ \mathcal{C} $ and $ \mathcal{\tilde C} $ are two ordered basis for $ W $ with $ \phi_\mathcal{C} $ and $ \phi_\mathcal{\tilde C} $ as the corresponding coordinate maps. The change of basis matrices are given in Theorem 2.12 Roman. This diagram summarizes the relation between the vectors in the abstract vector spaces $ V $ and $ W $ with their representation with we change the basis in either of the spaces. Also it capture the transformation that happens for the representation of linear maps when we change basis. For instance it is very easy to see
	\[ [\tau]_{\mathcal{\tilde B}\mathcal{\tilde C}} = M_{\mathcal{C}\mathcal{\tilde C}} [\tau]_{BC} M_{\mathcal{B}\mathcal{\tilde B}}^{-1}. \]
	It is also easier to memorize the following relation instead
	\[  [\tau]_{\mathcal{\tilde B} \mathcal{\tilde C}} M_{\mathcal{B}\mathcal{\tilde B}} =   M_{\mathcal{C}\mathcal{\tilde C}} [\tau]_{\mathcal{B}\mathcal{C}}. \]
\end{observation}


\begin{observation}[Characteristic of a filed and the alternating, v.s. skew-symmetric forms]
	It is only when $ \Char(F) \neq 2 $ we have the equivalence between the alternating forms and the skew-symmetric forms
	\[ \text{alternating} \qquad \Longleftrightarrow \qquad \text{skew-symmetric}. \]
	For the forward direction we don't need any restriction on the characteristic of the field. To see this let $ f $ be an alternating bi-linear form.
	\[ f(u+v,u+v) = f(u,u) + f(v,v) + f(u,v) + f(v,u). \]
	Since $ f $ is alternating, we have $ f(u+v,u+v) = 0 $ as well as $ f(u,u) = f(v,v) = 0 $. So we can conclude that 
	\[ f(u,v) = - f(v,u). \]
	However, for the converse, we need $ \Char(F) \ neq 2 $. Because if $ f $ is skew-symmetric, then $ f(u,u) = - f(u,u) $, which implies $ 2f(u,u) = 0  $. We can only conclude $ f(u,u) = 0 $ when $ \Char(F) \neq 2 $, i.e. when $ 2 $ is invertible in the field. 
\end{observation}


\begin{observation}[Symmetric and Antisymmetric tensor products with Roman's notation]
	In Roman text book, he introduces the notion of the symmetric and anti-symmetric tensor products with a notation that is not easy to understand, unless there is a running example. Here in this box, I will give an explicit example. Let $ V $ has dimension $ n=3 $ and let $ \set{e_1,e_2,e_3} $ be a basis for $ V $. We want to explicitly construct the basis vectors for the $ \ST^p(V) $ and $ \AT^p(V) $. We will have the following cases
	\begin{enumerate}[(i)]
		\item $ p = 2 $. Then the basis elements of $ \ST^2(V) $ will be
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,1} $ & $ e_1\otimes e_1 $ & $ e_1\vee e_1 $ \\
				\hline 
				$ \set{2,2} $ & $ e_2\otimes e_2 $ & $ e_2\vee e_2 $ \\
				\hline 
				$ \set{3,3} $ & $ e_3\otimes e_3 $ & $ e_3\vee e_3 $ \\
				\hline 
				$ \set{1,2} $ & $ e_1\otimes e_2 + e_2\otimes e_1 $ & $ e_1\vee e_2 $ \\
				\hline 
				$ \set{1,3} $ & $ e_1\otimes e_3 + e_3\otimes e_1 $ & $ e_1\vee e_3 $ \\
				\hline 
				$ \set{2,3} $ & $ e_1\otimes e_2 + e_2\otimes e_1 $ & $ e_1\vee e_2 $ \\
				\hline 
			\end{tabular}
		\end{center}
		
		And the basis elements of $ \AT^2(V) $ will be
		
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,2} $ & $ e_1\otimes e_2 - e_2\otimes e_1 $ & $ e_1\wedge e_2 $ \\
				\hline
				$ \set{1,3} $ & $ e_1\otimes e_3 - e_3\otimes e_1 $ & $ e_1\wedge e_3 $ \\
				\hline
				$ \set{2,3} $ & $ e_2\otimes e_3 - e_3\otimes e_2 $ & $ e_2\wedge e_3 $ \\
				\hline
			\end{tabular}
		\end{center}
		
		
		\item $ p=3 $. Then the basis elements of $ \ST^3(V) $ will be
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,1,1} $ & $ e_1\otimes e_1\otimes e_1 $ & $ e_1\vee e_1 \vee e_1 $ \\
				\hline
				$ \set{2,2,2} $ & $ e_2\otimes e_2\otimes e_2 $ & $ e_2\vee e_2 \vee e_2 $ \\
				\hline
				$ \set{3,3,3} $ & $ e_3\otimes e_3\otimes e_3 $ & $ e_3\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{1,2,2} $ & $ e_1\otimes e_2\otimes e_2 + e_2\otimes e_1\otimes e_2 + e_2\otimes e_2\otimes e_1 $ & $ e_1\vee e_2 \vee e_2 $ \\
				\hline
				$ \set{1,3,3} $ & $ e_1\otimes e_3\otimes e_3 + e_3\otimes e_1\otimes e_3 + e_3\otimes e_3\otimes e_1 $ & $ e_1\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{2,1,1} $ & $ e_2\otimes e_1\otimes e_1 + e_1\otimes e_2\otimes e_1 + e_2\otimes e_2\otimes e_1 $ & $ e_1\vee e_1 \vee e_2 $ \\
				\hline
				$ \set{2,3,3} $ & $ e_2\otimes e_3\otimes e_3 + e_3\otimes e_2\otimes e_3 + e_3\otimes e_3\otimes e_2 $ & $ e_2\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{3,1,1} $ & $ e_3\otimes e_1\otimes e_1 + e_1\otimes e_3\otimes e_1 + e_1\otimes e_1\otimes e_3 $ & $ e_1\vee e_1 \vee e_3 $ \\
				\hline
				$ \set{3,2,2} $ & $ e_3\otimes e_2\otimes e_2 + e_2\otimes e_3\otimes e_2 + e_2\otimes e_2\otimes e_3 $ & $ e_2\vee e_2 \vee e_3 $ \\
				\hline
				$ \set{1,2,3} $ & \makecell{$ e_1\otimes e_2\otimes e_3 + e_1\otimes e_3\otimes e_2 + e_2\otimes e_1\otimes e_3 $ \\ $ + e_2\otimes e_3\otimes e_1 + e_3\otimes e_1\otimes e_2 + e_3\otimes e_2\otimes e_1 $} & $ e_1\vee e_2 \vee e_3 $ \\
				\hline
			\end{tabular}
		\end{center}
		
		And for $ \AT^3(V) $ we have
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,2,3} $ & \makecell{$ e_1\otimes e_2\otimes e_3 - e_1\otimes e_3\otimes e_2 + e_2\otimes e_1\otimes e_3 $ \\ $ - e_2\otimes e_3\otimes e_1 + e_3\otimes e_1\otimes e_2 - e_3\otimes e_2\otimes e_1 $} & $ e_1\wedge  e_2 \wedge e_3 $ \\
				\hline
				
			\end{tabular}
		\end{center}
		
		This we have following two theorems.
		
		\begin{theorem}
			Let $ V $ be a finite dimensional vector space, and let $ \mathcal{B} = \set{e_1,\cdots,e_n} $ be a basis. Then 
			\[ \ST^p(V) \simeq F_p[e_1,\cdots,e_n], \]
			and
			\[ \AT^p(V) \simeq F_p^-[e_1,\cdots,e_n]. \]
			In words,
			\begin{quote}
				The symmetric tensor space $ \ST^p(V) $ is isomorphic to the algebra $ F_p[e_1,\cdots,e_n] $ of homogeneous polynomials of degree $ p $. 
			\end{quote}
			And similarly
			\begin{quote}
				The anti-symmetric tensor space $ \AT^p(V) $ is isomorphic to the algebra $ F_p^-[e_1,\cdots,e_n] $ of anti-commutative homogeneous polynomials of degree $ p $.
			\end{quote}
		\end{theorem}
		
		It is easy to see (Roman Theorem 14.18) that 
		\[ \dim(\AT^p(V)) = \binom{n}{p}, \qquad \dim(\ST^p(V))=\binom{n+p-1}{p}. \]
		The formula for the dimension of $ \ST^p(V) $ resembles the formula for all possible distribution of $ p $ units on energy in $ n $ containers (see Schroeder, Equation 2.9).
		
		
		
	\end{enumerate}
\end{observation}


\begin{observation}[Suitable basis for nilpotent maps]
	This note is meant to accompany the sections 8.4 to 8.8 of Leohen's advanced linear algebra. I demonstrate a concrete example to show how we can find a suitable basis for a nilpotent map such that its matrix representation has Jordan blocks. Consider the following nilpotent matrix.
	\[ A = 
	\begin{pmatrix}
		-16 & -20 & 34 \\
		2 & 52 & -29 \\
		0 & 72 & -36
	\end{pmatrix}. \]
	It is easy tot check that $ A^3 = 0 $. We want to find a suitable basis in which the matrix above has Jordan blocks in its structure. To do so, we first consider image $ A $. By writing the matrix in reduced row echelon form it is easy to see that the column space of $ A $ is
	\[ W =  \im(A) = \Span\set{\vecttt{-16}{2}{0}, \vecttt{-20}{52}{72}}. \]
	The subspace $ W $ is $ A $-invariant. So we can restrict $ A $ to this subspace. Fix the vectors above as the basis of $ W $. In this basis, let the matrix representation of $ A|_W $ is $ [A|_W] $. Then this needs to satisfy
	\[ AM = M[A|_W], \]
	where
	\[ M = \begin{pmatrix}
		-16 & 20 \\
		2 & 52 \\
		0 & 72
	\end{pmatrix}. \]
	One possible solution for $ B $ will be
	\[ [A|_W] = \begin{pmatrix}
		-16 & 128 \\
		2 & 16
	\end{pmatrix}. \]
	Again, we need to consider the image of this matrix. Writing the matrix $ A|_W $ in reduced row echelon form we can see that 
	\[ W' = \im(A|_W) = \Span\set{\vectt{-16}{2}}. \]
	Now we need to restrict $ A|_W $ to $ W' $ above. However since $ W' $ is one dimensional and $ A|_W $ in nilpotent (since $ A $ is nilpotent), $ (A|_W)|_{W'} $ will be the zero map. So we can choose 
	\[ e_1 = \vectt{-16}{2}, \]
	as the first vector in the desired basis. 
	Since $ \ker(A|_W) $ is the same as $ \im(A|_W) $, then $ \im(A|_W) + \ker(A|_W) = \im(A|_W) $ and the basis can not be extended further (stage to of finding suitable basis for the nilpotent map $ A|_W $). We need to perform stage $ 3 $ and find a vector in $ W $ that maps to $ W' $. One possible choice is 
	\[ e_2 = \vectt{1}{0}. \]
	Note that in both of the representations above we are assuming that the basis vectors for the space $ W $ is as fixed above. Lastly, we need to perform stage 2 for the map $ A $, and since $ \ker(A) \subset \im(A), $
	this has no interesting result. Finally, for stage 3 for $ A $, we need to find some $ e_3 \in V $
	such that $ Ae_3 = e_2 $. By using the pseudo inverse for $ A $ we can find 
	\[ e_3 = \frac{1}{14}\vecttt{5}{-3}{-6}. \]
	So the suitable basis is
	\[ e_1 = -16\vecttt{-6}{2}{0} + 2\vecttt{20}{52}{72},\quad  e_2 = \vecttt{-16}{2}{0},\quad  e_3 = \frac{1}{14}\vecttt{5}{-3}{-6}.  \]
	It is easy to see
	\[ Ae_1 = 0, \quad A^2e_2 =0, \quad A^3e_3 = 0. \]
	You can experiment with the code written in python to perform the computations above, where the code is included in the jupyter notebook file in the Codes folder of this latex project. 
	
	\begin{minted}{python}
		from sympy import Matrix
		import sympy as sp
		
		# Define the matrix
		
		# Perform row reduction (RREF - Reduced Row Echelon Form)
		# rref_matrix, pivot_columns = A_sym.rref()
		
		
		def imageBasis(matrix):
		# List of basis vectors
		colSpace = matrix.columnspace()  
		# Stack the first two vectors into a matrix
		return Matrix.hstack(*colSpace[:])  
		
		
		
		def kernelBasis(matrix):
		"""Computes a basis for the null space (kernel) of a matrix."""
		nullSpace = matrix.nullspace()
		# Returns a list of basis vectors for the null space
		return Matrix.hstack(*nullSpace[:])  
		
		
		def restrictTo(A,M):
		"""
		Note that colSpace(B) should be image of A (thus A invariant)
		"""
		
		return (M.T * M).pinv() * (M.T * A * M)
		
		
	\end{minted}
	
\end{observation}



\begin{observation}[Step by step walk through for calculating the generalized eigenspaces]
	Consider the following matrix
	\[
	A  = 
	\begin{bmatrix*}[r]
		7 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 7 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -4 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i & 1 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i & 1 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i
	\end{bmatrix*}.
	\]
	We want to construct the generalized eigenspaces for this matrix. First, observe that the spectrum of this matrix is $ \Spect_A = \set{7,-4,i} $. This can be calculated by
	\begin{minted}{python}
		A.eigenvals()
	\end{minted}
	From the form of the matrix, it is immediate that for $ \lambda = 7 $, there are $ 3 $ associated generalized eigenspaces. To find these three, we first calculate the eigenvalues associated to $ \lambda = 7 $. We will have
	\[ \ker(A - 7 I) = \Span\set{
	\begin{bmatrix}
		1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}
	} \]
	These computations can be done by
	\begin{minted}{python}
		I = sp.eye(J.shape[0])
		v1,v2,v3 = ((A - (7)*I)).nullspace()
	\end{minted}
	Now for each of the vectors above, we calculate their Jordan chain. I.e. we solve 
	\[ (A - \lambda I)w = v, \]
	where we start by $ v $ as one of the vector above to get $ w $ (second vector in the chain), and again, we replace $ v $  with $ w $ calculated above and solve the equation again to find a new $ w $, which will be the third element in the chain, and so on. This process will stop eventually (the solution of the equation will be the zero vector). Starting with the each of the vectors above, we will get
	\[ V_1^1 = \Span\set{
		\begin{bmatrix}
			1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}, \
		\begin{bmatrix}
			0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}
		}, \qquad
		V_1^2 = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}, \
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}
		}, \qquad
		V_1^3 = \Span\set{
			\begin{bmatrix}
				0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
			\end{bmatrix}
		}.
	\]
	Now for $ \lambda = -4 $ do a similar process. Observe that for this eigenvalue we have only one Jordan block matrix. First, we calculate the eigenvectors, and starting from each of them we compute the generalized eigenvectors. It turns out that there is just one eigenvalue for $ \lambda = -4 $
	\[ \ker(A-(-4)I) = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}
	}. \]
	Similar to above, we can calculate the Jordan chain for starting with this vector. We will see
	\[ 
	V_2^1 = \Span\set{
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}
	}.
	\]
	Finally, for $ \lambda = i $, it turns out (as also evident from the matrix), there are two eigenvalues (translates to two Jordan blocks) as
	\[ \ker(A - iI) = \Span\set{
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1
	\end{bmatrix}
	}. \]
	And with the calculations similar to above we will get
	\[ V_{3}^1 = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
		\end{bmatrix}, \
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0
		\end{bmatrix}
	}, \qquad
	V_{3}^2 = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1
		\end{bmatrix}
	}.
	 \]
	 
	The calculation above can be found under Example 3 in the JordanForm.ipynb file inside the Codes directory.
\end{observation}


\begin{observation}[Step by step calculation of Jordan canonical form]
	See StepByStepJordanCanonicalForm.ipynb in the Codes directory of this latex project for a complete example of calculating the Jordan canonical form.
\end{observation}

	
	
	



\section{Ongoing thoughts}

\begin{observation}[Ongoing thought on the relation of the space of linear operators and the tensor product]
	In many instances, I have noticed a similar structure between $ \mathcal{L}(V,W) $ and $ V \otimes W $. For instance, we know that while $ u\otimes v $ is a tensor (a pure tensor), but not every tensor can be written like this, but rather it is a linear combination of pure tensors. This is very similar to the idea that for $ A: V\to W $ and $ B: U\to W $, we can construct a linear map $ C: U\oplus V \to W $, that has a block diagonal representation. But, we can not write every matrix in a block diagonal representation. 
	
	Also, another hint is that $ \dim(\mathcal{L}(V,W)) = n\times m $, and similarly, $ \dim(U\otimes V) = n\times m $. Yet another hint is that every elements of $ U\otimes V $ has a matrix coordinate. I need to make this connection more clear and easy to see / understand.
\end{observation}



\section{Questions}
\begin{problem}
	\begin{enumerate}[(a)]
		\item Let $ Z $ be any vector space, and suppose we have for each $ i $ a linear map $ g_i: Z\to V_i $. Show that there is a unique $ g: Z\to \prod_i V_i $ such that $ \pi_i \circ g = g_i $ for all $ i $.
	\end{enumerate}
\end{problem}

\begin{solution}
	A quick reminder that $ \prod_{i\in I} V_i $ is 
	\[ \prod_{i\in I} V_i = \set{f: I \to \bigcup_i V_i\ :\ f(i) \in V_i}. \]
	This a generalization of $ n $-tuple. C
	% https://q.uiver.app/#q=WzAsMyxbMSwwLCJaIl0sWzEsMSwiXFxwcm9kX3tpXFxpbiBJfSBWX2kiXSxbMCwxLCIoVl9pKV97aVxcaW4gSX0iXSxbMCwxLCJnIl0sWzAsMiwiKGdfaSlfe2lcXGluIEl9IiwyXSxbMSwyLCJcXHBpX2kiXV0=
	\[\begin{tikzcd}
		& Z \\
		{(V_i)_{i\in I}} & {\prod_{i\in I} V_i}
		\arrow["{(g_i)_{i\in I}}"', from=1-2, to=2-1]
		\arrow["g", from=1-2, to=2-2]
		\arrow["{\pi_i}", from=2-2, to=2-1]
	\end{tikzcd}\]
	Defien $ g: Z\to \prod_i V_i $ by
	\[ \pi_i\circ g = g_i. \]
	Note that $ g $ is uniquely determined by this definition. Indeed, for $ v\in Z $ we have
	\[ (g(v))(i) = \pi_i (g(v)) = (\pi_i\circ g)(v) = g_i(v). \]
	We need to show that $ g $ is linear. Let $ v,u\in Z$ and $ \alpha,\beta $ scalars. Then
	\[ (g(\alpha v + \beta u))(i)  = (\pi_i\circ g)(\alpha v+\beta u) = g_i(\alpha v + \beta u) = \alpha g_i(v) + \beta g_i(u) = \alpha (g(u))(i) + \beta (g(v))(i).  \]
	Since this is true for the $ g $ component-wise, then it follows that
	\[ g(\alpha v + \beta u) = \alpha g(v) + \beta g(u). \]
\end{solution}



\begin{problem}
	Let $ U,V,W $ be vector spaces, and let $ T: U\to V $ and $ S: V\to W $. 
	\begin{enumerate}[(a)]
		\item Suppose $ U,V $ are finite dimensional with bases $ \set{u_j}_{j=1}^m \subset U $ and $ \set{v_i}_{i=1}^n \subset V $, and let $ A \in M_{n,m}(F) $ be the matrix of $ T $ in those bases. Show that the matrix of the dual map $ T': V'\to U' $ with respect to the dual bases $ \set{u^j}, \set{v^j} $ is the transpose of $ A $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ \tilde A $ denote the matrix representation of $ T' $. Then $ (\tilde A)_{i,j} $, is the $ i^\text{th} $ component (in the dual basis) of $ T' v^j $. Note that $ T'v^j $ is a functional on $ U $ and to get its $ i^\text{th} $ component we need to apply it on $ u_i  $. So
		\[ (\tilde A)_{ij} = (T'(v^j))(u_i) = v^j(Tu_i) = (A)_{ji}. \]
	\end{enumerate}
\end{solution}


