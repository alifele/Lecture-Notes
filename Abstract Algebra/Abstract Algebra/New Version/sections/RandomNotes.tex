\chapter{Random Notes}

\section{Interesting Observations from Roman}

\begin{observation}[Geometric Interpretation of Dual Vectors]
	The notion of the dual space of a vector space is somewhat abstract and one usually struggles to have a geometric realization of the functionals and dual spaces. Here, I provide a very interesting point of view. Let $ V $  be a finite dimensional vectors space. Then every $ f \in V^* $ is characterized by a hyperplane $ H $ such that $ H = \operatorname{ker} f $. 
	
	\noindent With this point of view, $ f(x) = 0 $ corresponds to the fact that $ x \in  H $. Also, it is very straight forward to see the following properties of functionals with this geometric point of view.
	\begin{proposition}
		\begin{enumerate}[(a)]
			\item If $ f(x) \neq 0 $ then 
			\[ V = \langle x \rangle \oplus \ker f. \]
			\item For every $ x\in V $ there exists $ f \in V^* $ such that $ f(x) \neq 0 $.
			\item For $ x\in V $, $ f(x) = 0 $ for all $ f\in V^* $ implies $ x = 0 $.
			\item 
		\end{enumerate}
	\end{proposition}
	\begin{proof}(Geometric interpretation)
		\begin{enumerate}[(a)]
			\item If $ x\notin H $ for some hyperplane $ H $, then 
			\[ V = \langle x \rangle \oplus H. \]
			\item Given any point of the space, there is some hyperplane that misses that particular point.s
			\item The only point that belongs to all hyperplanes is the origin.
		\end{enumerate}
	\end{proof}
\end{observation}



\begin{observation}[More Geometric Interpretation of Dual Vectors]
	The characterization above, i.e. identifying the linear functionals with their kernel, i.e. hyperplanes, work surprisingly well in characterizing very interesting facts. For instance, we can have the following definition of the annihilators of a set.
	\begin{definition}[Annihilators]
		Let $ M \subset V $ (no necessarily a linear subspace). Then the annihilators of $ M $, denoted by $ M^0 $ is the set of all linear functionals that kills $ M $. I.e.
		\[ M^0 = \set{f \in V^*| f(M) = 0}. \]
		With the geometric point of view above, the annihilators of $ M $ is the set of all hyperplanes that contain $ M $.
	\end{definition}
	For instance, let $ L $ be a one dimensional linear subspace of $ \R^3 $. Then $ L^0 $ will be the set of all hyperplanes containing $ L $. Each such hyperplane can be represented by a normal vectors. So the set of all hyperplanes containing $ L $ is isomorphic to a plane perpendicular to $ L $ and going through the origin (more generally, any 2-dimensional linear subspace of $ \R^3 $ that does not contain $ L $). It is now very straightforward to see the result of Theorem 3.14 part (2). The set $ M^{00} $ is the set of all hyperplanes containing $ M^0 $. There is just one such hyperplane, and since it can be parameterized using one normal vector (along $ L $), we have 
	\[ M^{00} \simeq \operatorname{span}L. \]
\end{observation}


\begin{observation}[Double Dual Map]
	We start with the following definition.
	\begin{definition}
		Let $ \tau \in \mathcal{L}(U,V) $. The dual map $ \tau^\times \in \mathcal{L}(V^*,U^*) $ and, the double dual map $ \tau^{\times\times} = \mathcal{L}(U^{**},V^{**}) $ is defined as
		\[  (\tau^\times f)(u) = f(\tau u), \qquad \text{for $ u\in U,\ f\in V^* $}, \]
		and 
		\[  (\tau^{\times\times} E)(f) = E (\tau^\times f), \qquad \text{for $ E\in V^{**},\ f\in W^* $}.  \]
		In finite dimension, the following is a very useful characterization of $ \tau^{\times\times} $. Let $ u\in U $ and using the canonical map $ u \mapsto E_u \in V^{**} $, where $ E_u $ is the evaluation map at $ u $.Also let $ f\in V^* $. Then we can write
		\begin{align*}
			(\tau^{\times\times} E_u)(f) &= E_u(\tau^\times f) \\
			&= (\tau^\times f)(u) \\
			&=f(\tau u) \\
			&=E_{\tau u} (f).
		\end{align*} 
		Thus we have
		\[ \tau^{\times\times}E_u = E_{\tau u}. \]
	\end{definition}
\end{observation}

\begin{observation}[Geometric Interpretation of Dual Map]
	For $ \tau \in \mathcal{L}(V,W) $, the dual map $ \tau^\times \in \mathcal{L}(W^*,V^*) $ is given by
	\[ (\tau^\times f)(v) = f(\tau v),  \]
	where $ f\in W^* $ and $ v \in V $. Using out geometric point of view of the functionals (as hyperplanes) we can have a geometric interpretation of what is the dual of a map. The following is a high level summary:
	\begin{quote}
		Let $ f\in W^* $ be a functional, i.e. a hyperplane. Then $ \tau^\times $ returns a hyperplane in $ V $ that is the pre-image of restriction of $ f $ to $ \im(\tau) $.
	\end{quote}
	For instance, if $ \tau : \R^2 \to \R^3 $ the inclusion map that sends $ \R^2 $ to the $ xy $ plane in $ \R^3 $, the $ \tau^\times $ map maps the following red hyperplane (as a functional in $ \R^3 $) to the green hyperplane (as a functional in $ \R^2 $).
	\begin{center}
			\includegraphics[width=0.4\linewidth]{Images/DualMap}
	\end{center}
	
	Using the interpretation above we can have the following ``geometric'' proof of the following facts in Roman (presented in Theorem 3.19).
	\begin{proposition}
		Let $ \tau \in \mathcal{L}(V,W) $. Then
		\begin{enumerate}[(a)]
			\item $ \ker(\tau^\times) = \im(\tau)^0 $.
			\item $ \im(\tau^\times) = \ker(\tau)^0 $.
		\end{enumerate}
		\begin{proof}[Geometric proof]
			\begin{enumerate}[(a)]
				\item We want to show $ \ker(\tau^\times ) \subset \im(\tau)^0 $. Let $ f\in \ker(\tau^\times) $ be a hyperplane (i.e. functional). This means that if we restrict $ f $ to $ \im(\tau) $ and then consider its pre-image, it should be the whole space (i.e. the zero functional). Thus $ f $ should contain $ \im(\tau) $. So $ f\in \im(\tau)^0 $ (remember that $ \im(\tau)^0 $) is the set of all hyperplanes containing $ \im(\tau) $. For the converse, we want to show $ \im(\tau)^0 \subset \ker(\tau^\times) $. Let $ f \in \im(\tau)^0 $. I.e. $ f $ is a hyperplane that contains $ \im(\tau) $. So restricting $ f $ to $ \im(\tau) $ will be whole $ \im(\tau) $. So the pre-image of the restriction of $ f $ to $ \im(\tau) $ will be the whole space $ V $ (thus the zero functional). So $ f\in \ker(\tau^\times) $. \emph{Note: We have used the fact that for any linear map $ \tau $ we have $ \im(\tau) \simeq \dom(\tau) $}.
				
				\item 
				
			\end{enumerate}
		\end{proof}
	\end{proposition}
\end{observation}



\begin{observation}[Coordinate maps]
	Let $ (V,F) $ be a vector space (defined on the field $ F $) with finite dimension $ n $. Once we choose an ordered basis for $ V $, like $ \mathcal{B} = (v_1,\cdots,v_n) $, we can define the coordinate map
	\[ \phi_\mathcal{B}: V \to F^n, \]
	that
	\[ v = \sum_i \alpha_i v_n \mapsto \begin{bmatrix}
		\alpha_1 \\
		\vdots \\
		\alpha_n
	\end{bmatrix}. \]
	In particular, for the basis vectors we have $ \phi(v_i) = e_i $, where $ e_i $ is a column vector whose entries are all zero, but the $ i^\text{th} $ row. This coordinate map $ \phi $ justifies the name ``vector space'' for this algebraic structure. The elements of any finite dimensional vector space defined on $ F $ can be ``coordinated'' by the elements of $ F^n $.
\end{observation}

\begin{observation}
	As a continuiation of the note above, lets now focus on the linear maps $ \mathcal{L}(F^n,F^m) $. We know that every matrix in $ A \in \mathcal{M}_{n,m} $ induces a linear map $ \tau_A \in \mathcal{L}(F^n,F^m) $, given by
	\[ \tau_A(v) = A v. \]
	The converse is also true. Every linear map $ \tau \in \mathcal{L}(F^n,F^m) $ has a matrix representation $ A \in \mathcal{M}_{n,m} $ given by
	\[ A = (\tau e_1 | \cdots | \tau e_n), \]
	i.e. apply $ \tau $ on the basis vectors, write the coordinates of the resulting vector in the columns of a matrix to get the matrix representation of the linear transformation.
\end{observation}


\begin{observation}
	I have started to notice a very interesting interaction between the following objects, and each pair of these notions induces a similar feeling. I have not yet been able to quantify this feeling. But I am sure there is some connection there. 
	\begin{center}
		\begin{tabular}{|c|c|c|c|}

			surjective & ker & spanning & exists \\
			\hline
			injective & img & linearly independent & for all \\

		\end{tabular}
	\end{center}
\end{observation}



\begin{observation}[Change of basis]
	Consider the following diagram.
	% https://q.uiver.app/#q=WzAsNixbMSwxLCJWIl0sWzEsMywiVyJdLFsyLDAsIkZebiJdLFswLDAsIkZebiJdLFsyLDQsIkZebSJdLFswLDQsIkZebSJdLFswLDIsIlxccGhpX1xcbWF0aGNhbHtCfSIsMl0sWzAsMywiXFxwaGlfXFxtYXRoY2Fse1xcdGlsZGUgQn0iXSxbMSw0LCJcXHBoaV97XFxtYXRoY2Fse0N9fSJdLFsxLDUsIlxccGhpX1xcbWF0aGNhbHtcXHRpbGRlIEN9IiwyXSxbMCwxLCJcXHRhdSIsMV0sWzIsNCwiW1xcdGF1XV97XFxtYXRoY2Fse0J9XFxtYXRoY2Fse0N9fSIsMCx7ImN1cnZlIjotMn1dLFszLDUsIltcXHRhdV1fe1xcbWF0aGNhbHtcXHRpbGRlIEJ9XFxtYXRoY2Fse1xcdGlsZGUgQ319IiwyLHsiY3VydmUiOjJ9XSxbMiwzLCJcXHBoaV97XFxtYXRoY2Fse0J9XFxtYXRoY2Fse1xcdGlsZGUgQn19IiwyXSxbNCw1LCJcXHBoaV97XFxtYXRoY2Fse0N9XFxtYXRoY2Fse1xcdGlsZGUgQ319Il1d
	\[\begin{tikzcd}
		{F^n} && {F^n} \\
		& V \\
		\\
		& W \\
		{F^m} && {F^m}
		\arrow["{[\tau]_{\mathcal{\tilde B}\mathcal{\tilde C}}}"', curve={height=12pt}, from=1-1, to=5-1]
		\arrow["{M_{\mathcal{B}\mathcal{\tilde B}} = \phi_{\mathcal{B}\mathcal{\tilde B}}}"', from=1-3, to=1-1]
		\arrow["{[\tau]_{\mathcal{B}\mathcal{C}}}", curve={height=-12pt}, from=1-3, to=5-3]
		\arrow["{\phi_\mathcal{\tilde B}}", from=2-2, to=1-1]
		\arrow["{\phi_\mathcal{B}}"', from=2-2, to=1-3]
		\arrow["\tau"{description}, from=2-2, to=4-2]
		\arrow["{\phi_\mathcal{\tilde C}}"', from=4-2, to=5-1]
		\arrow["{\phi_{\mathcal{C}}}", from=4-2, to=5-3]
		\arrow["{M_{\mathcal{C}\mathcal{\tilde C}} = \phi_{\mathcal{C}\mathcal{\tilde C}}}", from=5-3, to=5-1]
	\end{tikzcd}\]
	
	where $ V,W $ are vector spaces with dimension $ n $ and $ m $ respectively. Furthermore, $ \mathcal{B} $ and $ \mathcal{\tilde B} $ are two ordered basis for $ V $ with the coordinate maps $ \phi_\mathcal{B} $ and $ \phi_\mathcal{\tilde B} $ respectively. Similarly $ \mathcal{C} $ and $ \mathcal{\tilde C} $ are two ordered basis for $ W $ with $ \phi_\mathcal{C} $ and $ \phi_\mathcal{\tilde C} $ as the corresponding coordinate maps. The change of basis matrices are given in Theorem 2.12 Roman. This diagram summarizes the relation between the vectors in the abstract vector spaces $ V $ and $ W $ with their representation with we change the basis in either of the spaces. Also it capture the transformation that happens for the representation of linear maps when we change basis. For instance it is very easy to see
	\[ [\tau]_{\mathcal{\tilde B}\mathcal{\tilde C}} = M_{\mathcal{C}\mathcal{\tilde C}} [\tau]_{BC} M_{\mathcal{B}\mathcal{\tilde B}}^{-1}. \]
	It is also easier to memorize the following relation instead
	\[  [\tau]_{\mathcal{\tilde B} \mathcal{\tilde C}} M_{\mathcal{B}\mathcal{\tilde B}} =   M_{\mathcal{C}\mathcal{\tilde C}} [\tau]_{\mathcal{B}\mathcal{C}}. \]
\end{observation}


\begin{observation}[Characteristic of a filed and the alternating, v.s. skew-symmetric forms]
	It is only when $ \Char(F) \neq 2 $ we have the equivalence between the alternating forms and the skew-symmetric forms
	\[ \text{alternating} \qquad \Longleftrightarrow \qquad \text{skew-symmetric}. \]
	For the forward direction we don't need any restriction on the characteristic of the field. To see this let $ f $ be an alternating bi-linear form.
	\[ f(u+v,u+v) = f(u,u) + f(v,v) + f(u,v) + f(v,u). \]
	Since $ f $ is alternating, we have $ f(u+v,u+v) = 0 $ as well as $ f(u,u) = f(v,v) = 0 $. So we can conclude that 
	\[ f(u,v) = - f(v,u). \]
	However, for the converse, we need $ \Char(F) \ neq 2 $. Because if $ f $ is skew-symmetric, then $ f(u,u) = - f(u,u) $, which implies $ 2f(u,u) = 0  $. We can only conclude $ f(u,u) = 0 $ when $ \Char(F) \neq 2 $, i.e. when $ 2 $ is invertible in the field. 
\end{observation}


\begin{observation}[Symmetric and Antisymmetric tensor products with Roman's notation]
	In Roman text book, he introduces the notion of the symmetric and anti-symmetric tensor products with a notation that is not easy to understand, unless there is a running example. Here in this box, I will give an explicit example. Let $ V $ has dimension $ n=3 $ and let $ \set{e_1,e_2,e_3} $ be a basis for $ V $. We want to explicitly construct the basis vectors for the $ \ST^p(V) $ and $ \AT^p(V) $. We will have the following cases
	\begin{enumerate}[(i)]
		\item $ p = 2 $. Then the basis elements of $ \ST^2(V) $ will be
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,1} $ & $ e_1\otimes e_1 $ & $ e_1\vee e_1 $ \\
				\hline 
				$ \set{2,2} $ & $ e_2\otimes e_2 $ & $ e_2\vee e_2 $ \\
				\hline 
				$ \set{3,3} $ & $ e_3\otimes e_3 $ & $ e_3\vee e_3 $ \\
				\hline 
				$ \set{1,2} $ & $ e_1\otimes e_2 + e_2\otimes e_1 $ & $ e_1\vee e_2 $ \\
				\hline 
				$ \set{1,3} $ & $ e_1\otimes e_3 + e_3\otimes e_1 $ & $ e_1\vee e_3 $ \\
				\hline 
				$ \set{2,3} $ & $ e_1\otimes e_2 + e_2\otimes e_1 $ & $ e_1\vee e_2 $ \\
				\hline 
			\end{tabular}
		\end{center}
		
		And the basis elements of $ \AT^2(V) $ will be
		
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,2} $ & $ e_1\otimes e_2 - e_2\otimes e_1 $ & $ e_1\wedge e_2 $ \\
				\hline
				$ \set{1,3} $ & $ e_1\otimes e_3 - e_3\otimes e_1 $ & $ e_1\wedge e_3 $ \\
				\hline
				$ \set{2,3} $ & $ e_2\otimes e_3 - e_3\otimes e_2 $ & $ e_2\wedge e_3 $ \\
				\hline
			\end{tabular}
		\end{center}
		
		
		\item $ p=3 $. Then the basis elements of $ \ST^3(V) $ will be
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,1,1} $ & $ e_1\otimes e_1\otimes e_1 $ & $ e_1\vee e_1 \vee e_1 $ \\
				\hline
				$ \set{2,2,2} $ & $ e_2\otimes e_2\otimes e_2 $ & $ e_2\vee e_2 \vee e_2 $ \\
				\hline
				$ \set{3,3,3} $ & $ e_3\otimes e_3\otimes e_3 $ & $ e_3\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{1,2,2} $ & $ e_1\otimes e_2\otimes e_2 + e_2\otimes e_1\otimes e_2 + e_2\otimes e_2\otimes e_1 $ & $ e_1\vee e_2 \vee e_2 $ \\
				\hline
				$ \set{1,3,3} $ & $ e_1\otimes e_3\otimes e_3 + e_3\otimes e_1\otimes e_3 + e_3\otimes e_3\otimes e_1 $ & $ e_1\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{2,1,1} $ & $ e_2\otimes e_1\otimes e_1 + e_1\otimes e_2\otimes e_1 + e_2\otimes e_2\otimes e_1 $ & $ e_1\vee e_1 \vee e_2 $ \\
				\hline
				$ \set{2,3,3} $ & $ e_2\otimes e_3\otimes e_3 + e_3\otimes e_2\otimes e_3 + e_3\otimes e_3\otimes e_2 $ & $ e_2\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{3,1,1} $ & $ e_3\otimes e_1\otimes e_1 + e_1\otimes e_3\otimes e_1 + e_1\otimes e_1\otimes e_3 $ & $ e_1\vee e_1 \vee e_3 $ \\
				\hline
				$ \set{3,2,2} $ & $ e_3\otimes e_2\otimes e_2 + e_2\otimes e_3\otimes e_2 + e_2\otimes e_2\otimes e_3 $ & $ e_2\vee e_2 \vee e_3 $ \\
				\hline
				$ \set{1,2,3} $ & \makecell{$ e_1\otimes e_2\otimes e_3 + e_1\otimes e_3\otimes e_2 + e_2\otimes e_1\otimes e_3 $ \\ $ + e_2\otimes e_3\otimes e_1 + e_3\otimes e_1\otimes e_2 + e_3\otimes e_2\otimes e_1 $} & $ e_1\vee e_2 \vee e_3 $ \\
				\hline
			\end{tabular}
		\end{center}
		
		And for $ \AT^3(V) $ we have
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,2,3} $ & \makecell{$ e_1\otimes e_2\otimes e_3 - e_1\otimes e_3\otimes e_2 + e_2\otimes e_1\otimes e_3 $ \\ $ - e_2\otimes e_3\otimes e_1 + e_3\otimes e_1\otimes e_2 - e_3\otimes e_2\otimes e_1 $} & $ e_1\wedge  e_2 \wedge e_3 $ \\
				\hline
				
			\end{tabular}
		\end{center}
		
		This we have following two theorems.
		
		\begin{theorem}
			Let $ V $ be a finite dimensional vector space, and let $ \mathcal{B} = \set{e_1,\cdots,e_n} $ be a basis. Then 
			\[ \ST^p(V) \simeq F_p[e_1,\cdots,e_n], \]
			and
			\[ \AT^p(V) \simeq F_p^-[e_1,\cdots,e_n]. \]
			In words,
			\begin{quote}
				The symmetric tensor space $ \ST^p(V) $ is isomorphic to the algebra $ F_p[e_1,\cdots,e_n] $ of homogeneous polynomials of degree $ p $. 
			\end{quote}
			And similarly
			\begin{quote}
				The anti-symmetric tensor space $ \AT^p(V) $ is isomorphic to the algebra $ F_p^-[e_1,\cdots,e_n] $ of anti-commutative homogeneous polynomials of degree $ p $.
			\end{quote}
		\end{theorem}
		
		It is easy to see (Roman Theorem 14.18) that 
		\[ \dim(\AT^p(V)) = \binom{n}{p}, \qquad \dim(\ST^p(V))=\binom{n+p-1}{p}. \]
		The formula for the dimension of $ \ST^p(V) $ resembles the formula for all possible distribution of $ p $ units on energy in $ n $ containers (see Schroeder, Equation 2.9).
		
		
		
	\end{enumerate}
\end{observation}




\section{Ongoing thoughts}

\begin{observation}[Ongoing thought on the relation of the space of linear operators and the tensor product]
	In many instances, I have noticed a similar structure between $ \mathcal{L}(V,W) $ and $ V \otimes W $. For instance, we know that while $ u\otimes v $ is a tensor (a pure tensor), but not every tensor can be written like this, but rather it is a linear combination of pure tensors. This is very similar to the idea that for $ A: V\to W $ and $ B: U\to W $, we can construct a linear map $ C: U\oplus V \to W $, that has a block diagonal representation. But, we can not write every matrix in a block diagonal representation. 
	
	Also, another hint is that $ \dim(\mathcal{L}(V,W)) = n\times m $, and similarly, $ \dim(U\otimes V) = n\times m $. Yet another hint is that every elements of $ U\otimes V $ has a matrix coordinate. I need to make this connection more clear and easy to see / understand.
\end{observation}






