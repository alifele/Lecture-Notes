\chapter{Random Notes}

\section{Interesting Observations from Roman}

\begin{observation}[Geometric Interpretation of Dual Vectors]
	The notion of the dual space of a vector space is somewhat abstract and one usually struggles to have a geometric realization of the functionals and dual spaces. Here, I provide a very interesting point of view. Let $ V $  be a finite dimensional vectors space. Then every $ f \in V^* $ is characterized by a hyperplane $ H $ such that $ H = \operatorname{ker} f $. 
	
	\noindent With this point of view, $ f(x) = 0 $ corresponds to the fact that $ x \in  H $. Also, it is very straight forward to see the following properties of functionals with this geometric point of view.
	\begin{proposition}
		\begin{enumerate}[(a)]
			\item If $ f(x) \neq 0 $ then 
			\[ V = \langle x \rangle \oplus \ker f. \]
			\item For every $ x\in V $ there exists $ f \in V^* $ such that $ f(x) \neq 0 $.
			\item For $ x\in V $, $ f(x) = 0 $ for all $ f\in V^* $ implies $ x = 0 $.
			\item 
		\end{enumerate}
	\end{proposition}
	\begin{proof}(Geometric interpretation)
		\begin{enumerate}[(a)]
			\item If $ x\notin H $ for some hyperplane $ H $, then 
			\[ V = \langle x \rangle \oplus H. \]
			\item Given any point of the space, there is some hyperplane that misses that particular point.s
			\item The only point that belongs to all hyperplanes is the origin.
		\end{enumerate}
	\end{proof}
\end{observation}



\begin{observation}[More Geometric Interpretation of Dual Vectors]
	The characterization above, i.e. identifying the linear functionals with their kernel, i.e. hyperplanes, work surprisingly well in characterizing very interesting facts. For instance, we can have the following definition of the annihilators of a set.
	\begin{definition}[Annihilators]
		Let $ M \subset V $ (no necessarily a linear subspace). Then the annihilators of $ M $, denoted by $ M^0 $ is the set of all linear functionals that kills $ M $. I.e.
		\[ M^0 = \set{f \in V^*| f(M) = 0}. \]
		With the geometric point of view above, the annihilators of $ M $ is the set of all hyperplanes that contain $ M $.
	\end{definition}
	For instance, let $ L $ be a one dimensional linear subspace of $ \R^3 $. Then $ L^0 $ will be the set of all hyperplanes containing $ L $. Each such hyperplane can be represented by a normal vectors. So the set of all hyperplanes containing $ L $ is isomorphic to a plane perpendicular to $ L $ and going through the origin (more generally, any 2-dimensional linear subspace of $ \R^3 $ that does not contain $ L $). It is now very straightforward to see the result of Theorem 3.14 part (2). The set $ M^{00} $ is the set of all hyperplanes containing $ M^0 $. There is just one such hyperplane, and since it can be parameterized using one normal vector (along $ L $), we have 
	\[ M^{00} \simeq \operatorname{span}L. \]
\end{observation}


\begin{observation}[Double Dual Map]
	We start with the following definition.
	\begin{definition}
		Let $ \tau \in \mathcal{L}(U,V) $. The dual map $ \tau^\times \in \mathcal{L}(V^*,U^*) $ and, the double dual map $ \tau^{\times\times} = \mathcal{L}(U^{**},V^{**}) $ is defined as
		\[  (\tau^\times f)(u) = f(\tau u), \qquad \text{for $ u\in U,\ f\in V^* $}, \]
		and 
		\[  (\tau^{\times\times} E)(f) = E (\tau^\times f), \qquad \text{for $ E\in V^{**},\ f\in W^* $}.  \]
		In finite dimension, the following is a very useful characterization of $ \tau^{\times\times} $. Let $ u\in U $ and using the canonical map $ u \mapsto E_u \in V^{**} $, where $ E_u $ is the evaluation map at $ u $.Also let $ f\in V^* $. Then we can write
		\begin{align*}
			(\tau^{\times\times} E_u)(f) &= E_u(\tau^\times f) \\
			&= (\tau^\times f)(u) \\
			&=f(\tau u) \\
			&=E_{\tau u} (f).
		\end{align*} 
		Thus we have
		\[ \tau^{\times\times}E_u = E_{\tau u}. \]
	\end{definition}
\end{observation}

\begin{observation}[Geometric Interpretation of Dual Map]
	For $ \tau \in \mathcal{L}(V,W) $, the dual map $ \tau^\times \in \mathcal{L}(W^*,V^*) $ is given by
	\[ (\tau^\times f)(v) = f(\tau v),  \]
	where $ f\in W^* $ and $ v \in V $. Using out geometric point of view of the functionals (as hyperplanes) we can have a geometric interpretation of what is the dual of a map. The following is a high level summary:
	\begin{quote}
		Let $ f\in W^* $ be a functional, i.e. a hyperplane. Then $ \tau^\times $ returns a hyperplane in $ V $ that is the pre-image of restriction of $ f $ to $ \im(\tau) $.
	\end{quote}
	For instance, if $ \tau : \R^2 \to \R^3 $ the inclusion map that sends $ \R^2 $ to the $ xy $ plane in $ \R^3 $, the $ \tau^\times $ map maps the following red hyperplane (as a functional in $ \R^3 $) to the green hyperplane (as a functional in $ \R^2 $).
	\begin{center}
			\includegraphics[width=0.4\linewidth]{Images/DualMap}
	\end{center}
	
	Using the interpretation above we can have the following ``geometric'' proof of the following facts in Roman (presented in Theorem 3.19).
	\begin{proposition}
		Let $ \tau \in \mathcal{L}(V,W) $. Then
		\begin{enumerate}[(a)]
			\item $ \ker(\tau^\times) = \im(\tau)^0 $.
			\item $ \im(\tau^\times) = \ker(\tau)^0 $.
		\end{enumerate}
		\begin{proof}[Geometric proof]
			\begin{enumerate}[(a)]
				\item We want to show $ \ker(\tau^\times ) \subset \im(\tau)^0 $. Let $ f\in \ker(\tau^\times) $ be a hyperplane (i.e. functional). This means that if we restrict $ f $ to $ \im(\tau) $ and then consider its pre-image, it should be the whole space (i.e. the zero functional). Thus $ f $ should contain $ \im(\tau) $. So $ f\in \im(\tau)^0 $ (remember that $ \im(\tau)^0 $) is the set of all hyperplanes containing $ \im(\tau) $. For the converse, we want to show $ \im(\tau)^0 \subset \ker(\tau^\times) $. Let $ f \in \im(\tau)^0 $. I.e. $ f $ is a hyperplane that contains $ \im(\tau) $. So restricting $ f $ to $ \im(\tau) $ will be whole $ \im(\tau) $. So the pre-image of the restriction of $ f $ to $ \im(\tau) $ will be the whole space $ V $ (thus the zero functional). So $ f\in \ker(\tau^\times) $. \emph{Note: We have used the fact that for any linear map $ \tau $ we have $ \im(\tau) \simeq \dom(\tau) $}.
				
				\item 
				
			\end{enumerate}
		\end{proof}
	\end{proposition}
\end{observation}



\begin{observation}[Coordinate maps]
	Let $ (V,F) $ be a vector space (defined on the field $ F $) with finite dimension $ n $. Once we choose an ordered basis for $ V $, like $ \mathcal{B} = (v_1,\cdots,v_n) $, we can define the coordinate map
	\[ \phi_\mathcal{B}: V \to F^n, \]
	that
	\[ v = \sum_i \alpha_i v_n \mapsto \begin{bmatrix}
		\alpha_1 \\
		\vdots \\
		\alpha_n
	\end{bmatrix}. \]
	In particular, for the basis vectors we have $ \phi(v_i) = e_i $, where $ e_i $ is a column vector whose entries are all zero, but the $ i^\text{th} $ row. This coordinate map $ \phi $ justifies the name ``vector space'' for this algebraic structure. The elements of any finite dimensional vector space defined on $ F $ can be ``coordinated'' by the elements of $ F^n $.
\end{observation}

\begin{observation}
	As a continuiation of the note above, lets now focus on the linear maps $ \mathcal{L}(F^n,F^m) $. We know that every matrix in $ A \in \mathcal{M}_{n,m} $ induces a linear map $ \tau_A \in \mathcal{L}(F^n,F^m) $, given by
	\[ \tau_A(v) = A v. \]
	The converse is also true. Every linear map $ \tau \in \mathcal{L}(F^n,F^m) $ has a matrix representation $ A \in \mathcal{M}_{n,m} $ given by
	\[ A = (\tau e_1 | \cdots | \tau e_n), \]
	i.e. apply $ \tau $ on the basis vectors, write the coordinates of the resulting vector in the columns of a matrix to get the matrix representation of the linear transformation.
\end{observation}


\begin{observation}
	I have started to notice a very interesting interaction between the following objects, and each pair of these notions induces a similar feeling. I have not yet been able to quantify this feeling. But I am sure there is some connection there. 
	\begin{center}
		\begin{tabular}{|c|c|c|c|}

			surjective & ker & spanning & exists \\
			\hline
			injective & img & linearly independent & for all \\

		\end{tabular}
	\end{center}
\end{observation}



\begin{observation}[Change of basis]
	Consider the following diagram.
	% https://q.uiver.app/#q=WzAsNixbMSwxLCJWIl0sWzEsMywiVyJdLFsyLDAsIkZebiJdLFswLDAsIkZebiJdLFsyLDQsIkZebSJdLFswLDQsIkZebSJdLFswLDIsIlxccGhpX1xcbWF0aGNhbHtCfSIsMl0sWzAsMywiXFxwaGlfXFxtYXRoY2Fse1xcdGlsZGUgQn0iXSxbMSw0LCJcXHBoaV97XFxtYXRoY2Fse0N9fSJdLFsxLDUsIlxccGhpX1xcbWF0aGNhbHtcXHRpbGRlIEN9IiwyXSxbMCwxLCJcXHRhdSIsMV0sWzIsNCwiW1xcdGF1XV97XFxtYXRoY2Fse0J9XFxtYXRoY2Fse0N9fSIsMCx7ImN1cnZlIjotMn1dLFszLDUsIltcXHRhdV1fe1xcbWF0aGNhbHtcXHRpbGRlIEJ9XFxtYXRoY2Fse1xcdGlsZGUgQ319IiwyLHsiY3VydmUiOjJ9XSxbMiwzLCJcXHBoaV97XFxtYXRoY2Fse0J9XFxtYXRoY2Fse1xcdGlsZGUgQn19IiwyXSxbNCw1LCJcXHBoaV97XFxtYXRoY2Fse0N9XFxtYXRoY2Fse1xcdGlsZGUgQ319Il1d
	\[\begin{tikzcd}
		{F^n} && {F^n} \\
		& V \\
		\\
		& W \\
		{F^m} && {F^m}
		\arrow["{[\tau]_{\mathcal{\tilde B}\mathcal{\tilde C}}}"', curve={height=12pt}, from=1-1, to=5-1]
		\arrow["{M_{\mathcal{B}\mathcal{\tilde B}} = \phi_{\mathcal{B}\mathcal{\tilde B}}}"', from=1-3, to=1-1]
		\arrow["{[\tau]_{\mathcal{B}\mathcal{C}}}", curve={height=-12pt}, from=1-3, to=5-3]
		\arrow["{\phi_\mathcal{\tilde B}}", from=2-2, to=1-1]
		\arrow["{\phi_\mathcal{B}}"', from=2-2, to=1-3]
		\arrow["\tau"{description}, from=2-2, to=4-2]
		\arrow["{\phi_\mathcal{\tilde C}}"', from=4-2, to=5-1]
		\arrow["{\phi_{\mathcal{C}}}", from=4-2, to=5-3]
		\arrow["{M_{\mathcal{C}\mathcal{\tilde C}} = \phi_{\mathcal{C}\mathcal{\tilde C}}}", from=5-3, to=5-1]
	\end{tikzcd}\]
	
	where $ V,W $ are vector spaces with dimension $ n $ and $ m $ respectively. Furthermore, $ \mathcal{B} $ and $ \mathcal{\tilde B} $ are two ordered basis for $ V $ with the coordinate maps $ \phi_\mathcal{B} $ and $ \phi_\mathcal{\tilde B} $ respectively. Similarly $ \mathcal{C} $ and $ \mathcal{\tilde C} $ are two ordered basis for $ W $ with $ \phi_\mathcal{C} $ and $ \phi_\mathcal{\tilde C} $ as the corresponding coordinate maps. The change of basis matrices are given in Theorem 2.12 Roman. This diagram summarizes the relation between the vectors in the abstract vector spaces $ V $ and $ W $ with their representation with we change the basis in either of the spaces. Also it capture the transformation that happens for the representation of linear maps when we change basis. For instance it is very easy to see
	\[ [\tau]_{\mathcal{\tilde B}\mathcal{\tilde C}} = M_{\mathcal{C}\mathcal{\tilde C}} [\tau]_{BC} M_{\mathcal{B}\mathcal{\tilde B}}^{-1}. \]
	It is also easier to memorize the following relation instead
	\[  [\tau]_{\mathcal{\tilde B} \mathcal{\tilde C}} M_{\mathcal{B}\mathcal{\tilde B}} =   M_{\mathcal{C}\mathcal{\tilde C}} [\tau]_{\mathcal{B}\mathcal{C}}. \]
\end{observation}


\begin{observation}[Characteristic of a filed and the alternating, v.s. skew-symmetric forms]
	It is only when $ \Char(F) \neq 2 $ we have the equivalence between the alternating forms and the skew-symmetric forms
	\[ \text{alternating} \qquad \Longleftrightarrow \qquad \text{skew-symmetric}. \]
	For the forward direction we don't need any restriction on the characteristic of the field. To see this let $ f $ be an alternating bi-linear form.
	\[ f(u+v,u+v) = f(u,u) + f(v,v) + f(u,v) + f(v,u). \]
	Since $ f $ is alternating, we have $ f(u+v,u+v) = 0 $ as well as $ f(u,u) = f(v,v) = 0 $. So we can conclude that 
	\[ f(u,v) = - f(v,u). \]
	However, for the converse, we need $ \Char(F) \ neq 2 $. Because if $ f $ is skew-symmetric, then $ f(u,u) = - f(u,u) $, which implies $ 2f(u,u) = 0  $. We can only conclude $ f(u,u) = 0 $ when $ \Char(F) \neq 2 $, i.e. when $ 2 $ is invertible in the field. 
\end{observation}


\begin{observation}[Symmetric and Antisymmetric tensor products with Roman's notation]
	In Roman text book, he introduces the notion of the symmetric and anti-symmetric tensor products with a notation that is not easy to understand, unless there is a running example. Here in this box, I will give an explicit example. Let $ V $ has dimension $ n=3 $ and let $ \set{e_1,e_2,e_3} $ be a basis for $ V $. We want to explicitly construct the basis vectors for the $ \ST^p(V) $ and $ \AT^p(V) $. We will have the following cases
	\begin{enumerate}[(i)]
		\item $ p = 2 $. Then the basis elements of $ \ST^2(V) $ will be
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,1} $ & $ e_1\otimes e_1 $ & $ e_1\vee e_1 $ \\
				\hline 
				$ \set{2,2} $ & $ e_2\otimes e_2 $ & $ e_2\vee e_2 $ \\
				\hline 
				$ \set{3,3} $ & $ e_3\otimes e_3 $ & $ e_3\vee e_3 $ \\
				\hline 
				$ \set{1,2} $ & $ e_1\otimes e_2 + e_2\otimes e_1 $ & $ e_1\vee e_2 $ \\
				\hline 
				$ \set{1,3} $ & $ e_1\otimes e_3 + e_3\otimes e_1 $ & $ e_1\vee e_3 $ \\
				\hline 
				$ \set{2,3} $ & $ e_1\otimes e_2 + e_2\otimes e_1 $ & $ e_1\vee e_2 $ \\
				\hline 
			\end{tabular}
		\end{center}
		
		And the basis elements of $ \AT^2(V) $ will be
		
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,2} $ & $ e_1\otimes e_2 - e_2\otimes e_1 $ & $ e_1\wedge e_2 $ \\
				\hline
				$ \set{1,3} $ & $ e_1\otimes e_3 - e_3\otimes e_1 $ & $ e_1\wedge e_3 $ \\
				\hline
				$ \set{2,3} $ & $ e_2\otimes e_3 - e_3\otimes e_2 $ & $ e_2\wedge e_3 $ \\
				\hline
			\end{tabular}
		\end{center}
		
		
		\item $ p=3 $. Then the basis elements of $ \ST^3(V) $ will be
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,1,1} $ & $ e_1\otimes e_1\otimes e_1 $ & $ e_1\vee e_1 \vee e_1 $ \\
				\hline
				$ \set{2,2,2} $ & $ e_2\otimes e_2\otimes e_2 $ & $ e_2\vee e_2 \vee e_2 $ \\
				\hline
				$ \set{3,3,3} $ & $ e_3\otimes e_3\otimes e_3 $ & $ e_3\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{1,2,2} $ & $ e_1\otimes e_2\otimes e_2 + e_2\otimes e_1\otimes e_2 + e_2\otimes e_2\otimes e_1 $ & $ e_1\vee e_2 \vee e_2 $ \\
				\hline
				$ \set{1,3,3} $ & $ e_1\otimes e_3\otimes e_3 + e_3\otimes e_1\otimes e_3 + e_3\otimes e_3\otimes e_1 $ & $ e_1\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{2,1,1} $ & $ e_2\otimes e_1\otimes e_1 + e_1\otimes e_2\otimes e_1 + e_2\otimes e_2\otimes e_1 $ & $ e_1\vee e_1 \vee e_2 $ \\
				\hline
				$ \set{2,3,3} $ & $ e_2\otimes e_3\otimes e_3 + e_3\otimes e_2\otimes e_3 + e_3\otimes e_3\otimes e_2 $ & $ e_2\vee e_3 \vee e_3 $ \\
				\hline
				$ \set{3,1,1} $ & $ e_3\otimes e_1\otimes e_1 + e_1\otimes e_3\otimes e_1 + e_1\otimes e_1\otimes e_3 $ & $ e_1\vee e_1 \vee e_3 $ \\
				\hline
				$ \set{3,2,2} $ & $ e_3\otimes e_2\otimes e_2 + e_2\otimes e_3\otimes e_2 + e_2\otimes e_2\otimes e_3 $ & $ e_2\vee e_2 \vee e_3 $ \\
				\hline
				$ \set{1,2,3} $ & \makecell{$ e_1\otimes e_2\otimes e_3 + e_1\otimes e_3\otimes e_2 + e_2\otimes e_1\otimes e_3 $ \\ $ + e_2\otimes e_3\otimes e_1 + e_3\otimes e_1\otimes e_2 + e_3\otimes e_2\otimes e_1 $} & $ e_1\vee e_2 \vee e_3 $ \\
				\hline
			\end{tabular}
		\end{center}
		
		And for $ \AT^3(V) $ we have
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline
				$ M $ & $\sum_{t\in G_M}$t & equiv in $ F_2[e_1,e_2,e_3] $ \\
				\hline
				$ \set{1,2,3} $ & \makecell{$ e_1\otimes e_2\otimes e_3 - e_1\otimes e_3\otimes e_2 + e_2\otimes e_1\otimes e_3 $ \\ $ - e_2\otimes e_3\otimes e_1 + e_3\otimes e_1\otimes e_2 - e_3\otimes e_2\otimes e_1 $} & $ e_1\wedge  e_2 \wedge e_3 $ \\
				\hline
				
			\end{tabular}
		\end{center}
		
		This we have following two theorems.
		
		\begin{theorem}
			Let $ V $ be a finite dimensional vector space, and let $ \mathcal{B} = \set{e_1,\cdots,e_n} $ be a basis. Then 
			\[ \ST^p(V) \simeq F_p[e_1,\cdots,e_n], \]
			and
			\[ \AT^p(V) \simeq F_p^-[e_1,\cdots,e_n]. \]
			In words,
			\begin{quote}
				The symmetric tensor space $ \ST^p(V) $ is isomorphic to the algebra $ F_p[e_1,\cdots,e_n] $ of homogeneous polynomials of degree $ p $. 
			\end{quote}
			And similarly
			\begin{quote}
				The anti-symmetric tensor space $ \AT^p(V) $ is isomorphic to the algebra $ F_p^-[e_1,\cdots,e_n] $ of anti-commutative homogeneous polynomials of degree $ p $.
			\end{quote}
		\end{theorem}
		
		It is easy to see (Roman Theorem 14.18) that 
		\[ \dim(\AT^p(V)) = \binom{n}{p}, \qquad \dim(\ST^p(V))=\binom{n+p-1}{p}. \]
		The formula for the dimension of $ \ST^p(V) $ resembles the formula for all possible distribution of $ p $ units on energy in $ n $ containers (see Schroeder, Equation 2.9).
		
		
		
	\end{enumerate}
\end{observation}


\begin{observation}[Suitable basis for nilpotent maps]
	This note is meant to accompany the sections 8.4 to 8.8 of Leohen's advanced linear algebra. I demonstrate a concrete example to show how we can find a suitable basis for a nilpotent map such that its matrix representation has Jordan blocks. Consider the following nilpotent matrix.
	\[ A = 
	\begin{pmatrix}
		-16 & -20 & 34 \\
		2 & 52 & -29 \\
		0 & 72 & -36
	\end{pmatrix}. \]
	It is easy tot check that $ A^3 = 0 $. We want to find a suitable basis in which the matrix above has Jordan blocks in its structure. To do so, we first consider image $ A $. By writing the matrix in reduced row echelon form it is easy to see that the column space of $ A $ is
	\[ W =  \im(A) = \Span\set{\vecttt{-16}{2}{0}, \vecttt{-20}{52}{72}}. \]
	The subspace $ W $ is $ A $-invariant. So we can restrict $ A $ to this subspace. Fix the vectors above as the basis of $ W $. In this basis, let the matrix representation of $ A|_W $ is $ [A|_W] $. Then this needs to satisfy
	\[ AM = M[A|_W], \]
	where
	\[ M = \begin{pmatrix}
		-16 & 20 \\
		2 & 52 \\
		0 & 72
	\end{pmatrix}. \]
	One possible solution for $ B $ will be
	\[ [A|_W] = \begin{pmatrix}
		-16 & 128 \\
		2 & 16
	\end{pmatrix}. \]
	Again, we need to consider the image of this matrix. Writing the matrix $ A|_W $ in reduced row echelon form we can see that 
	\[ W' = \im(A|_W) = \Span\set{\vectt{-16}{2}}. \]
	Now we need to restrict $ A|_W $ to $ W' $ above. However since $ W' $ is one dimensional and $ A|_W $ in nilpotent (since $ A $ is nilpotent), $ (A|_W)|_{W'} $ will be the zero map. So we can choose 
	\[ e_1 = \vectt{-16}{2}, \]
	as the first vector in the desired basis. 
	Since $ \ker(A|_W) $ is the same as $ \im(A|_W) $, then $ \im(A|_W) + \ker(A|_W) = \im(A|_W) $ and the basis can not be extended further (stage to of finding suitable basis for the nilpotent map $ A|_W $). We need to perform stage $ 3 $ and find a vector in $ W $ that maps to $ W' $. One possible choice is 
	\[ e_2 = \vectt{1}{0}. \]
	Note that in both of the representations above we are assuming that the basis vectors for the space $ W $ is as fixed above. Lastly, we need to perform stage 2 for the map $ A $, and since $ \ker(A) \subset \im(A), $
	this has no interesting result. Finally, for stage 3 for $ A $, we need to find some $ e_3 \in V $
	such that $ Ae_3 = e_2 $. By using the pseudo inverse for $ A $ we can find 
	\[ e_3 = \frac{1}{14}\vecttt{5}{-3}{-6}. \]
	So the suitable basis is
	\[ e_1 = -16\vecttt{-6}{2}{0} + 2\vecttt{20}{52}{72},\quad  e_2 = \vecttt{-16}{2}{0},\quad  e_3 = \frac{1}{14}\vecttt{5}{-3}{-6}.  \]
	It is easy to see
	\[ Ae_1 = 0, \quad A^2e_2 =0, \quad A^3e_3 = 0. \]
	You can experiment with the code written in python to perform the computations above, where the code is included in the jupyter notebook file in the Codes folder of this latex project. 
	
	\begin{minted}{python}
		from sympy import Matrix
		import sympy as sp
		
		# Define the matrix
		
		# Perform row reduction (RREF - Reduced Row Echelon Form)
		# rref_matrix, pivot_columns = A_sym.rref()
		
		
		def imageBasis(matrix):
		# List of basis vectors
		colSpace = matrix.columnspace()  
		# Stack the first two vectors into a matrix
		return Matrix.hstack(*colSpace[:])  
		
		
		
		def kernelBasis(matrix):
		"""Computes a basis for the null space (kernel) of a matrix."""
		nullSpace = matrix.nullspace()
		# Returns a list of basis vectors for the null space
		return Matrix.hstack(*nullSpace[:])  
		
		
		def restrictTo(A,M):
		"""
		Note that colSpace(B) should be image of A (thus A invariant)
		"""
		
		return (M.T * M).pinv() * (M.T * A * M)
		
		
	\end{minted}
	
\end{observation}



\begin{observation}[Step by step walk through for calculating the generalized eigenspaces]
	Consider the following matrix
	\[
	A  = 
	\begin{bmatrix*}[r]
		7 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 7 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 7 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -4 & 1 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -4 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i & 1 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i & 1 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & i
	\end{bmatrix*}.
	\]
	We want to construct the generalized eigenspaces for this matrix. First, observe that the spectrum of this matrix is $ \Spect_A = \set{7,-4,i} $. This can be calculated by
	\begin{minted}{python}
		A.eigenvals()
	\end{minted}
	From the form of the matrix, it is immediate that for $ \lambda = 7 $, there are $ 3 $ associated generalized eigenspaces. To find these three, we first calculate the eigenvalues associated to $ \lambda = 7 $. We will have
	\[ \ker(A - 7 I) = \Span\set{
	\begin{bmatrix}
		1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}
	} \]
	These computations can be done by
	\begin{minted}{python}
		I = sp.eye(J.shape[0])
		v1,v2,v3 = ((A - (7)*I)).nullspace()
	\end{minted}
	Now for each of the vectors above, we calculate their Jordan chain. I.e. we solve 
	\[ (A - \lambda I)w = v, \]
	where we start by $ v $ as one of the vector above to get $ w $ (second vector in the chain), and again, we replace $ v $  with $ w $ calculated above and solve the equation again to find a new $ w $, which will be the third element in the chain, and so on. This process will stop eventually (the solution of the equation will be the zero vector). Starting with the each of the vectors above, we will get
	\[ V_1^1 = \Span\set{
		\begin{bmatrix}
			1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}, \
		\begin{bmatrix}
			0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}
		}, \qquad
		V_1^2 = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}, \
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}
		}, \qquad
		V_1^3 = \Span\set{
			\begin{bmatrix}
				0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
			\end{bmatrix}
		}.
	\]
	Now for $ \lambda = -4 $ do a similar process. Observe that for this eigenvalue we have only one Jordan block matrix. First, we calculate the eigenvectors, and starting from each of them we compute the generalized eigenvectors. It turns out that there is just one eigenvalue for $ \lambda = -4 $
	\[ \ker(A-(-4)I) = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
		\end{bmatrix}
	}. \]
	Similar to above, we can calculate the Jordan chain for starting with this vector. We will see
	\[ 
	V_2^1 = \Span\set{
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0
	\end{bmatrix}
	}.
	\]
	Finally, for $ \lambda = i $, it turns out (as also evident from the matrix), there are two eigenvalues (translates to two Jordan blocks) as
	\[ \ker(A - iI) = \Span\set{
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, \
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1
	\end{bmatrix}
	}. \]
	And with the calculations similar to above we will get
	\[ V_{3}^1 = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
		\end{bmatrix}, \
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0
		\end{bmatrix}
	}, \qquad
	V_{3}^2 = \Span\set{
		\begin{bmatrix}
			0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1
		\end{bmatrix}
	}.
	 \]
	 
	The calculation above can be found under Example 3 in the JordanForm.ipynb file inside the Codes directory.
\end{observation}


\begin{observation}[Step by step calculation of Jordan canonical form]
	See StepByStepJordanCanonicalForm.ipynb in the Codes directory of this latex project for a complete example of calculating the Jordan canonical form.
\end{observation}

	
	
\begin{observation}[Yet another chance to understand the universal property of tensor product]
	The universal property for the tensor product states that if $ U,V $ are two vector spaces, and $ W $ is any other vector space, where $ f: U\times V \to W $ is a bilinear map, then there exists a universal pair $ (\tau, U\otimes V) $ such that $ f $ can be factored through $ \tau $, i.e. $ f = t\circ \tau $. In other words the following diagram commutes
	% https://q.uiver.app/#q=WzAsMyxbMSwwLCJVXFx0aW1lcyBWIl0sWzEsMSwiVVxcb3RpbWVzIFYiXSxbMCwxLCJXIl0sWzAsMiwiZiIsMl0sWzAsMSwiXFx0YXUiXSxbMSwyLCJ0Il1d
	\[\begin{tikzcd}
		& {U\times V} \\
		W & {U\otimes V}
		\arrow["f"', from=1-2, to=2-1]
		\arrow["\tau", from=1-2, to=2-2]
		\arrow["t", from=2-2, to=2-1]
	\end{tikzcd}\]
	where $ f = t\circ \tau $.
	Here is an intuitive construction for this concept. Let $ V = \R^3 $ and $ U = \R^2 $ that $ V = \Span\set{e'_1,e'_2,e'_3} $ and $ U = \Span\set{e_1,e_2} $. Let $ f: U\times V \to W $ be a bilinear map where $ W $ is any vector space $ \R^n $. For $ (a,b) \in U\times V $ we can write
	\begin{align*}
		f(a,b) &= f(a_1e_1+a_2e_2, b_1e'_1+b_2e'_2+b_3e'_3) \\
		&= a_1b_1f(e_1,e'_1) + a_1b_2f(e_1,e'_2) + a_1b_3f(e_1,e'_3) + a_2b_1f(e_2,e'_1) + a_2b_2f(e_2,e'_2) + a_2b_3f(e_2,e'_3).
	\end{align*}
	So $ f $ is fully specified by knowing its effect on $ (e_i,e'_j) $ tuple.s Assume the map $ f $ sends
	\[ (e_1,e'_1) \mapsto w_1,\quad (e_1,e'_2)\mapsto w_2,\quad (e_1,e'_3)\mapsto w_3,\quad (e_2,e'_1)\mapsto w_4, \quad (e_2,e'_2)\mapsto w_5, \quad (e_2,e'_3)\mapsto w_6,   \]
	where $ w_1,\cdots,w_6 \in W $. So the effect of $ f $ can be written as
	\[ 
	\begin{pmatrix}
		| & | & \cdots & | \\
		w_1 & w_2 & \cdots & w_6 \\
		| & | & \cdots & | 
	\end{pmatrix}
	\begin{bmatrix}
		a_1 \begin{bmatrix}
			b_1 \\ b_2 \\ b_3
		\end{bmatrix} \\
		a_2 \begin{bmatrix}
			b_1 \\ b_2 \\ b_3
		\end{bmatrix}
	\end{bmatrix}
	= 
	\begin{pmatrix}
		| & | & \cdots & | \\
		w_1 & w_2 & \cdots & w_6 \\
		| & | & \cdots & | 
	\end{pmatrix}
	\begin{bmatrix}
		a_1b_1 \\ a_1b_2 \\ a_1b_3 \\ a_2b_1 \\ a_2 b_2 \\ a_2b_3
	\end{bmatrix}
	 \]
	Intuitively, we form a basis that is the Cartesian product of the basis of the initial spaces to be able to write the bilinear form as a matrix multiplication.
\end{observation}

\begin{observation}[Yet another chance to understand the universal property of direct sum]
	Let $ U = \R^2 = \Span\set{e_1,e_2} $ and $ V = \R^3 = \Span\set{e_1',e_2',e_3'} $. Let $ (f,g) $ be two linear maps $ f: U\to W $ and $ g: V\to W $ where $ W = \R^n $  is any vector space. Let $ a\in U $ and $ b\in V $ and observe that any such pair of linear maps $ (f,g) $ can be characterized by action of $ f $ and $ g $ as
	\[ f(a) = f(a_1e_1+a_2e_2) = a_1f(e_1) + a_2f(e_2), \]
	and
	\[ g(b) = g(b_1e_1'+b_2e_2'+b_3e_3') = b_1g(e_1')+b_2g(e_2')+b_3(e_3'). \]
	Assume 
	\[ f:\quad e_1 \mapsto w_1, \quad e_2 \mapsto w_2, \]
	and 
	\[ g: \quad e_1' \mapsto w_1',\quad e_2'\mapsto w_2',\quad e_3'\mapsto w_3' \]
	where $ w_1,w_2,w_1',w_2',w_3' \in W $. So this follows that we can write the action of $ f,g $ together as
	\[ 
	\begin{pmatrix}
		\begin{pmatrix}
			| & | \\
			w_1 & w_2 \\
			| & | 
		\end{pmatrix} & 0 \\
		0 & \begin{pmatrix}
			| & | & | \\
			w_1' & w_2' & w_3' \\
			| & | & | 
		\end{pmatrix}
	\end{pmatrix}
	\begin{bmatrix}
		a_1 \\ a_2 \\ b_1 \\ b_2 \\ b_3
	\end{bmatrix}
	 \]
	So in fact, we concatenate the basis vectors of the older spaces to get a new basis for the new space so that we can express any pair of linear maps $ (f,g) $ as a single linear map.
\end{observation}


\begin{observation}[How to quickly define a bilinear form?]
	Let $ U = \R^2 $ and $ V = \R^3 $ and assume we want to define a bilinear form. It is enough to specify
	\[ (e_1,e_1)\mapsto \alpha_1, \quad (e_1,e_2)\mapsto \alpha_2, \quad (e_1,e_3) \mapsto \alpha_3, \quad (e_2,e_1)\mapsto \alpha_4, \quad (e_2,e_2)\mapsto \alpha_5, \quad (e_2,e_3) \mapsto \alpha_6. \]
	Let $ u\in U $ and $ v \in V $. Then we can write $ f(u,v) $ as 
	\[ \begin{pmatrix}
		\alpha_1 & \alpha_2 & \alpha_3 & \alpha_4 & \alpha_5 & \alpha_6
	\end{pmatrix} 
	\begin{bmatrix}
		u_1 \begin{bmatrix}
			v_1 \\ v_2 \\ v_3
		\end{bmatrix} \\
		u_2 \begin{bmatrix}
			v_1 \\ v_2 \\ v_3
		\end{bmatrix}
	\end{bmatrix}.
	 \]
\end{observation}

\begin{observation}[A simple example for Theorem 14.6 Roman]
	In this observation box I will demonstrate some example for the representation of tensors. I will use the vector notation to represent the tensors (instead of the matrix notation) as it makes more sense to work with this notation in this example.
	Let $ U = \R^2 $ and $ V = \R^3 $, and we are using the convention that 
	\[ u \otimes v = \begin{bmatrix}
		u_1 \begin{bmatrix}
			v_1 \\ v_2 \\ v_3
		\end{bmatrix} \\
		u_2 \begin{bmatrix}
			v_1 \\ v_2 \\ v_3
		\end{bmatrix}
	\end{bmatrix} = 
	\begin{bmatrix}
		u_1 v_1 \\ u_1 v_2 \\ u_1 v_3 \\ u_2 v_1 \\ u_2 v_2 \\ u_2 v_3
	\end{bmatrix}. \]
	Consider the following tensor
	\[ z = r_{11}e_1\otimes e_1 + r_{12}e_1\otimes e_2 + r_{13} e_1\otimes e_3 + r_{21} e_2\otimes e_1 + r_{22} e_2\otimes e_2 + r_{23}e_2\otimes e_3. \]
	One representation for this tensor is to write (representation (1) in theorem 14.6 Roman)
	\[ z 
	= \begin{bmatrix}
		r_{11} \\ r_{12} \\ r_{13} \\ r_{21} \\ r_{22} \\ r_{23}
	\end{bmatrix}
	 \]
	 Another possible way to write this is (representation (2) in theorem 14.6)
	\[ \begin{bmatrix}
		r_{11} \\ r_{12} \\ r_{13} \\ r_{21} \\ r_{22} \\ r_{23}
	\end{bmatrix} = 
	\begin{bmatrix}
		r_{11} \\ r_{12} \\ r_{13} \\ 0 \\ 0 \\ 0
	\end{bmatrix} + 
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ r_{21} \\ r_{22} \\ r_{23}
	\end{bmatrix} = 
	\begin{bmatrix}
		1 \\ 0
	\end{bmatrix} \otimes 
	\begin{bmatrix}
		r_{11} \\ r_{12} \\ r_{13}
	\end{bmatrix} + 
	\begin{bmatrix}
		0 \\ 1
	\end{bmatrix} \otimes 
	\begin{bmatrix}
		r_{21} \\ r_{22} \\ r_{23}
	\end{bmatrix}
	 \]
	Also, we can write (representation (3) in theorem 14.6 Roman)
	\[  
	 \begin{bmatrix}
		r_{11} \\ r_{12} \\ r_{13} \\ r_{21} \\ r_{22} \\ r_{23}
	\end{bmatrix} =
	 \begin{bmatrix}
		r_{11} \\ 0 \\ 0 \\ r_{21} \\ 0 \\ 0
	\end{bmatrix}  + 
	 \begin{bmatrix}
		0 \\ r_{12} \\ 0 \\ 0 \\ r_{22} \\ 0 
	\end{bmatrix} + 
	 \begin{bmatrix}
		0 \\ 0 \\ r_{13} \\ 0 \\ 0 \\ r_{23}
	\end{bmatrix} =
	\begin{bmatrix}
		r_{11} \\ r_{21}
	\end{bmatrix} 
	\otimes
	\begin{bmatrix}
		1 \\ 0 \\ 0
	\end{bmatrix}
	+ 
	\begin{bmatrix}
		r_{12} \\ r_{22}
	\end{bmatrix} 
	\otimes
	\begin{bmatrix}
		0 \\ 1 \\ 0
	\end{bmatrix}
	+ 
	\begin{bmatrix}
		r_{13} \\ r_{23}
	\end{bmatrix} 
	\otimes
	\begin{bmatrix}
		0 \\ 0 \\ 1
	\end{bmatrix}
	\]
	
\end{observation}

\begin{observation}[Theorem 14.5 Roman]
	Let $ \set{u_1\cdots,u_n} \subset U $ be linearly independent, and $ \set{v_1\cdots,v_n} \subset V $ be arbitrary vectors. Then
	\[ \sum_{i=1}^{n} u_i \otimes v_i = 0 \]
	implies that $ v_i = 0 $ for all $ i $.
	\begin{proof}
		Let $ f = \tau \circ t $ be any bilinear form, where $ \tau $ is its mediating morphism. Then
		\[ \tau (\sum_{i=1}^{n} u_i \otimes v_i) = \sum_{i=1}^{n} \tau(u_i\otimes v_i) = \sum_{i=1}^{n} f(u_i,v_i) = 0.  \]
		This holds true for any bilinear form $ f $. Choose $ f $ to be given as 
		\[ f(u,v) = u^k(u)\beta(v), \]
		where $ u^k $ is the dual basis for $ \set{u_k} $ and $ \beta \in V^* $. Using above, we can see that
		\[ \tau (\sum_{i=1}^{n} u_i \otimes v_i) = \sum_{i=1}^{n}u^k(u)\beta(v_i) = \beta(v_k) = 0. \]
		This holds true for all $ k $ and all $ \beta $. So $ v_k = 0 $ for all $ k $.
	\end{proof}
\end{observation} 

\begin{observation}[Injectivity of inclusion map in Theorem 14.7 Roman]
	In this theorem, we define a linear map $ \theta: U^*\times V^* \to (U\times V)^* $ by using the universal property of tensor products. I.e. we exhibit a bilinear map $ F: U^*\times V^* \to (U\otimes V)^* $ defined by
	\[ (F(f,g))(u\otimes v) = f(u)g(v), \]
	for $ f \in U^* $ and $ g\in V^* $. So this induces a unique linear map $ \theta: U^*\otimes V^* \to (U\otimes V)^* $. We now want to show that this linear map is indeed an injection. Let $ h\in U^*\otimes V^* $ be in the kernel of $ \theta $. I.e. $ \theta(h) $ will be the zero functional on $ U\otimes V $. I.e. for all $ z\in U\otimes V $ we can write
	\[ (\theta(h))(z) = 0. \] 
	However, we can represent $ h $ as 
	\[ h = \sum_{i=1}^{m} f_i \otimes g_i \]
	where $ \set{f_i} \subset U^* $ and $ \set{g_i} \subset V^* $ are linearly independent. So
	\[ \theta(h) = \sum_{i=1}^{m} \theta(f_i\otimes g_i) = 0. \]
	Let $ \set{e_k \otimes e'_l}_{k,l} $ be basis vectors of $ U\otimes V $. Since $ \theta(h) $ is the zero functional, for all $ k,l $ we need to have
	\[ (\theta(h))(e_k\otimes e'_l) = \sum_{i=1}^{m} f_i(e_k)g_i(e'_l) = 0 \quad \forall k,l. \]
	We can interpret this in two ways:
	\[ \sum_{i=1}^{m} g_i(e'_l) f_i = 0, \quad \text{and}\quad \sum_{i=1}^{m} f_i(e_k) g_i = 0, \]
	Since $ \set{f_i} $ and $ \set{g_i} $ are linearly independent, it follows that $ g_i(e'_l) = 0 $ for all $ i $ and $ l $. So $ \set{g_i} $ are all zero functionals. We can similarly conclude that $ \set{f_i} $ are all zero functionals.

\end{observation}

\begin{observation}[Useful fact!]
	When we are seeking a linear map whose domain is a tensor product, we need to find a bilinear form whose domain is a Cartesian product of the spaces, and then to use to universal property of the tensor products get the linear map from the tensor product space.
\end{observation}
	
	
\begin{observation}[Infinitesimals Vs. Nilpotents]
	In the context of linear algebra, the nilpotent maps are the maps that if raised to a large enough power, will be a zero map. This is a very interesting concept because it has very interesting similarities to the infinitesimals. When working with Floating Point representation of numbers in computers, then we are also dealing with nilpotents. For instance, in a floating point system, we can have (0.1)**5 = 0. So in this system $ 0.1 $ acts as a nilpotent of degree less than five.
	
	Presence of nilpotents in linear algebra, terminates some power series involving matrices. With a similar idea, all of the power series in a floating point representation is a finite sum. That is because if small values are raised to high enough powers, then at some point they become negligible compared to the machine precision, and this leads to the termination of the power series.
\end{observation}

\begin{observation}[A convenient way to represent any bilinear form ]
	Any bilinear form $ (\cdot,\cdot): U\times V \to F $ can be represented in a very convenient way. Let $ u \in U $ and $ v \in V $ with $ u = \sum_{i=1}^{n}u_i e_i $ and $ v = \sum_{j=1}^{m} v_j e'_j $. So we can write the bilinear form as
	\[ (u,v) = (\sum_{i=1}^{n}u_i e_i, \sum_{j=1}^{m} v_j e'_j) = \sum_{i,j} u_i v_j (e_i,e'_j) = \sum_i u_i \sum_j (e_i,e'_j)v_j = \sum_i u_i (Fv)_i = a^tFb, \]
	where 
	\[ (F)_{i,j} = (e_i, e_j). \]
\end{observation}


\begin{observation}[Some thoughts on the tensor product of weird stuff with other weird stuff]
	It is quote common in the topic of tensor products to see tensor product of spaces that look very bizarre! For instance, $ U\otimes \End V$, $ U^*\otimes U $, $ U^*\otimes \Hom(V,W) $, and etc are few examples. Trying to interpret these objects can be quite messy and there is a danger of overthinking leading to a very detailed and non functional intuition about their nature. 
	
	However, there is a simple intuitive way to look at these objects. And in fact, everybody has been interacting with these kind of objects from their high school. One of the prime examples is working with functions that are parameterized. For instance, consider $ f(x) = x^2 - \alpha $ that arises in studying the bifurcations in dynamical systems. Assume we want to consider the behaviour of this function for different values of its parameters. One way is to define a function with a same name but $ f: \R \times I \to \R $, where $ \alpha \in I $. I.e. we can assume that the function is defined on the Cartesian product of $ \R $ (where $ x $ lives) and $ I $ (where $ \alpha $ lives). However, this is not continent in different ways, and instead, we might choose to talk about a family of functions $ \set{f_\alpha}_{\alpha \in I} $ and $ f_\alpha: \R \to \R $ given by $ f_\alpha (x) = x^2 - \alpha $. In this way, the emphasis is still on the the values of the function defined on the original space $ \R $, but we also consider the parameter $ \alpha $ in an elegant way.
	
	The objects in the tensor spaces above has a very similar stories, but with even more information. For instance, consider $ U \otimes \End V $. The objects in the space, are \emph{endomorphisms} from $ V $ to $ V $ that are parameterized by $ U $, and the parameterization is \emph{linear}. So $ \sigma \in U\otimes \End V $, in its input, should receive a package, that contains about $ v \in V $ (on which the elements of $ \End V $ acts). And in the output, we need to distribute these inputs among the appropriate consumers and combine their results in a suitable way (there might be several ways to do so). What we have described above, is simply saying that there is an injection
	\[ \theta: U\otimes \End V \to \Hom( V, U\otimes V), \]
	where for instance for $ u\otimes \sigma $ we can write
	\[ (\theta(u\otimes \sigma))(v) = u\otimes \sigma(v) \] 
	
	Or, as another example, consider $ U^* \otimes \Hom(V,W) $. Objects in this space are linear operators from $ V $ to $ W $ that are parameterized by some linear functional on $ U $. So for instance for $ f \otimes \sigma $ where $ f\in U^* $ and $ \sigma \in \Hom(V,W) $ we can write
	\[ (f\otimes \sigma) (u, v) = f(u) \sigma(v). \]
\end{observation}

\begin{observation}[Some verbal explanation on a hard looking problem]
	Somewhere in one of the homework problems, one asks to find a natural isomorphism 
	\[ \theta: \End(U\otimes V) \to \Hom(U,U\otimes \End(V)). \]
	At first, this might seem like a hard problem, but there is nothing hard about it. The proof is writing down almost everything comes into your mind in your natural language. First, lets make a guess on what this isomorphism can be. I.e. given any element in $ \End (U\otimes V) $, then $ \theta(\sigma) $ should be interpreted by an element in $ \Hom(U,U\otimes \End(V)) $. Let's go through this one step at a time. If $ \theta(\sigma) $ is an element in $ \Hom(U,U\otimes \End(V)) $, then it should accept $ u\in U $, and $ [\theta\sigma](u) $ should be an element in $ U\otimes \End V $, i.e. an endomorphism on $ V $ parameterized by a vector $ u $. So $ [\theta\sigma](u) $, thought as a parameterized endomorphism,  should receive $ v\in V $ and do with it anything that a parameterized endomorphsim does (i.e. parameter $ \otimes $ result of the endomorphism on $ v $). So 
	\[ ([\theta\sigma](u))(v) = \sigma(u\otimes v). \]
	
	As another example, we want to find a natural isomorphism 
	\[ \theta: \Hom(U\otimes V_1, U\otimes V_2) \to \Hom(U, U\otimes\Hom(V_1,V_2)). \]
	Let $ \sigma \in \Hom(U\otimes V_1,U\otimes V_2) $. So we can write
	\[ ((\theta\sigma)(u))(v_1) = \sigma(u\otimes v_1). \]
\end{observation}


\begin{observation}
	Let $ U, V $ be real vector spaces equipped with non-degenerate inner products. We can show that 
	\[ \langle u_1\otimes v_1, u_2\otimes v_2 \rangle_{U\otimes V} := \langle u_1, u_2 \rangle_U \langle v_1, v_2 \rangle. \]
	Let $ A \in \End(V) $ and $ B \in \End(V) $. Then we can show that 
	\[ (A\otimes B)^\dagger = A^\dagger \otimes B^\dagger. \]
	To see this observe that
	\begin{align*}
		\langle (A\otimes B)^\dagger u\otimes v, x\otimes y \rangle &= \langle u\otimes v, (A\otimes B)(x\otimes y) \rangle \\
		&= \langle u\otimes v, Ax\otimes By \rangle \\ 
		&= \langle u, Ax \rangle \langle v, By \rangle \\
		&= \langle A^\dagger u, x \rangle \langle B^\dagger v, y \rangle \\
		&= \langle A^\dagger u \otimes B^\dagger v, x\otimes y \rangle  \\
		&= \langle (A^\dagger \otimes B^\dagger)(u\otimes v), x\otimes y \rangle \\
	\end{align*}
\end{observation}

\begin{observation}
	Theorem 14.5 in Roman and P2 in Lior's homework problems seems to be quite uncompatible. But there is a very subtle difference. What Lior is saying is that a pure tensor is zero if and only if and only if one of their components is zero. But the statement in Theorem 14.5 is not saying this.
\end{observation}

\begin{observation}[Some notes and the dual basis]
	The dual basis might be quite confusing for people who encounter it for the first time. However, there is a very simple and intuitive semantics behind it, which is as described below. Let $ f \in U^* $ be a linear functional $ f: U\to F $, where $ F $ is the underlying field. $ f $ will be uniquely determined by its values on the basis vectors and extending it linearly to the whole space. So if $ U = \Span{u_1,\cdots,u_n} $ then to fully specify $ f $ we need to determine
	\[ u_1\mapsto \alpha_1,\quad u_2 \mapsto \alpha_2, \quad \cdots,\quad u_n \mapsto \alpha_n. \]
	An alternative way to write the mappings above in a more compact and computationally useful way is to write
	\[ f = \alpha_1 u^1 + \alpha_2 u^2 +\cdots + \alpha_n u^n, \]
	where $ u^i $ is a special kind of linear function that is determined by mapping $ u_i $ to one and all the rest basis vectors to zero. Now we can record all these information in a row matrix, and $ f(x) $ for some vector $ x $ will by simply multiplying that row vector in $ x $ from left. 
	
	Also, using the machinery of the tensor product we can do a similar thing with the bilinear forms. Let $ F: U\otimes U\to F $ be a bilinear form. By the universal property of the tensor products we can see that it induces a linear form $ \tau: U\otimes V $. So its value can be determined by its values on the basis vectors of $ U\otimes U $, that is $ \set{e_i\otimes e_j}_{i,j} $. One way to write down this specification is to write
	\[ e_1\otimes e_1 \mapsto \alpha_{11}, \quad e_1\otimes e_2 \mapsto \alpha_{12}, \quad  \cdots, e_{i} \otimes e_j \mapsto \alpha_{i,j}, \quad e_n\otimes e_n \mapsto \alpha_{n,n}. \]
	Or since $ \sigma  $ is a linear functional, similar to the previous example we can write it simply as 
	\[ \sigma = \sum_{i,j} \alpha_{i,j} e^i\otimes e^j, \]
	where $ e^i\otimes e^j $ is a special kind of linear functional on $ U\otimes U $ that is specified by sending $ e_i\otimes e_j $ to zero and everything else to zero. In other words
	\[ (e^i\otimes e^j)(e_k\otimes e_l) = \delta_{i,k}\delta_{j,l}.  \]
	Now we can record all this information in a matrix as 
	\[ M = \begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1m} \\
		a_{21} & a_{22} & \cdots & a_{2m} \\ 
		\vdots & \vdots & \ddots & \vdots \\
		a_{n1} & a_{n2} & \cdots & a_{nn}
	\end{pmatrix} \]
	And then $ \sigma(u\otimes v) = F(u,v) $ will simply be
	\[ u^t M v. \]
\end{observation}

	



\section{Ongoing thoughts}

\begin{observation}[Ongoing thought on the relation of the space of linear operators and the tensor product]
	In many instances, I have noticed a similar structure between $ \mathcal{L}(V,W) $ and $ V \otimes W $. For instance, we know that while $ u\otimes v $ is a tensor (a pure tensor), but not every tensor can be written like this, but rather it is a linear combination of pure tensors. This is very similar to the idea that for $ A: V\to W $ and $ B: U\to W $, we can construct a linear map $ C: U\oplus V \to W $, that has a block diagonal representation. But, we can not write every matrix in a block diagonal representation. 
	
	Also, another hint is that $ \dim(\mathcal{L}(V,W)) = n\times m $, and similarly, $ \dim(U\otimes V) = n\times m $. Yet another hint is that every elements of $ U\otimes V $ has a matrix coordinate. I need to make this connection more clear and easy to see / understand.
\end{observation}



\section{Questions}
\begin{problem}
	\begin{enumerate}[(a)]
		\item Let $ Z $ be any vector space, and suppose we have for each $ i $ a linear map $ g_i: Z\to V_i $. Show that there is a unique $ g: Z\to \prod_i V_i $ such that $ \pi_i \circ g = g_i $ for all $ i $.
	\end{enumerate}
\end{problem}

\begin{solution}
	A quick reminder that $ \prod_{i\in I} V_i $ is 
	\[ \prod_{i\in I} V_i = \set{f: I \to \bigcup_i V_i\ :\ f(i) \in V_i}. \]
	This a generalization of $ n $-tuple. C
	% https://q.uiver.app/#q=WzAsMyxbMSwwLCJaIl0sWzEsMSwiXFxwcm9kX3tpXFxpbiBJfSBWX2kiXSxbMCwxLCIoVl9pKV97aVxcaW4gSX0iXSxbMCwxLCJnIl0sWzAsMiwiKGdfaSlfe2lcXGluIEl9IiwyXSxbMSwyLCJcXHBpX2kiXV0=
	\[\begin{tikzcd}
		& Z \\
		{(V_i)_{i\in I}} & {\prod_{i\in I} V_i}
		\arrow["{(g_i)_{i\in I}}"', from=1-2, to=2-1]
		\arrow["g", from=1-2, to=2-2]
		\arrow["{\pi_i}", from=2-2, to=2-1]
	\end{tikzcd}\]
	Defien $ g: Z\to \prod_i V_i $ by
	\[ \pi_i\circ g = g_i. \]
	Note that $ g $ is uniquely determined by this definition. Indeed, for $ v\in Z $ we have
	\[ (g(v))(i) = \pi_i (g(v)) = (\pi_i\circ g)(v) = g_i(v). \]
	We need to show that $ g $ is linear. Let $ v,u\in Z$ and $ \alpha,\beta $ scalars. Then
	\[ (g(\alpha v + \beta u))(i)  = (\pi_i\circ g)(\alpha v+\beta u) = g_i(\alpha v + \beta u) = \alpha g_i(v) + \beta g_i(u) = \alpha (g(u))(i) + \beta (g(v))(i).  \]
	Since this is true for the $ g $ component-wise, then it follows that
	\[ g(\alpha v + \beta u) = \alpha g(v) + \beta g(u). \]
\end{solution}



\begin{problem}
	Let $ U,V,W $ be vector spaces, and let $ T: U\to V $ and $ S: V\to W $. 
	\begin{enumerate}[(a)]
		\item Suppose $ U,V $ are finite dimensional with bases $ \set{u_j}_{j=1}^m \subset U $ and $ \set{v_i}_{i=1}^n \subset V $, and let $ A \in M_{n,m}(F) $ be the matrix of $ T $ in those bases. Show that the matrix of the dual map $ T': V'\to U' $ with respect to the dual bases $ \set{u^j}, \set{v^j} $ is the transpose of $ A $.
	\end{enumerate}
\end{problem}
\begin{solution}
	\begin{enumerate}[(a)]
		\item Let $ \tilde A $ denote the matrix representation of $ T' $. Then $ (\tilde A)_{i,j} $, is the $ i^\text{th} $ component (in the dual basis) of $ T' v^j $. Note that $ T'v^j $ is a functional on $ U $ and to get its $ i^\text{th} $ component we need to apply it on $ u_i  $. So
		\[ (\tilde A)_{ij} = (T'(v^j))(u_i) = v^j(Tu_i) = (A)_{ji}. \]
	\end{enumerate}
\end{solution}


