\chapter{Halmos Solution Manual}


\section{Fields}

\begin{observation}
	In a group we can only add one element an integer number of times with itself. For instance we can only have $ \alpha + \alpha + \cdot + \alpha = m\alpha  $ for some $ m \in \N $. However, field is a generalization of group in the sense that we can have more general many times addition with itself for every element. For instance we can have $ q \alpha  $ where $ q $ is not necessarily an integer. The same is true in vector spaces. The fact that we can multiply a vector by some element of the underlying field (i.e. the scalar) shows this. 
\end{observation}


\begin{problem} 
	\begin{solution}
		\begin{enumerate}[(a)]
			\item Holds because $ (F,0,+) $ is an abelean group.
			\item Since $ (\mathcal{F},0,+) $ is an abelian group, then $ \alpha $ has an inverse (i.e. $ -\alpha $). Add this to both sides of the equation.
			\item We can write
			\begin{align*}
				\alpha + (\beta - \alpha) &= \alpha + (\beta + (-\alpha)) \\
				&= \alpha + \beta + (-\alpha) && \text{(distributivity of multiplication)} \\
				&= \alpha + (-\alpha) + \beta = \beta && \text{($ (F,0,+) $ is abelian group)}
			\end{align*}
			\item We can write
			\begin{align*}
				\alpha\cdot0 = \alpha \cdot (0 + 0) = \alpha\cdot 0 + \alpha\cdot 0 = 2\alpha\cdot 0.
			\end{align*}
			Adding the inverse of $ \alpha\cdot 0 $ to both sides we will get
			\[ \alpha\cdot0 = 0. \]
			\item We can use the distributivity of multiplication and write
			\[ (-1)\alpha + \alpha = (-1 + 1)\alpha = 0\cdot\alpha = 0. \]
			Adding the inverse of $ \alpha $ to both sides we will get
			\[ (-1)\alpha = -\alpha. \]
			
			\item We can write
			\begin{align*}
				(-\alpha)(-\beta) &= (-1(\alpha))(-\beta)  && \text{(property proved above)} \\
				&= ((-1)(\alpha))((-1)(\beta)) && \text{(property proved above)} \\
				&= (-1)(-1)\alpha\beta && \text{(associativity of product)} \\
				&= -(-1 )\alpha \beta = \alpha\beta
			\end{align*}
			
			\item If both $ \alpha $ and $ \beta $ are zero, then it follows that $ \alpha\beta = 0 $. If one of them is not zero, WLOG we can assume $ \beta \neq 0 $, then $ \beta^{-1} $ exists, and multiplying it on the both sides we will get 
			\[ \alpha = 0. \]
			{\color{red} \noindent In a second run, I observed that my proof above might be wrong. I am somehow using the conclusion to prove the hypothesis.}
		\end{enumerate}
	\end{solution}
\end{problem}


\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item No. $ (F,0,+) $ is not a group. $ (F\backslash\set{0},1,\cdot) $ is not a group.
			\item No. $ (F\backslash\set{0},1,\cdot) $ is not a group. The set of all integers is a Ring with identity.
			\item Yes. For instance, consider the set of all integers $ \Z $. Let $ \phi:\Z\to \Q $ be a bijection. Define the addition and multiplication as
			\[ m \oplus n = \inv{\phi}(\phi(m) + \phi(n)),\qquad m\odot n = \inv{\phi}(\phi(m)\cdot\phi(n)). \]
			Let $ z_1 = \inv{\phi}(1) $ and $ z_0= \inv{\phi}(0) $. Then we claim that $ (\Z,z_0,z_1,\oplus,\odot) $ is a field. It is straightforward to check that $ (\Z,z_0,\oplus) $, and $ (\Z\backslash\set{z_0},z_1,\odot) $ are groups, and the distributitivty law holds. For instance, to check for the associativity of addition let $ m,n,l \in \Z $. Then we have
			\begin{align*}
				(m\oplus n)\oplus l &= \inv{\phi}(\phi(m\oplus n) + \phi(l)) \\
				&= \inv{\phi}(\phi(m) + \phi(n) + \phi(l)) \\
				&= \inv{\phi}(\phi(m) + \phi(n\oplus l)) \\
				&= m \oplus (n\oplus l),
			\end{align*}
			where we have used the fact that $ \phi(m\oplus n) = \phi(m) + \phi(n) $.
		\end{enumerate}
	\end{solution}
\end{problem}


\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item We can solve this part with different levels of abstraction. But we want to use the fact that the multiplicative group $ U(n) $ (i.e. the set of numbers in $ \Z_n $ that are prime relative to $ n $) is a group under multiplication (see example 3.3 Lee Abstract Algebra). When $ n $ is prime, then $ U(n) $ contains all numbers $ 1,\cdots,n-1 $. Thus when $ n $ is prime, $ (\Z_n\backslash\set{0}, \cdot) $ and $ (\Z_n,+) $ are both groups, and since the distribution law holds, it follows that $ \Z_n $ is a field if and only if $ n $ is a prime number. 
			\item $ -1 = 4 $ in $ \Z_5 $.
			\item $ \frac{1}{3} = 5 $ in $ \Z_7 $.
		\end{enumerate}
	\end{solution}
\end{problem}
\begin{observation}
	The field $ \Z_p $ has characteristic $ p $.
\end{observation}


\begin{problem}
	\begin{solution}
		First, observe that if the cardinality of the underlying set of the field is infinite, then we have $ \underbrace{1+\cdots + 1}_m = m\cdot 1 \neq 0 $ for all $ m\in \N $.
		However, when $ F $ is finite, then 
		\[ F\text{ is finite} \implies m\cdot 1 =0 \text{ for some } m\in\N. \]
		We can see this by contrapositive. If $ m\cdot 1 \neq 0 $ for all $ m\in \N $ then $ F $ has at least $ \N $ many elements. We want to show that the smallest such $ m $ is prime. Assume otherwise. Then $ m=pq $ for some $ p,q\neq 0 $. Then 
		\[ 0 = m\cdot 1 = pq\cdot 1 = pq. \]
		Since every field is an integral domain (has no zero divisors) (see Theorem 8.9 Lee Abstract Algebra), then it implies that $ p=0 $  or $ q=0 $, this is a contradiction.
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item Yes. It is easy to check that $ (\Q(\sqrt{2}),0,+) $ and $ (\Q(\sqrt{2})\backslash\set{0},1,\cdot) $ are groups. The associativity and closedness of the operators can be shown directly. For instance
			\begin{align*}
				(\alpha + \beta\sqrt{2})(\eta + \gamma\sqrt{2}) = (\alpha\eta  + 2\beta\gamma )+ \sqrt{2}(\alpha\gamma + \beta\eta),
			\end{align*}
			hence the multiplication is closed. Also, $ 0,1 \in \Q $ are the same as $ 0,1\in \Q(\sqrt{2}) $. Also, it is easy to check that the additive inverse of $ \alpha + \sqrt{2}\beta $ is $ -\alpha - \sqrt{2}\beta $. And the multiplicative inverse is easy to calculate and follows from the observation that
			\[ (\alpha + \beta\sqrt{2}) \cdot (\frac{\alpha - \beta\sqrt{2}}{\alpha^2 - 2\beta^2}) =1. \]
			So the multiplicative inverse of $ \alpha + \beta\sqrt{2} $ is 
			\[ \frac{\alpha - \beta\sqrt{2}}{\alpha^2 - 2\beta^2}. \]
			\item No. Because $ 1+\sqrt{2} $ has no inverse of the form $ \alpha + \beta\sqrt{2} $ where $ \alpha,\beta \in \Z $. 
		\end{enumerate}
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item No. Not every polynomial has an inverse with integer coefficients. For instance, $ p= 2x^2 -1 $ should be multiplied by
			\[ -1 + 2x^2 - 4x^4 + 8x^6 - \cdots \]
			to get the $ 1 $ polynomial. But the expression above is not a polynomial.
			\item No. The same problem above. The set of all polynomials with integer or real coefficients forms a commutative Ring. 
		\end{enumerate}
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item The addition part of OK! I.e. $ (F,(0,0),+) $ forms an abelian group. However, $ (F\backslash\set{(0,0)},\mathds{1},\cdot) $ does not form a group as defined above. Because by the provided definition of multiplication we need to have $ (\alpha,\beta)\mathds{1} = (\alpha,\beta) $ that implies that the only choice for $ \mathds{1} $ is
			\[ \mathds{1} = (1,1). \]
			But then the elements $ (0,1) $ and $ (1,0) $ have no multiplicative inverses. This is not the only obstacle though.
			
			\item Yes. This multiplication resolves the obstacles above and $ (F\backslash\set{(0,0)},\mathds{1},\cdot) $ is an abelian group. It is easy to check that the multiplicative identity should be
			\[ \mathds{1} = (1,0). \]
			I.e. this is the only choice that satisfies $ (\alpha,\beta)\cdot \mathds{1} = (\alpha,\beta) $. It is also easy to check that the inverse for a non-zero element $ (\alpha,\beta) $ is
			\[ (\frac{\alpha}{\alpha^2 + \beta^2}, \frac{-\beta}{\alpha^2 + \beta^2}). \]
			
			\item It will lead to the same kind of structure. 
		\end{enumerate}
	\end{solution}
\end{problem}


\section{Vector Spaces}
\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item This follows from $ (V,0,+) $ being an abelian group.
			\item The additive inverse of the zero element in an additive group is itself. So this follows from $ (V,0,+) $ being an abelian group.
			\item We can write
			\[ \alpha\cdot0 = \alpha\cdot(0 + 0) = \alpha\cdot 0 + \alpha\cdot 0 \]
			Since the set of vectors is an additive abelian group, we can add the inverse of $ \alpha\cdot 0 $ to both sides and get
			\[ \alpha\cdot 0 = 0. \]
			\item We can write
			\[ 0\cdot x = (0+0)\cdot x = 0\cdot x + 0\cdot x. \]
			Since the set of vectors is an additive abelian group, then adding the inverse of $ 0\cdot x $ to both sides we will get
			\[ 0\cdot x = 0. \]
			\begin{remark}
				Note that in the expression above, the zero on the LHS is the zero element of the field, and the zero on the RHS is the zero element of the vector field. 
			\end{remark}
			\item {\color{red} \noindent Still thinking. I was trying to do a similar proof as for problem 1 part (g), but I realized that my proof for that part is also not correct.}
			
			\item We can add $ x = 1\cdot x $ to $ (-1)x $. Then using the distributivity law we can write
			\[ x + (-1)x = 0. \]
			Adding the additive inverse of $ x $ to both sides we will get
			\[ (-1)x = -x. \]
			
			\item We can write
			\[ y + (x-y) = 1\cdot y + 1\cdot (x-y) = 1\cdot(y+x+(-y)) = 1\cdot x = x. \]
 		\end{enumerate}
	\end{solution}
\end{problem}

\begin{problem}
	The elements of $ \Z_p^n $ are the n-tuples, or equivalently the set of all functions $ f:[p]\to \Z_p $ where $ [n]= \set{1,2,\cdots,n} $. There are $ p^n $ such functions. 
\end{problem}


\begin{problem}
	\begin{solution}
		No. One of the immediate problems that I can see is that the scalar $ 1 $ does not interact nicely with the vectors. I.e. in the vector space axioms we have $ 1\cdot x = x $ for all $ x\in V $. However, in the definition above we have $ 1\cdot(\xi_1,\xi_2) = (1\xi_1,0) = (\xi_1,0) \neq (\xi_1,\xi_2).    $
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		We assume that the vector space $ \C^3 $ is defined on $ \C $ rather than $ \R $.
		\begin{enumerate}[(a)]
			\item No. While the vector space $ (V,0,+) $ forms a group, but the scalars does not behave nicely. For instance $ i\cdot(r_1,\xi_2,\xi_3) = (ir_1,\xi_2,\xi_3)  $ and the first argument is not longer a real number.
			\item Yes.
			\item No. because $ (\xi_1,0,\xi_2) + (0,\tilde\xi_2,\tilde\xi_3) = (\xi_1,\tilde\xi_2,\xi_2+\tilde\xi_3) $ and neither its first or second argument is zero.
			\item The vector space $ (V,0,+) $ forms a group. The addition is closed: Let $ (\xi_1,\xi_2,\xi_3) $ and $ (\tilde\xi_1,\tilde\xi_2,\tilde\xi_3) $ be in the subspace. Then 
			\begin{align*}
				\alpha (\xi_1,\xi_2,\xi_3) + \beta(\tilde\xi_1,\tilde\xi_2,\tilde\xi_3) = (\alpha\xi_1+\beta\tilde\xi_1,\alpha\xi_2+\beta\tilde\xi_2,\xi_3+\tilde\xi_3).
			\end{align*}
			Since
			\[ \alpha(\xi_1 + \xi_2) + \beta (\tilde\xi_1 + \tilde\xi_2) = 0, \]
			then the sum is also in the subspace, and the addition is closed. Also, in additive inverse of $ (\xi_1,\xi_2,\xi_3) $ is
			\[ (-\xi_1,-\xi_2,-\xi_3) \]
			where since $ -(\xi_1+\xi_2) = 0 $ it implies that the inverse is also in the subspace. It is also easy to check that the scalars behave nicely with the vector space.
			\item No. This subspace does not contain the origin $ (0,0,0) $.
		\end{enumerate}
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		The answers to this question depends on the set where the coefficients of the polynomial belongs, as well as the scalars on which the vector space is defined. We assume that the coefficients are complex numbers and the scalars are also complex numbers. 
		\begin{enumerate}[(a)]
			\item Yes.
			\item Yes. It is easier to see this if we identify each such polynomial with a 4-tuple $ (a_0,a_1,a_2,a_3) $, where these coordinates record the coefficients of the monomials $ 1,x^1,x^2$, and $ x^3 $ respectively. Then the subset of interest is 
			\[ \tilde V = \set{(a_0,a_1,a_2,a_3): a_1 = 2a_0}. \]
			This is subspace. Because the structure $ (\tilde V,+,0) $ is a group (it is easy to check that under addition of two such tuples the resulting tuple still satisfies $ \xi_1 = 2\xi_0 $). Also, it is straightforward to see that $ (0,0,0,0) \in \tilde V $. And the inverse of $ (a_0,2a_0,a_2,a_3) $ is $ (-a_0,-2a_0,-a_2,-a_3) $.
			\item No. let $ x\in V $. So $ x(t) \geq 0 $ for $ t\in [0,1] $. Let $ \alpha = 2 $ be a scalar. Then $ \alpha x(t) \leq 0 $ for $ t\in [0,1] $. So $ \alpha x(t) \notin V $.
			
			\item Yes. The specified condition leads to 
			\[ a_0 + a_1t + a_2t^2 + a_3t^3 = a_0 + a_1(1-t) + a_2(1-t)^2 + a_3(1-t)^3.  \]
			This simplifies to 
			\[ (2t-1)a_1 + (t^2-(1-t)^2)a_2 + (t^3 - (1-t)^3)a_3 = 0. \]
			Observe that
			\[ t^2 - (1-t)^2 = 2t-1. \]
			Also
			\[ t^3 - (1-t)^3 = 2t^3 - 3t^2 + 3t - 1. \]
			So we will have
			\[ (2t-1)a_1 + (2t-1)a_2 + (2t^3-3t^2+3t-1)a_3 = 0. \]
			This is a hyperplane passing through the origin in $ \R^4 $ (if we identify each polynomial with a 4-tuple). This defines a subspace. Note: We could have said this without simplifying the coefficients, and I did that for no good reason!
		\end{enumerate}
	\end{solution}
\end{problem}

\section{Bases}
\begin{observation}[Dimension depends on basis]
	Consider the vector space $ \C^1 $. Let $ z_1,z_2 \in \C^1 $ be any non-zero vectors. Then $ z_1,z_2 $ are linearly dependent vectors. The reason that I am highlighting this is that I was putting too much emphasis on the looking at $ \C^1 $ as $ \R^2 $ that I was not expecting to see that any two non-zero vectors in $ \C^1 $ is linearly dependent (which is definitely false, in $ \R^2 $ as not any two non-zero vectors are linearly dependent). Then I realized that it is the magic of scalars that makes the difference. If the vector space of consideration is $ (\C^1,\R^1) $, i.e. the scalars are real numbers, then not every two vectors in $ \C^1 $ is linearly dependent. For instance the vectors $ 1 $ and $ i $ are linearly independent (as there are no ways to multiply $ 1 $ at a real number and get $ i $). But in the case of $ (\C^1, \C) $ any two vectors are linearly dependent. Because in the example above we can multiply $ i $ be $ -i $ and get $ 1 $. I.e. in the second case the scalars are not only for enlarging the vectors, but also to rotate them.
\end{observation}

\begin{observation}[$ \R $ can be infinite dimensional]
	We often omit the underlying field when talking about the vector space $ \R $. But it turns out to be crucial when considering the dimension of the space. For instance, it is easy to see that $ (\R,\R) $ (i.e. the underlying field is $ \R $) is one dimensional. However, $ (\R,\Q) $ is infinite dimensional.
\end{observation}

\begin{problem}
	\begin{solution}
		To show that $ x,y,z $ are linearly independent, consider the linear combination
		\[ \alpha x + \beta y + \gamma z = 0. \]
		Considering the components of $ x,y,z $ we will have
		\[ \begin{cases}
			\alpha = 0,\\
			\beta = 0,\\
			\gamma = 0.
		\end{cases}. \]
		This $ \set{x,y,z} $ is linearly independent. For other combinations of vectors, one can easily mimic a similar approach.
	\end{solution} 
\end{problem}

\begin{problem}
	\begin{solution}
		\noindent $ \boxed{\Rightarrow} $ We want to show that $ 1,\xi $ being linearly independent implies $ \xi \notin \Q $. We use prove by contrapositive. Let $ \xi \in\Q $. Then we will have
		\[ \alpha + \beta \xi = 0 \]
		when $ \beta = -\alpha / \xi \in \Q $. So $ 1,\xi $ is not linearly independent.
		
		\noindent $ \boxed{\Leftarrow} $ We want to show that $ \xi \notin \Q $ implies $ 1,\xi $ are linearly independent. We again use prove by contrapositive. Assume $ 1,\xi $ be linearly dependent. Then
		\[ \alpha  + \beta \xi = 0 \]
		holds while not both of $ \alpha,\beta = 0 $. However note that either $ \alpha $ or $ \beta $ being zero forces the other one to be zero as well. So $ \xi = -\alpha/\beta $ thus $ \xi \in \Q $. This completes the proof.
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		Consider the linear combination
		\[ \alpha (x+y) + \beta (y+z) + \gamma (z+x) = 0. \]
		By rearranging the terms we will have
		\[ (\alpha + \gamma) x + (\alpha + \beta) y + (\beta + \gamma)z = 0. \]
		Invoking the linear independence of $ x,y,z $ we will have
		\[ \begin{cases}
			\alpha = -\gamma ,\\
			\alpha = -\beta, \\
			\beta = -\gamma.
		\end{cases} \]
		This implies $ \alpha = \gamma = \beta = 0 $. So the specified vectors are linearly independent.
	\end{solution}
\end{problem}


\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item Let $ A = (1+\xi) $ and $ B = (1-\xi) $. Consider the equation
			\[ \alpha \vectt{A}{B} + \beta\vectt{B}{A} = \vectt{0}{0}. \]
			We can write the system of equations as
			\[ \matt{A}{B}{B}{A}\vectt{\alpha}{\beta} = \vectt{0}{0}. \]
			If the determinant of the matrix above is non-zero, then the only solution is $ \alpha = 0,\beta = 0 $ (multiply both sides at the inverse of the matrix). Thus the vectors are linearly dependent if we have
			\[ A^2 - B^2 = 0. \]
			This implies
			\[ \xi = 0. \]
			
			\item Similar to the solution above we need to have
			\[ \det \begin{pmatrix}
				\xi & 1 & 0 \\
				1 & \xi & 1 \\ 
				0 & 1 & \xi
			\end{pmatrix} = 0. \]
			This happens when
			\[ \xi^3  = 2\xi, \]
			The solutions for the equation above is
			\[ \xi = 0, \qquad \xi = \pm\sqrt{2}. \]
			
			\item Then the vectors are linearly independent only when $ \xi = 0 $ (note that $ \xi $ can not take the values $ \pm \sqrt{2} $.)
		\end{enumerate}
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item In order for $ (\xi_1,\xi_2) $ and $ (\eta_1,\eta_2) $ be linearly dependent, we need to have
			\[ \det\matt{\xi_1}{\eta_1}{\xi_2}{\eta_2} = 0. \]
			This holds if $\xi_1\eta_2 = \xi_2\eta_1 $.
			
			\item For two vectors $ x,y \in \C^3 $ to be \emph{linearly independent}, the following matrix need to have rank 2.
			\[ \begin{pmatrix}
				x_1 & y_1 \\
				x_2 & y_2 \\
				x_3 & y_3
			\end{pmatrix}. \]
			This does not hold true if
			\[ x_1y_2 = x_2y_1, \quad \text{and} \quad x_2y_3 = x_3y_2. \]
			And for three vectors $ x,y,z\in \C^3 $ to be linearly dependent, the matrix below should have zero determinant:
			\[ \det \begin{pmatrix}
				x_1 & y_1 & z_1 \\
				x_2 & y_2 & z_2 \\
				x_3 & y_3 & z_3
			\end{pmatrix} = 0. \]
			
			\item No. 
		\end{enumerate}
	\end{solution}
\end{problem}


\begin{problem}
	Skipped. Will be added later.
\end{problem}

\begin{problem}
	The following basis will do the job
	\[ \mathcal{B}_1 = \big\{
	\begin{bmatrix}
		0 \\ 0 \\ 1 \\ 1
	\end{bmatrix}, 
	\begin{bmatrix}
		1 \\ 1 \\ 0 \\ 0
	\end{bmatrix}, 
	\begin{bmatrix}
		0 \\ 1 \\ 0 \\ 0
	\end{bmatrix}, 
	\begin{bmatrix}
		0 \\ 0 \\ 1 \\ 0
	\end{bmatrix}
	\big\}, \]
	and 
	\[ \mathcal{B}_1 = \big\{
	\begin{bmatrix}
		0 \\ 0 \\ 1 \\ 1
	\end{bmatrix}, 
	\begin{bmatrix}
		1 \\ 1 \\ 0 \\ 0
	\end{bmatrix}, 
	\begin{bmatrix}
		1 \\ 0 \\ 0 \\ 0
	\end{bmatrix}, 
	\begin{bmatrix}
		0 \\ 0 \\ 0 \\ 1
	\end{bmatrix}
	\big\}. \]
\end{problem}

\begin{problem}
	Skipped. Will be added later.
\end{problem}

\begin{problem}
	Skipped. Will be added later.
\end{problem}

\begin{problem}
	Skipped. Will be added later.
\end{problem}

\begin{problem}
	Skipped. Will be added later.
\end{problem}



\section{Isomorphism}
\begin{observation}[A better proof of Theorem in \S 9 ]
	Here we give a more formal prove of the theorem below.
	\begin{theorem}
		Every $ n $-dimensional vector space (defined over the field $ F $) is isomorphic to $ F^n $.
	\end{theorem}
	\begin{proof}
		Let $ \set{b_i} $ be a basis for $ V $ and let $ \set{e_i} $ be the standard basis for $ F^n $. Define the map $ \phi: V \to F^n $ by
		\[ \phi(b_i) = e_i \]
		and extend to the whole space by linearity. We claim that $ \phi $ is an isomorphism. First, observe that $ \phi $ is a function. Indeed $ x \in V $ can be expanded $ x = \sum_i \alpha_i b_i $, and
		\[ T(x) = T(\sum_i \alpha_i b_i) = \sum_i \alpha_i T(b_i) = \sum_i e_i.  \]
		Also, $ \phi $ is linear by construction. Furthermore, we claim that $ \phi $ is injective. To show this we prove that $ \ker T = \set{0} $. Let $ x \in \ker T $. We expand $ x = \sum_i \alpha_i b_i $ and by applying $ T $  we have
		\[ 0 = T(x) = T(\sum_i \alpha_i b_i) = \sum_i \alpha_i T(b_i) = \sum_i \alpha_i e_i.  \]
		Since $ \set{e_i} $ is linearly independent, it follows that $ \alpha_i = 0 $ for all $ i $, thus $ x = 0 $. Also, we claim that $ T $ is surjective. To see this let $ y \in F^n $ with expansion $ y = \sum_i \alpha_i e_i $. Let $ x = \sum_i \alpha_i b_i $. We claim that $ T(x) = y $. Indeed
		\[ T(x) = T(\sum_i \alpha_i b_i) = \sum_i \alpha_i T(b_i) = \sum_i \alpha_i e_i = y. \]
	\end{proof}
\end{observation}

\begin{problem}
	\begin{solution}
		\begin{enumerate}[(a)]
			\item For $ V = (\C,\R) $ the dimension is $ 2 $. Let $ \set{1,i} \subset \C $. Construct the map
			\[ \phi: \C \to \R^2, \]
			given by $ \phi(1) = e_1, \phi(i) = e_2 $, extended to the whole space by linearity. It is straight forward to see that $ \phi $ is an isomorphism of vector spaces. Thus $ V $ is two dimensional.
			
			\item We can choose the basis $ \set{(1,0\cdots,0),(i,0,\cdots,0),(0,1,0,\cdots,0),(0,i,0,\cdots,0),\cdots (0,\cdots,0,i)}, $ and we can construct an isomorphism to $ \set{e_1,e_2,\cdots,e_{2n}} $ similar to above. Thus $ V^- $ is $ 2n $ dimensional.
		\end{enumerate}
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		No! $ (\R,\Q) $ is in fact infinite dimensional vector space with $ \dim_FV = \mathfrak{c} $. A basis for this space is $ \R/\Q $.
	\end{solution}
\end{problem}

\begin{problem}
	\begin{solution}
		For an $ n $-dimensional vector space over the field $ \Z_p $ there is an isomorphism to $ (\Z_p)^n $. There are $ p^n $ vectors in this space. To see this note that the product space $ (\Z_p)^n $ is the space of all functions $ f:[n]\to\Z_p $ where $ f(i) \in \Z_p $ (using the notation $ [n] = \set{1,2,\cdots,n} $). There are $ p^n $ such maps. 
		\begin{remark}
			The functions in the product space is the same as the tuples if the dimension is finite.
		\end{remark}
	\end{solution}
\end{problem}


\begin{problem}
	\begin{solution}
		We assume that the vector spaces of interest of finite dimensional. Then the assertion is not true. Because $ \Q^n $ is countable for all $ n\in \N $, thus its cardinality is $ \aleph_0 $. However, dimension is an isomorphism invariant. So the assertion is not true. In particular, $ \Q^2 $ and $ \Q^3 $ both as the same cardinality $ \aleph_0 $, but they are not isomorphic (because they have different dimension).
	\end{solution}
\end{problem}


\section{Dimension of a subspace}
\begin{observation}
	Denote by $ \mathcal{S} $, the set of all subspaces of a vector space $ V $. Observe that $ V,\emptyset \in \mathcal{S} $, and $ \mathcal{S} $ is closed under intersection (it is not closed under union).
\end{observation}

\begin{observation}[Definition of spanning]
	In this definition box, I am highlighting the definition of the span of a set.
	\begin{definition}[Span of a set]
		Let $ S \subset V $ where $ V $ is some finite dimensional vector space. The span of $ S $, denoted by $ \Span S $, is the smallest linear subspace containing all $ S $.
	\end{definition}
	\begin{proposition}
		Let $ {S},V $ be as in the definition above. Then $ \Span S $ is the same as the all linear combinations of vectors in $ S $.
	\end{proposition}
	\begin{proof}
		The proof will have two parts. First, we prove that
		\[ \Span S \subset \set{\sum_i \alpha_i s_i | s_i \in S}. \]
		For easier notation we denote $ W = \set{\sum_i \alpha_i s_i | s_i \in S} $. To see the set inclusion above observe that $ W $ is a subspace. Indeed, any linear combination of linear combinations of vectors in $ S $ can be written as a linear combination of vectors in $ S $. Also, note that $ W  $ contains $ S $. However, by definition $ \Span S $ is the smallest subspace containing $ S $. So $ W $ must also contain $ S $, i.e. $ S\subset W $. Now we prove the reverse inclusion
		\[ W \subset \Span S. \]
		To see this note that $ \Span S $ is a subspace, and it contains $ S $. So it also contains all linear combination of vectors in $ S $ as well. So it contains $ W $. I.e. $ W\subset S $.
	\end{proof}
	The proof of the following proposition is left to the reader in \S11 Halmos.
	\begin{proposition}
		Let $ K,H $ be two linear subspaces of $ V $ and let $ S = \Span (K\cup H) $. Then $ S $ is the same as the set of all vectors of the form $ x+y $ where $ x\in K $ and $ y\in H $. 
	\end{proposition}
	\begin{proof}
		From the proposition above we know that $ S $ contains all of linear combination of of elements of $ K\cup H $. Such a linear combination can be written as $ x + y $ where $ x \in K $ and $ y \in H $, and any linear combination of the form $ x+y $ is a linear combination of elements of $ K\cup H $. So $ S $ contains all such vectors.
	\end{proof}
\end{observation}



\begin{problem}
	\begin{solution}
		Let $ \set{b_i}_{i=1}^n $ be a basis for $ M $. Since $ M\subset N $, and both of the are subspaces, then we can extend this basis to a basis on $ N $. Since $ N $ is also $ n $-dimensional, $ \set{b_i}_{i=1}^n $ is already a basis for $ N $ (adding any new vector will make the collection linearly dependent). Let $ y \in N $. We can expand $ y = \sum_i \alpha_i b_i $. This implies $ y \in M $. So $ N \subset M $. Then it follows that $ M = N $.
	\end{solution}
\end{problem}


\begin{problem}
	\begin{solution}
		We state the problem in a better language: If $ M,N $ are subspaces of a vector space $ V $, and if $ V \subset M \cup N $, then $ V = M $ or $ V = N $. We prove by contrapositive. Assuming $ M\varsubsetneq V $ and $ N\varsubsetneq V $. Then $ \exists x \in V $ such that $ x \notin M $ and $ x\notin N $. I.e. $ x \in M^c \cap N^c = (M\cup N)^c $. Thus $ x \notin M\cup C $.
		This completes the proof.
	\end{solution}
\end{problem}




